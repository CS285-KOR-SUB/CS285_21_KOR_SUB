1
00:00:01,040 --> 00:00:04,935
그래서, 앞서 distributional shift problem를

2
00:00:04,960 --> 00:00:06,730
Dagger 알고리즘을 사용하여
해결할 수 있음을 확인했습니다.

3
00:00:06,755 --> 00:00:09,739
그러나 DAgger 알고리즘을
실질적으로 사용하기에는

4
00:00:09,764 --> 00:00:14,879
추가적인 labeling 단계 때문에
사용하기에 어려움이 있습니다.

5
00:00:14,880 --> 00:00:20,069
더 많은 데이터를 수집하지 않고도
deep imitation learning을 할 수 있을까요?

6
00:00:20,093 --> 00:00:25,020
DAgger는 distributional shift/drift 문제를 다룹니다.

7
00:00:25,045 --> 00:00:26,997
하지만 우리 모델이 너무 좋아서

8
00:00:27,022 --> 00:00:29,029
drifting 문제가 아예 없다면 어떨까요?

9
00:00:29,054 --> 00:00:30,660
만약 그것이 너무 정확해서

10
00:00:30,720 --> 00:00:38,700
p_data(o_t)에서 너무 벗어나지 않고
여전히 좋은 상태로 유지된다면 어떨까요?

11
00:00:38,719 --> 00:00:43,059
그렇게 하려면 전문가의 behavior을
매우 정확하게 따라해야 합니다.

12
00:00:43,120 --> 00:00:46,063
그러나 동시에 overfit 되지 않도록 해야 합니다.

13
00:00:46,088 --> 00:00:48,860
실제로, 몇 가지 이를 할 수 있는 방법들이 있습니다.

14
00:00:48,879 --> 00:00:51,498
비록 DAgger처럼 가지고 있는 이론적 보장은 없지만,

15
00:00:51,523 --> 00:00:56,079
실제 경험적으로는 작동할 수 있습니다.

16
00:00:56,364 --> 00:00:59,259
우리가 할 수 있는 것을 이해하기 위해

17
00:00:59,280 --> 00:01:01,334
먼저 왜 우리가 전문가 데이터를 맞추지 못할까를
생각해봐야 합니다.

18
00:01:01,359 --> 00:01:04,469
우리 모델이 왜 완벽하지 않을까요.

19
00:01:04,494 --> 00:01:06,660
몇 가지 가능한 원인들이 있습니다.

20
00:01:06,685 --> 00:01:11,168
한 가지 이유는 observation이
완전히 markovian이라고 하더라도

21
00:01:11,193 --> 00:01:15,120
observation이 state 추론을 완전히
무시하기에 충분하다는 것입니다.

22
00:01:15,195 --> 00:01:19,200
인간의 행동은 markovian이 아닐 수 있습니다.

23
00:01:19,200 --> 00:01:23,500
두 번째 가능성은
우리가 continuous action을 한다면

24
00:01:23,520 --> 00:01:26,535
시행자(demonstrator)의 행동이
multimodal일 수 있다는 것입니다.

25
00:01:26,560 --> 00:01:31,160
즉, distribution의 여러 다른 mode 중에서

26
00:01:31,200 --> 00:01:35,540
일관성 없이 선택하기 때문에
행동을 따라하기 어렵습니다.

27
00:01:35,600 --> 00:01:38,760
첫 번째 문제부터 봅시다.

28
00:01:38,840 --> 00:01:43,878
일반적으로 markovian policy을 배울 것입니다.

29
00:01:43,903 --> 00:01:47,575
이 용어는 우리의 policy가 o_(t-1) 또는 o_(t-2)가 아닌

30
00:01:47,600 --> 00:01:51,840
o_t에만 의존한다는 것을 의미하는 전문 용어입니다.

31
00:01:52,479 --> 00:01:56,399
그리고 사람은 실제로 이런
식으로 행동하지 않을 수 있으므로

32
00:01:56,424 --> 00:02:00,360
markovian이라는 것은 기본적으로
동일한 것을 두 번 본다면,

33
00:02:00,399 --> 00:02:04,320
우리는 이전에 무슨 일이 있었는지에 관계없이
같은 일을 두 번 반복한다는 의미입니다.

34
00:02:04,320 --> 00:02:06,421
어떤 경우에는
사람은 이런 식으로 행동합니다.

35
00:02:06,446 --> 00:02:09,020
그러나 많은 경우에 그렇지 않습니다.
당신이 차를 운전하고 있는데

36
00:02:09,039 --> 00:02:11,039
당신이 누군가 끼어들었다면

37
00:02:11,064 --> 00:02:13,940
조금 떨리거나 불안할지도 모릅니다.

38
00:02:13,964 --> 00:02:16,840
아마도 그 사람이 그렇게 하지 않았다면
그 다음에 무슨일이 일어나든

39
00:02:16,879 --> 00:02:20,360
약간 다르게 반응했을 거고,
조금 덜 화났을 겁니다.

40
00:02:20,400 --> 00:02:23,760
일반적으로 사람은
매우 일관성이 없으며,

41
00:02:23,840 --> 00:02:27,020
control하기 위해 어떤 strategy를 사용하든

42
00:02:27,040 --> 00:02:34,000
시스템이 non-markovian일 수 있습니다.
최적의 markovian strategy이 존재한다고 해도요.

43
00:02:34,319 --> 00:02:37,326
따라서 인간 시연자가
완벽한 markovian 전문가가 되기위해

44
00:02:37,351 --> 00:02:41,975
똑같은 state에서 똑같은 action을 하는 것은

45
00:02:42,000 --> 00:02:45,940
부자연스럽습니다.

46
00:02:46,000 --> 00:02:48,560
따라서 사람은 아마 이렇게 행동할 것입니다.

47
00:02:48,640 --> 00:02:51,199
아마 실제 o_1~o_t까지의
observation 전체 history를 조건부로

48
00:02:51,200 --> 00:02:56,320
action에 대한 분포를 가질 것 입니다.

49
00:02:56,400 --> 00:03:01,494
물론 이것이 전문가가 행동하는 방식이라면
마코비안 정책 없이는

50
00:03:01,519 --> 00:03:05,840
전문가의 행동을 완벽하게 재현할 수 없습니다.

51
00:03:06,959 --> 00:03:10,660
이제 우리는 전체 history를 모방할 수 있습니다.

52
00:03:10,720 --> 00:03:15,012
차가 관찰하고 있는 현재 이미지만을 전제하지 않고도요.

53
00:03:15,037 --> 00:03:18,520
우리는 전체 history에 있는 이미지들을 전제할 수 있습니다.

54
00:03:18,560 --> 00:03:21,210
그러나 여기에는 몇 가지 실질적인 문제가 있습니다.

55
00:03:21,235 --> 00:03:25,014
먼저, 모든 이미지를 단순히 연결(concatenate)하면

56
00:03:25,039 --> 00:03:29,540
history의 길이에 따라
이미지 갯수들이 다양할 수 있습니다.

57
00:03:29,599 --> 00:03:33,540
표준 컨볼루션 신경망에 어떻게 넣어줘야 하는지는 분명하지 않습니다.

58
00:03:33,599 --> 00:03:38,620
또한 이미지 수가 많으면
weight가 너무 많이 필요할 겁니다.

59
00:03:38,640 --> 00:03:44,948
이 문제에 대한 일반적인 솔루션은
RNN을 사용하는 것입니다.

60
00:03:44,973 --> 00:03:50,520
많이 쓰이는 유명한 아키텍쳐들 중 하나는
convolutional encoder 입니다.

61
00:03:50,560 --> 00:03:55,622
CNN은 이미지를 읽고 해당 이미지를
RNN의 state로 넣어줍니다.

62
00:03:55,647 --> 00:04:01,380
결과적으로 우리는 RNN 네트워크를 가지게 됩니다.
이 RNN backbone은

63
00:04:01,439 --> 00:04:04,520
임의의 길이인 sequence를 현재 state로 인코딩하고

64
00:04:04,560 --> 00:04:08,400
그 후 이것을 이용하여 action을 생성합니다.

65
00:04:08,400 --> 00:04:13,986
구현할 때 일반적으로 이 RNN을
LSTM 셀과 같은 것을 사용하여 나타냅니다.

66
00:04:14,011 --> 00:04:17,334
여기에서 neural network 아키텍처를
자세히 다루지 않을 것이므로

67
00:04:17,359 --> 00:04:23,714
RNN, LSTM 셀에 대해 배우고 싶다면
혼자서 관련 서적들을 찾아보세요.

68
00:04:23,739 --> 00:04:32,454
요점은, 이러한 종류의 LSTM 또는 RNN 아키텍처가
 non-markovian 문제를 상당히 해결할 수 있다는 것입니다.

69
00:04:32,479 --> 00:04:38,720
일반적으로 weights를 공유하는 공통 encoder와
RNN을 얻게 됩니다.

70
00:04:40,160 --> 00:04:44,540
자, 잠깐 제가 간단히 언급하고 싶은 것은 제쳐두고

71
00:04:44,560 --> 00:04:49,980
보통 이 RNN 접근 방식이 non-markovian
policy 문제를 해결합니다.

72
00:04:50,000 --> 00:04:54,080
그러나 몇 가지 다른 이유들이 여전히 있습니다.

73
00:04:54,160 --> 00:04:58,039
따라서 일반적으로 imitation learning의

74
00:04:58,080 --> 00:05:00,540
잠재적인 문제를 설명하자면,

75
00:05:00,639 --> 00:05:04,439
그러나 특히 histories를 다룰 때 중요한데요.

76
00:05:04,464 --> 00:05:08,137
다음 시나리오를 봅시다.

77
00:05:08,162 --> 00:05:12,517
당신이 차를 운전하고 있다고 가정해 봅시다.

78
00:05:12,542 --> 00:05:18,120
그리고 차량 내부에 카메라가 있어
카메라가 앞 유리창을 볼 수 있고

79
00:05:18,145 --> 00:05:23,913
대시보드도 볼 수 있습니다. 이 차는 계기판에 대부분의 자동차에는 없는
재미있는 브레이크 표시기가 있습니다.

80
00:05:23,938 --> 00:05:29,840
브레이크를 밟을 때마다 계기판에 불이 켜지는 작은 빛이 있다고 합시다.

81
00:05:29,840 --> 00:05:33,997
사람이 도로에 서 있으면 브레이크를 밟아야 합니다.

82
00:05:34,022 --> 00:05:37,198
데이터에 이러한 example이 있다고 해봅시다.

83
00:05:37,223 --> 00:05:41,500
이런 모든 examples에서
브레이크를 밟았을 때

84
00:05:41,520 --> 00:05:45,820
계기판의 표시등이 켜지고
차 앞에 사람이 있습니다.

85
00:05:45,840 --> 00:05:48,887
이제 neural network model이

86
00:05:48,912 --> 00:05:53,300
사람의 출현으로 인해 브레이크가 작동되었는지

87
00:05:53,360 --> 00:05:55,438
아니면 불이 켜진 것에 의한 것인지
알아내야 합니다.

88
00:05:55,463 --> 00:05:58,858
neural network가 빛과 브레이크와 연관시키는 것은 매우 쉽습니다.

89
00:05:58,883 --> 00:06:02,339
브레이크를 밟을 때 빛이 항상 켜지기 때문이죠.

90
00:06:02,400 --> 00:06:06,279
이것은 매우 일관된 단서인 반면,
차 앞에 서 있는 사람은

91
00:06:06,304 --> 00:06:09,840
알아내기가 조금 더 복잡합니다.

92
00:06:10,319 --> 00:06:15,127
만약 브레이크 등이 모호하거나
그 정보가 observation에서 제거된 경우

93
00:06:15,152 --> 00:06:17,302
이 이슈는 사라집니다.

94
00:06:17,786 --> 00:06:21,240
그래서 여기서 좀 재미있는 난제가 생기는데요.
우리가 observation을 더할 때,

95
00:06:21,280 --> 00:06:23,145
브레이크 등을 추가할 때,

96
00:06:23,170 --> 00:06:25,343
사실상 imitation learning을 더 어렵게 만듭니다.

97
00:06:25,368 --> 00:06:28,962
왜냐하면 일종의 인과적 혼란(causal confusion)
문제를 야기하기 때문이죠.

98
00:06:28,987 --> 00:06:36,700
이것은 observation의 부분들 간의
인과관계가 생기는 상황을 만듭니다.

99
00:06:36,720 --> 00:06:39,624
그리고 action은 데이터만을 가지고 추론하기가 어려워집니다.

100
00:06:39,649 --> 00:06:42,319
따라서 브레이크는 사람의 출현으로 작동하게 된거고

101
00:06:42,319 --> 00:06:44,698
그리고 브레이크로 인해 불이 꺼졌습니다.

102
00:06:44,723 --> 00:06:47,944
그러나 model은 이걸 이해하지 못할 수 있습니다.
그래서 대신에 다음과 같은 결론을 내릴 수 있습니다.

103
00:06:47,969 --> 00:06:52,080
브레이크는 빛에 의해 발생해야만 합니다.

104
00:06:53,599 --> 00:06:59,720
따라서 이런 현상에 대해 더 알고 싶다면
"causal confusion and imitation learning"이라는 de Haan의 논문이 있습니다.

105
00:06:59,759 --> 00:07:06,171
이건 markovian 또는 non-markovian policy에만 국한되지 않으며
모든 imitation learning 시나리오에서 일어날 수 있습니다.

106
00:07:06,196 --> 00:07:09,783
그런데 여기 생각해봐야 2가지 질문이 있습니다.

107
00:07:09,808 --> 00:07:16,021
첫번째는, history를 포함하는 것이
causal confusion을 일으키거나 더 나쁘게 만드는 가? 입니다.

108
00:07:16,046 --> 00:07:21,380
이 질문에 대해 생각해보고 자신의 답을 써보기를 바랍니다.

109
00:07:21,440 --> 00:07:24,756
서두번째는, DAgger는 casual confusion을 해결할 수 있을까요?

110
00:07:24,781 --> 00:07:29,560
DAgger을 사용하면 이 문제가 해결되는지
그렇다면 왜 그런지

111
00:07:29,599 --> 00:07:32,210
이번 강의에서 답을 말해주지 않을 겁니다.

112
00:07:32,235 --> 00:07:36,880
그러나 이것에 대해서 discussion section에서 좀 더 말해보도록 하죠.

113
00:07:37,759 --> 00:07:40,400
그래서, 그것은 non-markovian 행동입니다.

114
00:07:40,400 --> 00:07:42,539
이제 다음으로
전문가의 행동에 완벽하게 맞출 수 없는

115
00:07:42,560 --> 00:07:45,780
다른 잠재적인 이유인

116
00:07:45,847 --> 00:07:48,466
multimodal behavior을 보죠.

117
00:07:48,491 --> 00:07:53,280
당신이 드론을 비행한다고 가정해 보겠습니다.

118
00:07:53,280 --> 00:07:55,899
그리고 나무 주위를 날아야 합니다.

119
00:07:55,924 --> 00:07:57,699
그리고 아마 사람 전문가는

120
00:07:57,759 --> 00:08:00,494
때로는 나무의 왼쪽으로 돌아서 날리고

121
00:08:00,519 --> 00:08:04,140
때로는 나무의 오른쪽으로 돌아가도록 날릴 것입니다.

122
00:08:04,720 --> 00:08:07,140
만약 당신의 action이 discrete하다면

123
00:08:07,165 --> 00:08:10,519
만약 당신이 그냥 왼쪽, 오른쪽, 직진 action을 가지고 있다면

124
00:08:10,560 --> 00:08:14,935
이건 문제가 아닙니다.
왜냐하면 softmax 분포,

125
00:08:14,960 --> 00:08:16,991
흔히 우리가 neural networks에서

126
00:08:17,016 --> 00:08:18,400
categorical 분포를 출력하기를 원할 때

127
00:08:18,400 --> 00:08:20,375
사용하는 분포, softmax 분포가

128
00:08:20,400 --> 00:08:22,294
쉽게 그 notion을 파악할 수 있습니다.

129
00:08:22,319 --> 00:08:26,614
왼쪽 action을 할 확률이 높고,
오른쪽 action을 할 확률이 높고,

130
00:08:26,639 --> 00:08:30,700
반면 직진 action은 확률이 낮죠.
그래서 이건 문제가 아닙니다.

131
00:08:30,720 --> 00:08:32,910
하지만 만약 continuous한 action을 가지게 된다면

132
00:08:32,935 --> 00:08:34,450
그러면 일반적으로

133
00:08:34,475 --> 00:08:39,580
output distribution를 multivariate normal distribution로 나타냅니다.

134
00:08:39,599 --> 00:08:42,297
평균과 분산에 의해 분포의 모양이 결정되죠.

135
00:08:42,880 --> 00:08:45,870
이제, 만약 하나의 평균과 하나의 분산을 고른다면

136
00:08:45,895 --> 00:08:47,679
어떻게 그 평균과 분산을 가지고 model에게

137
00:08:47,680 --> 00:08:49,254
왼쪽으로 가거나

138
00:08:49,279 --> 00:08:52,400
오른쪽으로 가고 직진은 절대하지 않는다는
사실을 알려줄 수 있을까요?

139
00:08:52,480 --> 00:08:55,575
이건 큰 문제입니다.
왜냐하면 필수적으로

140
00:08:55,600 --> 00:08:58,620
모든 가능성들을 같이 평균을 하기 때문이죠.

141
00:08:58,640 --> 00:09:00,295
그리고 반드시 잘못된 것을 하게 됩니다.

142
00:09:00,320 --> 00:09:02,480
반드시 직진을 하게 될 겁니다.

143
00:09:02,480 --> 00:09:05,375
왜냐하면 왼쪽과 오른쪽의 평균이기 때문이죠.

144
00:09:05,400 --> 00:09:08,478
이 것이 multimodal behavior이
문제가 되는 이유입니다.

145
00:09:08,503 --> 00:09:10,700
그러나 일반적으로 사람은 multimodal 경향이 있습니다.

146
00:09:10,725 --> 00:09:14,535
사람들은 시연에서 매우 복잡한 분포를 보이는 경향이 있습니다.

147
00:09:14,560 --> 00:09:19,962
매우 간단한 작업하는 것도 아니고,
큰 문제가 될 수 있는 continuous action이라면 말이죠.

148
00:09:19,987 --> 00:09:22,060
몇가지 가능한 솔루션들이 있습니다.

149
00:09:22,080 --> 00:09:28,320
한 가지 솔루션은
한 개의 gaussian output distribution을 사용하지 않고,

150
00:09:28,399 --> 00:09:32,280
한 개 gaussian의 평균과 분산을 출력하는 것 대신에,

151
00:09:32,320 --> 00:09:36,535
mixture of gaussians을 출력할 수 있습니다.
여러개의 평균과 분산으로

152
00:09:36,560 --> 00:09:39,420
multiple modes를 파악할 수 있습니다.

153
00:09:39,519 --> 00:09:42,535
또한 latent variable model을 사용할 수 잇습니다.
좀 더 정교한 방법으로

154
00:09:42,560 --> 00:09:46,860
continuous 공간에서의 복잡한 확률분포를
나타낼 수 있습니다.

155
00:09:46,880 --> 00:09:50,215
또한, autoregressive discretization이라는
방법으로도 할 수 있습니다.

156
00:09:50,240 --> 00:09:53,400
3개의 각각에 대해서는 다음에 보도록 하죠.

157
00:09:53,440 --> 00:09:59,795
먼저 mixtures of gaussians에 대해 이야기해 보죠.
mixture density networks라고도 합니다.

158
00:09:59,820 --> 00:10:05,230
이 아이디어는 한 개의 gaussian의 평균과 분산을 출력하는 것 대신에

159
00:10:05,255 --> 00:10:10,392
N개의 μ, N개의 σ, 그리고 N개의 w들을 출력합니다.

160
00:10:10,417 --> 00:10:15,090
주어진 o의 확률은 평균 μ_i와 공분산 σ_i를 가진
가우스 분포와

161
00:10:15,115 --> 00:10:22,765
w_i의 각각의 곱들의 합
gaussian mixture로 주어집니다.

162
00:10:22,790 --> 00:10:24,758
매우 간단한 접근 방식입니다.

163
00:10:24,783 --> 00:10:29,095
gaussian mixture model들의 몇가지 trade-off는

164
00:10:29,120 --> 00:10:33,895
더 많은 output parameter들이 필요하다는 것입니다.

165
00:10:33,920 --> 00:10:41,095
그리고 고차원에서 multimodal distribution를
모델링하는 것은

166
00:10:41,120 --> 00:10:44,934
매우 어렵습니다.
즉 차원이 높을 수록 더 많은 mixture element들이 필요하죠.

167
00:10:44,959 --> 00:10:47,140
일반적으로, 임의의 분포의 경우
이론적으로

168
00:10:47,165 --> 00:10:49,735
잘 모델링하는 데 필요한
mixture element의 수는

169
00:10:49,760 --> 00:10:52,659
차원에 따라 기하급수적으로 증가합니다.

170
00:10:52,684 --> 00:10:56,980
따라서 2차원에 불과한
자동차의 가스와 브레이크 제어, 그리고 핸들을 조절한다면

171
00:10:57,040 --> 00:10:58,880
문제가 되지않지만
그러나

172
00:10:58,880 --> 00:11:00,859
수백 개가 될 수 있는

173
00:11:00,895 --> 00:11:02,875
휴머노이드 로봇의 모든 관절을 제어하거나

174
00:11:02,959 --> 00:11:05,360
또는 수백만개일 아마존의

175
00:11:05,360 --> 00:11:07,280
모든 제품들의 가격을 관리한다면

176
00:11:07,360 --> 00:11:12,295
gaussian mixtures은 좋은 선택이 아닐 수 있습니다.

177
00:11:12,320 --> 00:11:16,154
더 정교하지만 적용하기 어려운 한 선택은

178
00:11:16,179 --> 00:11:17,939
latent variable model 입니다.

179
00:11:17,964 --> 00:11:23,140
latent variable model에서
출력 분포는 여전히 gaussian 입니다.

180
00:11:23,200 --> 00:11:25,415
그러나 이미지를 입력할 때,

181
00:11:25,440 --> 00:11:29,924
모델에 latent variable도 넣어줍니다.

182
00:11:29,949 --> 00:11:32,455
이것은 prior distribution에서 추출될 수 있습니다.

183
00:11:32,480 --> 00:11:36,459
즉 본질적으로 model은
이미지와 약간의 노이즈를 받게되고

184
00:11:36,480 --> 00:11:41,040
이미지와 노이즈를
gaussian distribution 반응으로 바꿔줍니다.

185
00:11:41,040 --> 00:11:44,615
다른 노이즈가 들어가면
다른 gaussian distribution이 생성될 수 있습니다.

186
00:11:44,640 --> 00:11:50,170
이론적으로 실제로 그런 모델이
임의의 분포를 나타낼 수 있다는 것을 보여줄 수 있습니다.

187
00:11:50,195 --> 00:11:53,266
그러나 그 모델을 training하는 것은
조금 까다로울 수 있습니다.

188
00:11:53,291 --> 00:11:57,472
그래서 저는 latent variable model을 훈련시키는 방법에 대한
수학에 대해 자세히 설명하지 않을 것 입니다.

189
00:11:57,497 --> 00:12:02,865
그렇지만 수업 후반부에 variational autoencoders에 대한
강의를 할 것 입니다.

190
00:12:02,890 --> 00:12:05,575
하지만 지금은 variable models에 관심이 있으면

191
00:12:05,600 --> 00:12:09,420
conditional variational autoencoder이나

192
00:12:09,440 --> 00:12:15,120
flow 정규화나
stein variational gradient descent을 찾아보세요.

193
00:12:16,000 --> 00:12:17,944
마지막 옵션은

194
00:12:17,969 --> 00:12:24,285
단순성과 표현성 사이에 좋은 균형을 이룬다고 생각하는
autoregressive discretization 입니다.

195
00:12:24,310 --> 00:12:26,460
mixture of gaussians은 매우 간단하지만

196
00:12:26,480 --> 00:12:29,500
매우 복잡한 분포에는 어려움이 있습니다.

197
00:12:29,519 --> 00:12:33,494
latent variable model은 표현력이
뛰어나지만 적용하기 복잡합니다.

198
00:12:33,519 --> 00:12:37,549
다른 regressive discretization이
둘 사이의 좋은 중간 지점일 것입니다.

199
00:12:37,574 --> 00:12:41,850
임의의 분포를 나타낼 수 있고, 제 생각에는,

200
00:12:41,875 --> 00:12:43,962
latent variable models보다 사용하기가 훨씬 쉽습니다.

201
00:12:44,398 --> 00:12:47,833
따라서 Autoregressive discretization의 아이디어는 다음과 같습니다.

202
00:12:47,858 --> 00:12:50,285
만약 우리가 discrete action을 한다면,

203
00:12:50,310 --> 00:12:52,785
이 multimodality problem는 문제가 아닙니다.

204
00:12:52,810 --> 00:12:58,695
softmax 범주형 분포가 어떤 분포도 쉽게 나타낼 수 있기 때문이죠.

205
00:12:58,720 --> 00:13:01,040
그러나 continuous action일 때는,

206
00:13:01,040 --> 00:13:03,415
이를 이산화하는 것(discretizing)이 어려울 수 있습니다.

207
00:13:03,440 --> 00:13:06,855
일반적으로 이산화하는데 필요한 bin의 수는

208
00:13:06,880 --> 00:13:10,319
n차원 action space에서
n의 지수 입니다.

209
00:13:10,320 --> 00:13:14,215
따라서 steering and gas / brake와
같은 두 가지 동작 차원만 있으면

210
00:13:14,240 --> 00:13:19,335
수행하기 쉽지만 더 많은 차원이
있으면 바로 비실용적이게 됩니다.

211
00:13:19,360 --> 00:13:21,254
autoregressive discretization는

212
00:13:21,279 --> 00:13:23,900
한 번에 한 차원을 이산화(discretize)합니다.

213
00:13:23,920 --> 00:13:27,735
그러나 영리한 neural network trick을 사용하여

214
00:13:27,760 --> 00:13:31,120
임의의 distribution를 나타낼 수 있습니다.

215
00:13:31,600 --> 00:13:36,562
그래서 먼저 action의 첫 번째 차원을 이산화하고

216
00:13:36,587 --> 00:13:41,909
이미지를 가져와
action의 첫번째 차원의 이산화를 출력하는

217
00:13:41,934 --> 00:13:44,860
neural net을 갖게 될 것입니다.

218
00:13:44,885 --> 00:13:49,720
그런다음
softmax에서 샘플을 뽑을 것입니다.

219
00:13:49,760 --> 00:13:53,900
그러면 첫 번째 action 차원에 대한 value을 갖게 될 것입니다.

220
00:13:53,920 --> 00:13:58,855
이 value을 distribution를 출력할
다른 neural net에 입력합니다.

221
00:13:58,880 --> 00:14:01,316
두 번째 action 차원에서도

222
00:14:01,341 --> 00:14:03,399
이런 샘플링하고 하는 과정을 반복합니다.

223
00:14:03,440 --> 00:14:06,339
그래서 한 번에 한 차원을 이산화하여,

224
00:14:06,399 --> 00:14:09,700
즉, 그 말은 우리가 기하급수적인 cost를
부담할 필요가 없다는 것을 의미합니다.

225
00:14:09,760 --> 00:14:14,380
그러나 우리는 이전 조건에서
다음 차원 조건에 대한 distribution을 모델링하기 때문에

226
00:14:14,399 --> 00:14:18,700
확률의 chain rule에 의해
실제로 full joint distribution를 나타낼 수 있습니다.

227
00:14:18,720 --> 00:14:21,600
어떤 action dimension에서든지

228
00:14:21,600 --> 00:14:23,600
구현하기 쉬운 트릭입니다.

229
00:14:23,600 --> 00:14:28,460
실제로 적용하기에 매우 좋습니다.

230
00:14:28,959 --> 00:14:33,340
자, 요약하자면

231
00:14:33,360 --> 00:14:36,215
우리는 imitation learning 자체가

232
00:14:36,240 --> 00:14:38,934
behavioral cloning 이며,
항상은 아니지만

233
00:14:38,959 --> 00:14:42,299
종종 distribution mismatch 문제로
부족하다고 이야기했습니다.

234
00:14:42,320 --> 00:14:44,407
그러나 때로는 잘 작동합니다.

235
00:14:44,432 --> 00:14:48,399
nvidia 논문의 예에서와 같이 잘 작동할 수 있습니다.

236
00:14:48,399 --> 00:14:51,655
시스템을 안정화하기 위해 몇 가지 트릭을 사용하거나

237
00:14:51,680 --> 00:14:56,215
보다 일반적으로 샘플이
안정적인 trajectory distribution에서 나오거나

238
00:14:56,240 --> 00:15:00,934
또는 예를 들어 DAgger을 사용하여
더 많은 on-policy 데이터를 추가한다면요.

239
00:15:00,959 --> 00:15:07,174
또한 다양한 트릭들을 할 수 있다면
그 트릭들을 추가할 수 있습니다.

240
00:15:07,199 --> 00:15:10,840
모델을 개선하여 데이터에 훨씬 더 정확하게 맞도록하기 위해서요.

241
00:15:10,880 --> 00:15:14,480
이론적으로는 distributional shift 문제를 완화하지 않지만

242
00:15:14,480 --> 00:15:18,455
적용 측면에서는 naive behavior cloning도
가능하게 할 수 있습니다.

243
00:15:18,480 --> 00:15:22,054
우리가 non-markovian policy들과

244
00:15:22,079 --> 00:15:27,839
multimodality를 다루는 트릭을
잘 신경쓴다면 말이죠.

