1
00:00:00,880 --> 00:00:06,160
이 로봇에게 어떻게 물체를 집어야 하는지
알려준다고 해봅시다

2
00:00:06,160 --> 00:00:08,160
그러기 위해서는 어떤 시스템을
만들어야 하는데, 이 시스템은

3
00:00:08,160 --> 00:00:10,400
로봇의 카메라를 통해 이미지를 입력받고

4
00:00:10,400 --> 00:00:12,799
집게를 적절한 위치로 옮긴 뒤

5
00:00:12,799 --> 00:00:14,160
물체를 집어 통 밖으로 꺼내도록 해야 합니다

6
00:00:14,160 --> 00:00:17,279
예를 들면 이 시스템은 창고 관리에 쓸 수 있지요

7
00:00:17,279 --> 00:00:21,439
정리하자면 이미지가 들어오고, 그걸 우리가 만들 방법론에 넣어서

8
00:00:21,439 --> 00:00:24,560
xyz 좌표를 만들어낸 다음

9
00:00:24,560 --> 00:00:28,240
로봇이 집게를 움직여 물체를 집도록 합니다

10
00:00:28,240 --> 00:00:32,800
그렇다면 이러한 과제를 해결하기 위해 여러가지 접근법을 떠올릴 수 있을 것 같은데요

11
00:00:32,800 --> 00:00:36,719
예를 들면 우리가 가진 로봇 시스템 관련 지식을 활용해서

12
00:00:36,719 --> 00:00:39,040
집게의 좌표를 측정하고

13
00:00:39,040 --> 00:00:41,120
이 좌표와 카메라가 어떻게 관련되어 있는지 측정하고

14
00:00:41,120 --> 00:00:43,260
3D 재구성 알고리즘을 만들어

15
00:00:43,260 --> 00:00:45,120
물체의 기하학적 구조를 알아내

16
00:00:45,120 --> 00:00:48,879
물체를 집을 때마다 집게를 적절하게 위치시킬 수 있습니다

17
00:00:48,879 --> 00:00:50,719
어떤 경우에는 이 방법이 잘 통할 수 있습니다

18
00:00:50,719 --> 00:00:53,690
예를 들면 이렇게 생긴 딱딱한 물체를 집는 경우에는요

19
00:00:53,690 --> 00:00:57,960
아마 로봇은 집게를 적절한 위치로 옮겨 물체를 잡을 수 있습니다

20
00:00:57,960 --> 00:01:01,680
그러나 어떤 물체들은 좀 더 많은 지식을 활용해야 합니다

21
00:01:01,680 --> 00:01:03,199
예를 들면 이것처럼 크고 무거운 물체의 경우에요

22
00:01:03,199 --> 00:01:06,799
이 경우 질량의 중심이 특정 부분 주위에 집중되어 있다는 것을 알아내야 합니다

23
00:01:06,799 --> 00:01:08,560
그리고 적절하게 물체를 집어야 하죠

24
00:01:08,560 --> 00:01:11,470
그리고 어떤 물체들은 우리의 예상을 뒤엎습니다

25
00:01:11,470 --> 00:01:14,880
예를 들어 부드럽고 변형 가능한 물체를 잡아야 한다면

26
00:01:14,880 --> 00:01:18,400
중간 부분을 꼬집는 것처럼 잡는 방식이 물체를 집는 데 있어 훨씬 더 좋은 방법입니다

27
00:01:18,400 --> 00:01:22,960
단단한 물체를 집는 방식에 비해서요

28
00:01:22,960 --> 00:01:27,360
다른 방법은 이것을 learning problem으로 생각하는 것입니다

29
00:01:27,360 --> 00:01:31,280
그러나 여기에 standard supervised machine learning 방법론을 적용하려면

30
00:01:31,280 --> 00:01:33,300
몇 가지 해결해야 할 점들이 있습니다

31
00:01:33,300 --> 00:01:38,560
standard supervised machine learning 방법론들은 다음과 같은 것을 필요로 하는데요

32
00:01:38,560 --> 00:01:45,150
누군가 labeled tuple of images를 만들고, 로봇이 성공적으로 물체를 집기 위한
 ground truth xyz 좌표 위치를 제공해야 합니다

33
00:01:45,150 --> 00:01:50,799
그런데 우리는 로봇이 아니기 때문에 이런 것들을 하기 어렵습니다

34
00:01:50,799 --> 00:01:56,640
우리는 로봇이 물체를 집기 위해 집게를 어디에 위치시켜야 하는지에 대한 좋은 직관이 없습니다

35
00:01:56,640 --> 00:02:02,180
그래서 이 수업의 주제는 이런 종류의 기술을 습득하는 과정을 자동화할 수 있는

36
00:02:02,180 --> 00:02:07,600
reinforcement learning 방법론을 개발하는 것입니다.

37
00:02:07,600 --> 00:02:11,440
개괄적으로 설명하자면 강화학습이란 다음과 같이 생각할 수 있는데요

38
00:02:11,440 --> 00:02:14,879
데이터를 활용하는데 이것이 꼭 ground truth data여야만 하는 것은 아니고,

39
00:02:14,879 --> 00:02:18,240
또 좋은, 성공적인 행동 데이터여야만 하는 것도 아니지만,

40
00:02:18,240 --> 00:02:23,000
데이터가 현실에서 일어날 수 있는 결과에 대한 
경험을 제공해줄 수 있으면 됩니다

41
00:02:23,000 --> 00:02:27,440
그럼 만약 이 모든 로봇들이 다양한 종류의 물체를 집는다고 할 때

42
00:02:27,440 --> 00:02:30,180
성공하거나 실패하거나 하며 경험을 쌓을 수 있다면

43
00:02:30,180 --> 00:02:35,360
이 경험들을 물체 집기 시스템에 활용할 수 있을까요?

44
00:02:35,360 --> 00:02:41,000
이 경우에는 사람이 데이터를 제공하는 것이 아니라 데이터를 자동으로 수집하는 것에서 출발합니다

45
00:02:41,000 --> 00:02:46,480
그리고 각 데이터는 동일한 xyz tuple로 이루어진 이미지들로 구성되어 있습니다

46
00:02:46,480 --> 00:02:50,560
이 tuple 데이터들은 꼭 성공한 데이터여야만 하는 것은 아닙니다

47
00:02:50,560 --> 00:02:56,319
대신 그 결과가 성공이냐 실패냐에 따라 그 결과 데이터가 라벨링되어

48
00:02:56,319 --> 00:02:59,200
reinforcement learning algorithm에 제공되고

49
00:02:59,200 --> 00:03:07,120
로봇들이 데이터를 수집할 때 했던 것보다 
이 과제를 잘 수행할 수 있도록 하는 policy를 만드는 데 쓰입니다

50
00:03:07,120 --> 00:03:10,560
reinforcement learning algorithm이 아직 끝난 것은 아닙니다

51
00:03:10,560 --> 00:03:14,080
지금까지의 결과로 나온 policy가 현실에 배포되어

52
00:03:14,080 --> 00:03:18,959
추후 행동을 더 개선시킬 수 있도록 추가 데이터를 수집하게 되죠

53
00:03:18,959 --> 00:03:21,480
그래서 reinforcement learning란 무엇이냐

54
00:03:21,480 --> 00:03:25,300
본질적으로 reinforcement learning은 2가지인데요

55
00:03:25,300 --> 00:03:28,480
학습 기반의 의사 결정 방법을 수학적으로 공식화한 것입니다

56
00:03:28,480 --> 00:03:31,200
이를 통해 우리가 알고리즘을 설계할 수 있도록 해주죠

57
00:03:31,200 --> 00:03:35,519
그리고 데이터로부터의 경험을 통해 의사결정이나 제어 방법을 학습할 수 있도록 하는 접근법입니다

58
00:03:35,519 --> 00:03:42,159
제어기 또는 수동으로 설계한 policy에 의존하는 것 대신에요

59
00:03:42,480 --> 00:03:46,640
reinforcement learning이 다른 machine learning 주제들과는 어떻게 다른가 하면

60
00:03:46,640 --> 00:03:49,680
만약 우리가 standard (supervised) machine learning을 생각해보면

61
00:03:49,680 --> 00:03:53,519
일반적으로 몇 가지 가정을 합니다. 예를 들어 우리에게

62
00:03:53,519 --> 00:03:57,599
input x와 output y로 구성된 데이터가 주어지고

63
00:03:57,599 --> 00:04:04,560
우리의 목표가 x로부터 y를 예측할 수 있도록 학습하는 것이라 하면
 이는 본질적으로 y를 결과값으로 가지는 x에 대한 함수 f를 구하는 것이죠

64
00:04:04,560 --> 00:04:07,760
이 때 우리는 데이터가 실제로 라벨링되어 있을 것이라 가정할 뿐만 아니라

65
00:04:07,760 --> 00:04:13,439
해당 데이터가 i.i.d. (independent and identically distributed) 가정을 만족한다고 가정합니다

66
00:04:13,439 --> 00:04:18,160
이는 주어진 input x를 가지고 만든 output y가

67
00:04:18,160 --> 00:04:21,519
다른 input spec에 대해 어떠한 영향도 미치지 않는다는 것을 의미합니다

68
00:04:21,519 --> 00:04:26,320
또한 우리는 학습 과정에서 ground truth output을 알고 있을 거라 가정해야만 하는데요

69
00:04:26,320 --> 00:04:30,720
이와 반대로 reinforcement learning은 데이터가 i.i.d가 아닌 상황을 다루는데

70
00:04:30,720 --> 00:04:34,880
과거의 output이 미래의 input에 영향을 주기 때문입니다

71
00:04:34,880 --> 00:04:38,880
그래서 이전에 했던 action이 미래에 관찰할 수 있는 대상에 영향을 줍니다

72
00:04:38,880 --> 00:04:41,360
그리고 ground truth answer는 일반적으로 알 수 없습니다

73
00:04:41,360 --> 00:04:49,360
사실 그 대신에 알아내야 하는 것은 성공 또는 실패 여부이고, 
혹은 좀 더 일반적으로 말하자면 어떤 reward를 얻었는지입니다

74
00:04:49,360 --> 00:04:54,880
그래서 우리는 reinforcement learning에서 agent와 environment가 상호작용하는 의사결정 시스템을 모델링합니다

75
00:04:54,880 --> 00:04:59,120
agent는 우리가 action이라 부르는 결정을 만들어내고

76
00:04:59,120 --> 00:05:01,560
environment는 이런 결정들에 대해 반응하여

77
00:05:01,560 --> 00:05:04,820
state나 reward 같은 observation들을 제공합니다

78
00:05:04,820 --> 00:05:10,240
그리고 결정적으로 이러한 과정은 한 episode에서 여러 번 반복됩니다

79
00:05:10,240 --> 00:05:15,440
reinforcement learning에는 이것이 유한 번 반복되는 finite horizon 방식과

80
00:05:15,440 --> 00:05:21,360
의사결정 주기가 무한히 반복되는 infinite horizon 방식이 있습니다

81
00:05:21,919 --> 00:05:27,919
여기 우리가 어떻게 현실의 문제를 reinforcement framework로 가져올 수 있는지에 대한 몇 가지 예시가 있습니다

82
00:05:27,919 --> 00:05:31,759
만약 개에게 어떤 행동을 가르치고 싶다면

83
00:05:31,759 --> 00:05:34,720
아마 개의 action은 근육의 수축이 될 것이고

84
00:05:34,720 --> 00:05:37,500
개의 observation은 개가 느끼는 시각적 자극이나 냄새로 이루어질 것입니다

85
00:05:37,500 --> 00:05:44,320
그리고 reward는 우리가 원하는 것을 개가 했을 때 개에게 주는 간식이거나, 
우리가 원하는 것을 하지 않을 경우 간식을 주지 않는 것이 됩니다

86
00:05:44,320 --> 00:05:49,000
만약 로봇을 reinforcement learning을 활용해 학습시키고 싶다면

87
00:05:49,000 --> 00:05:52,880
아마 로봇의 action은 모터의 전류나 회전이 될 것이고

88
00:05:52,880 --> 00:05:57,759
observation은 로봇의 카메라 같은 센서로부터 들어오는 입력,

89
00:05:57,759 --> 00:06:01,840
reward는 과제를 성공적으로 수행한 정도가 됩니다

90
00:06:01,840 --> 00:06:03,680
예를 들어 로봇에게 달리는 것을 가르치고 싶다면

91
00:06:03,680 --> 00:06:06,880
성공적으로 수행한 정도는 얼마나 빠르게 달렸는가를 측정하여 구할 수 있습니다

92
00:06:06,880 --> 00:06:10,319
그러나 reinforcement learning은 단순히 개나 로봇에 한정되지 않습니다

93
00:06:10,319 --> 00:06:16,720
reinforcement learning은 재고관리와 같은 
운용과학(Operation Research) 분야의 문제에도 적용할 수 있습니다

94
00:06:16,720 --> 00:06:20,160
이 문제에서 action은 무엇을 구매할지에 대한 결정들로 이루어지고

95
00:06:20,160 --> 00:06:23,840
observation은 각 시점의 재고 수준으로 구성됩니다

96
00:06:23,840 --> 00:06:29,080
그리고 reward는 아마 얻을 수 있는 순이익으로 구성되겠죠

97
00:06:29,080 --> 00:06:35,600
그래서 이와 같은 문제 또는 다른 문제에 reinforcement learning을 적용해볼 수 있습니다

98
00:06:35,600 --> 00:06:39,520
복잡한 물리적 과제를 reinforcement learning을 활용해 학습하는 것도 생각해 볼 수 있겠죠

99
00:06:39,520 --> 00:06:45,039
예를 들어 로봇 손을 활용해 망치로 못을 박거나 하는 식으로요

100
00:06:45,039 --> 00:06:51,199
이 비디오처럼 reinforcement learning을 사용하여 예상치 못한 해결책을 자동으로 찾도록 하는 것도 생각해볼 수 있습니다

101
00:06:51,199 --> 00:06:54,960
Q-learning algorithm을 활용하여 벽돌깨기를 하는 영상인데요

102
00:06:54,960 --> 00:06:58,880
처음에는 학습된 policy가 예상할 수 있는 방식으로 게임을 하지만

103
00:06:58,880 --> 00:07:03,440
나중에는 공을 위쪽으로 올릴 경우 한동안 위에서 공이 튄다는 것을 발견하게 됩니다

104
00:07:03,440 --> 00:07:06,960
다른 방식으로 얻을 수 있는 점수보다 훨씬 더 많은 점수를 얻을 수 있죠

105
00:07:06,960 --> 00:07:10,240
우리는 실세계에서 reinforcement learning을 사용할 수 있습니다

106
00:07:10,240 --> 00:07:13,599
다시 물건 집는 로봇 예시로 돌아가보죠

107
00:07:13,599 --> 00:07:18,080
다양한 물체를 집는 연습을 하는 로봇을 사용할 수 있습니다.

108
00:07:18,080 --> 00:07:22,240
그리고 그러한 연습을 함으로써 복잡한 기술을 하는 법을 알아낼 수 있습니다

109
00:07:22,240 --> 00:07:26,639
예를 들어 만약 물체가 가까이 위치해 있다면 조금 밀어서 서로 떨어지게 한다던가 말이죠 

110
00:07:26,639 --> 00:07:30,080
그리고 나서 각 물체를 집고

111
00:07:30,080 --> 00:07:34,800
각 조각들을 하나하나 떨어뜨려 놓을 수 있죠

112
00:07:38,160 --> 00:07:41,440
그럼 다음과 같은 것도 알 수 있는데

113
00:07:41,440 --> 00:07:46,240
만약 물체를 집으려는 시도가 성공하면 바로 치우면 되지만

114
00:07:46,240 --> 00:07:50,400
만약 물체를 집는 데 실패하고 놓치게 되면

115
00:07:50,400 --> 00:07:59,840
집게를 다시 위치시켜 다른 시도를 하는 것이 좋습니다

116
00:08:09,280 --> 00:08:12,639
또한 이런 policy를 학습하여

117
00:08:12,639 --> 00:08:14,610
다양한 종류의 다른 물체를 집을 수 있게 되는데

118
00:08:14,610 --> 00:08:19,440
computer vision 응용 분야에서의 일반화를 볼 수 있는 것과 같은 방식으로

119
00:08:19,440 --> 00:08:22,720
reinforcement learning system에서도 일반화를 볼 수 있습니다

120
00:08:22,720 --> 00:08:31,120
심지어 누군가 집게가 잡은 물체를 건드려서 다시 시도해야 하는, 방해하는 경우에도 generalization이 가능합니다

121
00:08:31,120 --> 00:08:36,920
하지만 물론 게임이나 로봇에만 해당하는 것은 아닙니다
아까 언급한 것처럼 재고 관리 같은 분야에 reinforcement learning을 사용할 수 있습니다

122
00:08:36,920 --> 00:08:43,360
Cathy Wu의 연구가 그 예시입니다
여기 Berkeley에서 박사과정을 하고 현재는 MIT에 교수로 가 있지요

123
00:08:43,360 --> 00:08:50,880
교통정리에 reinforcement learning을 적용한 것인데요
이 비디오를 보면 하얀색 차들은 사람이 운전하는 것을 시뮬레이션한 것입니다

124
00:08:50,880 --> 00:08:53,440
그리고 빨간색 차는 자율주행 차입니다

125
00:08:53,440 --> 00:09:00,720
이 차의 목표는 이 모든 차들이 최대한 원활하게 원을 돌며 주행할 수 있도록 교통정리를 하는 것입니다

126
00:09:00,720 --> 00:09:05,920
그리고 여기서 볼 수 있는 것은 자율주행차가 실제로 동적으로 의사결정을 하여

127
00:09:05,920 --> 00:09:11,279
속도를 줄이거나 높여서 전반적인 교통 수준을 유지한다는 것입니다

128
00:09:11,279 --> 00:09:15,440
그래서 처음에는 약간의 교통체증이 자동적으로 발생하지만 시간이 지나면

129
00:09:15,440 --> 00:09:19,680
이 자율주행차가 교통의 흐름을 조절해는 방법을 알아내고

130
00:09:19,680 --> 00:09:23,040
속도를 조절하여 그 앞에 있는 차와 좀 더 거리를 둡니다

131
00:09:23,040 --> 00:09:29,360
그리고 그 결과 전체적인 차들이 훨씬 더 원활하게 주행할 수 있도록 해줍니다

132
00:09:29,360 --> 00:09:33,680
이건 좀 더 복잡한 예시인데
8자형 도로가 있고

133
00:09:33,680 --> 00:09:38,720
처음에는 하얀색 차만 보이실 겁니다
무인자동차는 여기 없습니다

134
00:09:38,720 --> 00:09:42,800
즉 이것은 사람들이 수동으로 운전하는 것을 시뮬레이션한 것인데 이 교차로 부분에 모여들게 되죠

135
00:09:42,800 --> 00:09:46,800
그리고 결국 서로를 기다리며 꽤나 시간을 쓰게 됩니다

136
00:09:48,399 --> 00:09:53,519
그러나 우리가 빨간색 자율주행차를 투입해보면

137
00:09:53,519 --> 00:09:57,600
이 자율주행차가 교통정리 작업을 수행하기 시작해

138
00:09:57,600 --> 00:10:01,600
모두가 일정한 속도로 가게 만들죠 
이것이 실제로 하는 일은 잠시 기다려서

139
00:10:01,600 --> 00:10:07,600
가장 마지막에 있는 차가 교차로를 지나간 후 
차들의 대열이 그 시간에 맞춰 교차로를 통과하도록 하는 것입니다

140
00:10:07,600 --> 00:10:12,720
이를 통해 아무도 정지 표지판이 있는 교차로에서 기다릴 필요가 없게 됩니다

141
00:10:13,360 --> 00:10:17,440
좋습니다

