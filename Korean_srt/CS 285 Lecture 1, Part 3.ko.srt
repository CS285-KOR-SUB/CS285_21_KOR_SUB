1
00:00:01,199 --> 00:00:03,199
실제 세상에 있는 순차적 의사 결정을 하기 위해

2
00:00:03,199 --> 00:00:05,802
우리가 풀어야 할 다른 문제들은 무엇이 있을까요

3
00:00:06,480 --> 00:00:09,480
강화학습은 보상을 최대화하는 문제를 다루므로

4
00:00:09,519 --> 00:00:13,019
그래서 강화학습은 시스템과 상호 작용할 수 있다고 가정합니다.

5
00:00:13,019 --> 00:00:16,827
강화학습은 ground truth은 아니지만
ground truth 보상에 대한 지도를 받고

6
00:00:16,852 --> 00:00:19,575
그 보상들을 최대화 하기 위한

7
00:00:19,607 --> 00:00:22,028
행동들을 할 수 있도록 하는 
적절한 출력을 알아내야 합니다.

8
00:00:22,250 --> 00:00:26,144
그러나 이것만이 순차적 의사 결정에서의 
유일한 문제는 아닙니다.

9
00:00:26,400 --> 00:00:29,482
이번 수업에서 다룰 심화 주제들로는

10
00:00:29,553 --> 00:00:33,050
예들로 부터 보상 함수를 배우는

11
00:00:33,120 --> 00:00:35,761
inverse reinforcement learning
(역강화학습)이 있습니다.

12
00:00:35,994 --> 00:00:38,583
서로 다른 도메인 간에 지식을 전수하는

13
00:00:38,666 --> 00:00:40,849
transfer learning(전이학습)
또는 meta learning(메타학습)도 있습니다.

14
00:00:40,889 --> 00:00:45,975
이 방법론은 이전에 해결할 수 없었던 
새로운 과제를 해결하기 위해

15
00:00:46,000 --> 00:00:49,312
이 과제와는 다르지만 관련된 과거 경험을 어떻게
사용하는지에 대한 질문을 다룹니다.

16
00:00:50,079 --> 00:00:52,835
예측을 학습하고 
예측을 사용하여 행동하는 것을

17
00:00:52,914 --> 00:00:55,575
model based reinforcement learning
(모델 기반 강화 학습)이라고 합니다.

18
00:00:55,600 --> 00:00:59,279
행동을 직접 배우는 것 대신에

19
00:00:59,280 --> 00:01:01,341
세상이 어떻게 작동하는지에 대한 표현들을 학습합니다.

20
00:01:01,366 --> 00:01:06,776
이 표현들은 policy를 만들거나 
계획을 세우는데 사용될 수 있습니다.

21
00:01:07,065 --> 00:01:10,377
다음으로는 수업의 후반부에서 다룰 한가지 질문입니다.

22
00:01:10,479 --> 00:01:12,221
보상은 어디에서 올까요?

23
00:01:12,525 --> 00:01:17,199
처음에 이야기한 grasping robot 같은 경우에는

24
00:01:17,224 --> 00:01:21,123
무언가를 잘 잡는지에 따라 
보상이 결정되는 것이 매우 당연합니다.

25
00:01:21,226 --> 00:01:23,226
만약 당신이 개를 훈련시키고 있다면,

26
00:01:23,360 --> 00:01:26,478
아마도 개의 보상 함수는 
받는 간식에 달려 있을 것입니다.

27
00:01:26,880 --> 00:01:28,880
그러나 보상이 어디에서 왔는지 좀 더 확장해봅시다.

28
00:01:30,079 --> 00:01:33,079
아타리 게임을 배워보면 아마도 답은 명확할 것입니다.

29
00:01:33,348 --> 00:01:36,801
하지만 만약 물 한 컵 부어주는 로봇이 필요하다면,

30
00:01:36,880 --> 00:01:39,724
이것은 어떤 어린이라도 할 수 있는 일이지만,

31
00:01:39,777 --> 00:01:42,552
로봇의 경우에는 컵에 물을 잘 부었는지

32
00:01:42,577 --> 00:01:46,577
판단하기 위해 통합적인 인지 시스템이 필요합니다.

33
00:01:48,560 --> 00:01:53,910
버클리 인공지능 연구소의 동료들이

34
00:01:53,935 --> 00:01:55,935
몇 년 전에 발표한 논문이 있었습니다.

35
00:01:55,960 --> 00:02:00,054
그리고 그 논문 마지막 부분에서 
human agents인 우리는

36
00:02:00,079 --> 00:02:02,299
보상을 기반으로 수행하는데 익숙하지만

37
00:02:02,324 --> 00:02:07,324
보상은 너무 희박해서 
평생에 한 두 번만 경험하게 된다고 말하고 있습니다.

38
00:02:07,973 --> 00:02:11,673
그리고 reddit의 누군가는 
"나는 저자가 불쌍하다"라고 했습니다.

39
00:02:12,560 --> 00:02:16,517
그러나 다소 비극적이지만, 
이 말은 우리가 현실에서

40
00:02:16,542 --> 00:02:18,584
실제로 해결해야 하는 많은 과제들을 
설명해주고 있습니다.

41
00:02:18,640 --> 00:02:21,960
지금 이 수업을 듣는 분들은

42
00:02:22,080 --> 00:02:27,655
나중에 지연된 보상을 받을 것이기 때문에
이 수업을 듣는다고 말할 수도 있습니다.

43
00:02:27,680 --> 00:02:30,767
여러분이 컴퓨터 과학 학위로 졸업할 때

44
00:02:30,879 --> 00:02:34,315
시행착오를 통해 학위를

45
00:02:34,480 --> 00:02:37,551
취득하지는 못할 것입니다.

46
00:02:37,576 --> 00:02:40,576
학위 취득은 일생에 한 번뿐일 일이기 때문입니다.

47
00:02:41,920 --> 00:02:46,559
우리는 뇌에 기저 신경절(basal ganglia)과 
같은 보상 메커니즘을

48
00:02:46,560 --> 00:02:49,053
담당하는 구조가 있다는 것을 알고 있습니다.

49
00:02:49,077 --> 00:02:53,199
그리고 이는 복잡한 구조로 되어 있어서
뇌에서 일어나는 보상 체계는

50
00:02:53,200 --> 00:02:56,057
특정한 간단한 사건을 다루는
스위치 같지 않습니다.

51
00:02:56,082 --> 00:03:00,710
적절한 행동을 했을 때에 대한 적절한 보상을 주는

52
00:03:00,735 --> 00:03:02,171
뇌의 기제들이 많이 있습니다.

53
00:03:02,196 --> 00:03:04,528
이것이 왜 필요한지에 대해 알아보는 것은 
어렵지 않습니다.

54
00:03:04,553 --> 00:03:08,829
대학 학위 예 외에도

55
00:03:08,854 --> 00:03:13,241
가젤을 쫓는 치타와 같은 
더 자연스러운 예를 상상할 수 있습니다.

56
00:03:13,638 --> 00:03:19,919
치타가 아무렇게나 날뛰다가 우연히 가젤을 낚아채게 되고

57
00:03:19,920 --> 00:03:23,815
이 사건으로부터 그런 행동을 더 자주 해야
보상을 얻는다라는 것을 알게 된다는 것은 말이 안됩니다.

58
00:03:23,840 --> 00:03:26,251
치타가 수행하기에 꽤 어려운 행동이기 때문입니다.

59
00:03:26,276 --> 00:03:31,180
이 행동을 하게 된 원인에 대한

60
00:03:31,205 --> 00:03:33,244
더 정교하고 자세한 안내가 필요합니다.

61
00:03:34,959 --> 00:03:36,959
그래서 이런 매우 드물고 희소한 보상들 외에

62
00:03:36,984 --> 00:03:41,182
다른 형태의 supervision(지시)가 있을까요?

63
00:03:41,599 --> 00:03:46,479
데모로부터 어떤 행동들의 표현들을 얻을 수 있습니다.

64
00:03:46,480 --> 00:03:49,162
관찰한 행동을 직접적으로 따라하거나

65
00:03:49,187 --> 00:03:53,375
또는 그 행동에서 더 자세한 보상 함수를 추론하면서 말이죠.

66
00:03:53,400 --> 00:03:55,209
이 방법이 inverse reinforcement learning 
(역강화학습)입니다.

67
00:03:55,265 --> 00:04:00,238
그래서 아마도 그 치타는
어떤 노련한 치타가 가젤을 쫓는 것을 보았을 것이고

68
00:04:00,239 --> 00:04:02,667
그때 보상을 받을 수 있는

69
00:04:02,692 --> 00:04:05,739
몇 가지 행동들이 있다는 것을
깨달았을 것입니다.

70
00:04:07,120 --> 00:04:09,786
또한, 세상에 일어나고 있는 일들을 관찰하고 배우면서

71
00:04:09,811 --> 00:04:12,945
미래 사건들을 예측하는 것을 학습할 수 있습니다.

72
00:04:12,993 --> 00:04:17,015
만약 실제 세상의 물리적 인과 구조에 대해 뭔가를 배운다면,

73
00:04:17,040 --> 00:04:19,976
배운 지식을 가지고

74
00:04:20,001 --> 00:04:22,809
어떤 행동이 원하는 결과로 이어지는 지 파악할 수 있습니다.

75
00:04:24,400 --> 00:04:27,835
비지도 학습 방법이 이 문제의 일부를 다룹니다.

76
00:04:27,860 --> 00:04:29,947
또한, 다른 과제로부터도 배울 수 있습니다.

77
00:04:29,972 --> 00:04:34,135
아마 이 특정 과제는 해결하지 못했을 수도 있지만,

78
00:04:34,160 --> 00:04:36,302
이처럼 아마 이전에 컴퓨터 공학 학위를 
받은 적은 없겠지만,

79
00:04:36,327 --> 00:04:39,015
그러나 구조적으로 관련된 다른 과제를 수행했었을 겁니다.

80
00:04:39,040 --> 00:04:43,365
고등학교에 다니고 고등학교 학위를 취득했을 것입니다.

81
00:04:43,390 --> 00:04:49,770
그리고 그때 어떻게 교육을 받는지,
어떻게 시험을 통과하는 지에 대해 어느 정도 이해했을 겁니다.

82
00:04:50,320 --> 00:04:52,772
이것을 transfer learning(전이학습)이라고 합니다.

83
00:04:53,002 --> 00:04:56,719
보다 정교한 버전으로는
meta learning(메타 학습)이라고 하는데,

84
00:04:56,720 --> 00:04:59,504
단순히 새로운 일을 잘하는 것뿐만 아니라

85
00:04:59,529 --> 00:05:01,711
실제로 새로운 일을 습득하는 것을 더 잘하게 됩니다.

86
00:05:01,736 --> 00:05:03,616
진짜로 배우는 법을 배우는 것이죠.

87
00:05:03,641 --> 00:05:07,554
이건 이전 수업을 통해 모두 배운 것들입니다.

88
00:05:10,080 --> 00:05:14,478
이 카테고리에 있는 방법론들은 
꽤 인상적인 것들을 보여주었습니다.

89
00:05:14,479 --> 00:05:18,479
이것은 약 4년 전에 nvidia에서 공개한 영상인데요.

90
00:05:18,504 --> 00:05:23,439
자율 주행을 위한 imitation learning(모방 학습) 시스템을 보여주고 있습니다.
강화 학습 시스템은 아니기에,

91
00:05:23,440 --> 00:05:29,796
reward supervision으로 배우지 않고
실제 인간 운전자를 모방하기 위해 학습합니다.

92
00:05:31,440 --> 00:05:35,241
다음 주에 imitation learning(모방 학습)에 대해
자세히 이야기하겠습니다.

93
00:05:37,120 --> 00:05:43,468
그런데, 유용한 행동을 하는 
다른 에이전트들을 관찰하는 것은

94
00:05:43,493 --> 00:05:46,516
직접 모방하는 것 이상으로 활용될 수 있습니다.

95
00:05:46,541 --> 00:05:49,905
이건 아동 심리학 실험 영상입니다.

96
00:05:49,930 --> 00:05:52,977
한 남자는 책을 치워 두려고 합니다.

97
00:05:53,002 --> 00:05:55,652
그리고 이 아이는 이 사람을 흉내내지 않고

98
00:05:55,677 --> 00:05:58,175
그가 하는 일을 지켜볼 것입니다.

99
00:05:58,200 --> 00:05:59,961
대신에 아이는

100
00:05:59,986 --> 00:06:02,747
이 사람이 하려고 하는 것이 무엇인지를 추론하고,

101
00:06:02,772 --> 00:06:06,549
이 사람이 하려고 하는 작업을 수행하지만, 
더 최적의 방법으로 하게 됩니다.

102
00:06:06,574 --> 00:06:09,240
그래서 누군가 무엇을 하는 것을 관찰하고,

103
00:06:09,265 --> 00:06:13,279
그 사람이 하려는 목표를 알게 되면,
아마도 당신은 그 목표를 더 효과적으로 수행할 수 있을 겁니다.

104
00:06:13,280 --> 00:06:17,073
추론한 목표를 가지고 약간의 강화 학습을 하면서 말이죠.

105
00:06:17,955 --> 00:06:20,859
물론, 이것을 실제 세상에서의 알고리즘으로 바꿀 수 있습니다.

106
00:06:20,884 --> 00:06:23,803
여기 보이는 이것은 Chelsea Finn이 진행한 실험입니다.

107
00:06:23,828 --> 00:06:28,937
Chelsea는 한 컵에서 다른 컵으로 
액체를 붓는 로봇을 시연했습니다.

108
00:06:28,962 --> 00:06:31,311
이 로봇은 단지 행동을 따라하는 것 뿐만 아니라

109
00:06:31,336 --> 00:06:33,914
목표를 추론하게 됩니다.

110
00:06:33,939 --> 00:06:36,375
주황색 컵이나 노란색 컵을 어디에 놔야하는지를 말이죠.

111
00:06:36,400 --> 00:06:39,136
그런 다음, 액체를 붓는 물리적인 방식이 
조금 다를 수 있는

112
00:06:39,161 --> 00:06:42,153
다른 위치에 있는 컵에 붓습니다.

113
00:06:42,178 --> 00:06:44,122
하지만 목표는 같을 겁니다.

114
00:06:48,800 --> 00:06:57,707
예측은 사람(동물) 제어와 의사결정에 있어서 매우
기본적인 구성 요소입니다.

115
00:06:57,732 --> 00:07:03,087
이 인용구에서, 우리가 motor commands
(운동명령)의 결과를 예측한다는 생각이

116
00:07:03,112 --> 00:07:07,532
sensory motor control(감각 운동통제)의 모든 측면에서
중요한 이론적 개념이 되었다고 말합니다.

117
00:07:08,365 --> 00:07:12,308
실제 세계에서의 제어에 대한 예측은

118
00:07:12,333 --> 00:07:14,610
model-based reinforcement learning
(모델 기반 강화 학습)에서 연구 중입니다.

119
00:07:14,635 --> 00:07:19,439
여기 UC Berkeley의 학생인
Frederick Ebert의 연구인데요.

120
00:07:19,440 --> 00:07:22,935
여기에 주변 환경의 물체와
상호 작용하는 로봇이 있습니다.

121
00:07:22,960 --> 00:07:29,133
매우 의도적으로 상호 작용하는 것은 확실합니다.
마치 물건들을 가지고 놀듯, 물건들 주위에 돌아다니고 무작위로 물건들을 밀죠.

122
00:07:29,158 --> 00:07:34,719
그리고 이 경험은 특정 일을 하기 위한
policy를 배우는 데 사용되지 않을 것입니다.

123
00:07:34,720 --> 00:07:38,055
하지만 로봇이 지금 무엇을 보고 있고
미래에 어떤 행동을 취할지를 고려 할 수 있다면,

124
00:07:38,080 --> 00:07:42,399
그 다음 무슨일이 일어날지 예측하는
모델을 배운다고 할 수 있습니다.

125
00:07:42,400 --> 00:07:44,630
그래서 이 사진들의 각 행은

126
00:07:44,655 --> 00:07:47,014
동일한 시작 이미지에서 시작하는데요.

127
00:07:47,039 --> 00:07:49,760
그러나 다른 행동들에 대한 결과들을 예측합니다.

128
00:07:49,785 --> 00:07:53,269
그리고 이 모델은 로봇이 보는 픽셀들로부터
직접적으로(directly) 예측합니다.

129
00:07:53,340 --> 00:07:57,300
따라서 맨 위 행의 사진들이

130
00:07:57,325 --> 00:08:02,483
로봇 팔이 위로 움직이면 보이는 픽셀들입니다.
만약 로봇팔이 아래로 움직이면 픽셀들이 달라지겠죠.

131
00:08:02,508 --> 00:08:04,801
물론 아래 행에서 볼 수 있듯이,

132
00:08:04,826 --> 00:08:08,597
모델은 팔이 오른쪽으로 움직이면
물체들이 밀릴 것이라고 올바르게 예측합니다.

133
00:08:08,622 --> 00:08:11,336
예측은 다소 명확하지 않지만,

134
00:08:11,361 --> 00:08:15,392
물체들이 어떻게 움직일지에 대한
몇 가지 기본 속성을 고려합니다.

135
00:08:15,646 --> 00:08:17,827
그리고 로봇에게 목표를 줄 수도 있습니다.

136
00:08:17,852 --> 00:08:20,017
이 빨간 점에 위치한 물체를

137
00:08:20,042 --> 00:08:22,105
이 녹색 점의 위치로 옮기라고 말이죠.

138
00:08:22,130 --> 00:08:24,556
그리고 그 행동의 결과를 바로 예측함으로써

139
00:08:24,581 --> 00:08:28,460
로봇은 물체를 잘 옮길 수 있는
일련의 행동을 계획할 수 있습니다.

140
00:08:28,485 --> 00:08:32,826
실제로 실행한 결과는 다음과 같습니다.

141
00:08:34,640 --> 00:08:38,107
보다 정교한 행동들에도 예측 모델을 사용할 수 있습니다.

142
00:08:38,132 --> 00:08:41,266
이것은 버클리 학부생이었던 Annie의 연구인데요.

143
00:08:41,291 --> 00:08:45,599
그녀는 예측 모델을 사용하여
도구 사용 방법을 계획하도록

144
00:08:45,600 --> 00:08:47,234
시스템을 확장했습니다.

145
00:08:47,259 --> 00:08:51,415
여기 이 로봇은 스펀지를 집고 이것을 사용하여
물체들을 옮길 계획을 세울 수 있습니다.

146
00:08:51,440 --> 00:08:56,287
또한, 작은 갈고리를 집고 이를 이용하여
파란색 물체를 위로 이동시킬 수 있습니다.

147
00:08:56,312 --> 00:09:02,375
그리고 물병을 사용하여 쓰레기통의 가장자리로
쓰레기들을 쓸 수 있다는 것을 알아내고 바로 할 수도 있습니다.

148
00:09:05,120 --> 00:09:07,185
또한 예측 모델을 다른 작업에 사용할 수 있습니다.

149
00:09:07,210 --> 00:09:13,805
비디오 게임을 위한 예측 모델에 초점을 맞춘
 Kaiser의 연구인데요.

150
00:09:14,305 --> 00:09:19,199
왼쪽에서 이 특정 비디오 게임에 대한
모델이 만든 예측을 볼 수 있고,

151
00:09:19,200 --> 00:09:23,319
가운데에서 실제 이미지를 볼 수 있습니다.
오른쪽 이미지는 둘의 차이를 보여줍니다.

152
00:09:23,360 --> 00:09:25,907
사실, 예측은 완벽하게 정확하지 않습니다.

153
00:09:25,932 --> 00:09:30,535
예를 들어, 실제 이미지에는 세 명의 상대가 있지만
예측 이미지에는 한 명만 있습니다.

154
00:09:30,560 --> 00:09:33,199
그러나 예측들은 내용상으로는(qualitatively) 맞습니다.

155
00:09:33,200 --> 00:09:35,969
모델은 당신이 상대방을 차면 그들이 사라질 것이고,

156
00:09:35,994 --> 00:09:40,398
당신에게 다양한 수의 상대방들이 
나타날 것이라고 예측합니다.

157
00:09:40,399 --> 00:09:43,510
물론 완벽하지 않으므로
때때로 몇 가지 문제가 있습니다.

158
00:09:43,535 --> 00:09:49,065
이 경우 복싱 게임 예가 있는데요. 짧게만 봅시다.

159
00:09:49,090 --> 00:09:51,691
이 권투 경기에는 복서가 두 명 있습니다.

160
00:09:51,716 --> 00:09:56,079
그런데 그들이 흐릿해지다가
두 번째 복서가 나타나게 됩니다.

161
00:09:56,104 --> 00:09:59,335
이 두 번째 권투 선수는 점점 커지고 
세 번째 팔이 나타납니다.

162
00:09:59,360 --> 00:10:01,717
여기에서 매우 이상한 일이 일어나고 있는데요.

163
00:10:01,742 --> 00:10:05,677
조금 이상하지만 기본적으로 모델이 일반화하고
있는 방식입니다.

164
00:10:05,702 --> 00:10:07,702
약간 다른 방식인거죠.

