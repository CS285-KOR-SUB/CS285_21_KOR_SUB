1
00:00:00,719 --> 00:00:05,200
안녕하세요. 오늘 cs285의 두 번째 강의에 오신 것을 환영합니다.

2
00:00:05,200 --> 00:00:09,200
우리는

3
00:00:06,240 --> 00:00:10,800
행동의 지도 학습에 대해 이야기할

4
00:00:09,200 --> 00:00:13,120
것이므로

5
00:00:13,120 --> 00:00:16,960
규칙적인 지도 학습 문제가 있는 경우 약간의 용어와 표기법으로 시작

6
00:00:16,960 --> 00:00:20,240
하겠습니다.

7
00:00:20,240 --> 00:00:24,640
우리는 이미지의 객체를 인식하고 싶을 수

8
00:00:22,240 --> 00:00:27,039
있으므로 심층 신경망을 통과하는 일부 입력 이미지를 가질 수

9
00:00:27,039 --> 00:00:31,840
있고 출력은 레이블

10
00:00:29,199 --> 00:00:32,320
이므로 여기서 사용할 용어는

11
00:00:32,320 --> 00:00:36,079
일종의 강화 학습 용어가 될 것입니다.

12
00:00:33,840 --> 00:00:37,200
표준 지도 학습 예제에 대해

13
00:00:37,200 --> 00:00:41,120
우리의 강화 용어를 사용하는 것부터 점진적으로 작업하여

14
00:00:41,760 --> 00:00:46,160
이를 강화

15
00:00:44,480 --> 00:00:48,879
학습 문제

16
00:00:46,160 --> 00:00:49,920
로 전환하여 관찰을 위해 입력을 o

17
00:00:49,920 --> 00:00:53,920
라고 부르고 행동을 위해 출력을 호출할 것입니다.

18
00:00:51,520 --> 00:00:56,000
지금은 입력이 이미지

19
00:00:53,920 --> 00:00:57,920
이고 출력이

20
00:00:56,000 --> 00:00:59,280
중간에 있는 신경망 레이블 또는

21
00:00:57,920 --> 00:01:00,079
일반적으로 해당 맵을 원하는 모델의 종류입니다.

22
00:01:00,079 --> 00:01:03,520
행동에 대한 관찰을

23
00:01:03,520 --> 00:01:07,600
정책이라고 부를

24
00:01:07,600 --> 00:01:12,479
것이며 아래첨자 ta

25
00:01:10,240 --> 00:01:12,960
는 해당 정책의 매개변수를 나타내는 문자 pi로 표시할 것이므로 신경망

26
00:01:12,960 --> 00:01:16,960
에서 ta는 해당 신경망의 가중치를 나타내

27
00:01:14,880 --> 00:01:19,759
므로 입력이 있습니다.

28
00:01:16,960 --> 00:01:20,080
o 출력 a와 그들 사이의 매핑

29
00:01:19,759 --> 00:01:21,759
pi

30
00:01:20,080 --> 00:01:23,200
아래 첨자 ta는 주어진 o에 대한 분포를 제공합니다.

31
00:01:26,400 --> 00:01:30,479
이제 강화 학습에서 물론

32
00:01:28,320 --> 00:01:33,200
우리는

33
00:01:30,479 --> 00:01:35,200
순차적 의사 결정 문제에 관심이 있으므로

34
00:01:33,200 --> 00:01:37,200
이러한 모든 입력 및 출력

35
00:01:35,200 --> 00:01:39,119
이 특정 시점에서 발생하므로 우리는

36
00:01:37,200 --> 00:01:41,200
일반적으로 첨자

37
00:01:39,119 --> 00:01:43,600
t를 사용하여 강화 학습에서 일반적으로 발생하는 시간 단계를 나타냅니다.

38
00:01:43,600 --> 00:01:47,119
우리는 이산 시간 문제를 다루므로

39
00:01:47,119 --> 00:01:50,960
시간이 작은 이산

40
00:01:49,040 --> 00:01:51,600
단계로 나뉘고 t는 어느 단계를 나타내는 정수라고 가정

41
00:01:51,600 --> 00:01:55,119
합니다.  o를 관찰하고

42
00:01:54,159 --> 00:01:58,320
어느 단계에서

43
00:01:55,119 --> 00:02:00,240
a를 방출합니까? 이제 pita

44
00:01:58,320 --> 00:02:02,880
는 조건부 ot에서 분포를 제공

45
00:02:02,880 --> 00:02:06,960
하며 물론

46
00:02:04,880 --> 00:02:09,440
강화 학습의 일반 지도 학습과

47
00:02:06,960 --> 00:02:10,080
달리  e 한 단계의 출력은 다음 단계

48
00:02:09,440 --> 00:02:13,599
의 입력

49
00:02:10,080 --> 00:02:17,200
에 영향을 미치므로 at은 ot

50
00:02:13,599 --> 00:02:19,520
에 1을 더한 영향을 미치므로 예를 들어

51
00:02:17,200 --> 00:02:20,560
호랑이를 인식하지 못하면

52
00:02:19,520 --> 00:02:23,120
다음 단계

53
00:02:23,120 --> 00:02:26,400
에서 호랑이가 많을 것 같은 바람직하지 않은 것을 볼 수 있습니다.

54
00:02:26,400 --> 00:02:32,640
이 기본 아이디어를 확장

55
00:02:32,640 --> 00:02:36,239
하여 제어에 대한 정책을 학습할 수 있으므로

56
00:02:34,800 --> 00:02:37,120
레이블을 출력하는 대신

57
00:02:37,120 --> 00:02:40,239
작업과 훨씬 비슷하게 보이는 것을 출력

58
00:02:38,879 --> 00:02:40,959
할 수 있지만 여전히 소프트 맥스 분포를 사용할 수 있는 개별 작업일

59
00:02:40,239 --> 00:02:43,200
수 있습니다.

60
00:02:40,959 --> 00:02:44,560
예를

61
00:02:43,200 --> 00:02:45,920
들어 호랑이를 볼 때

62
00:02:44,560 --> 00:02:48,400
이산 옵션 세트에서 선택할 수

63
00:02:45,920 --> 00:02:50,239
있지만

64
00:02:50,239 --> 00:02:54,560
pita

65
00:02:55,440 --> 00:02:59,200
가 다변량 정규 또는 가우스 분포의 평균 및 분산과 같은 일부 연속 분포의 매개 변수를 출력하는 연속 작업을 가질 수도 있습니다.

66
00:03:01,599 --> 00:03:06,560
용어를 요약하기 위해 ot

67
00:03:04,480 --> 00:03:09,920
는 관찰

68
00:03:06,560 --> 00:03:13,280
을 나타내고 는 행동을 나타내고

69
00:03:09,920 --> 00:03:16,080
pi는 주어진 ot에서 첨자 ta

70
00:03:13,280 --> 00:03:16,080
가 이제 정책입니다.

71
00:03:19,680 --> 00:03:25,599
강화 학습

72
00:03:21,120 --> 00:03:27,519
에서 많이 보게 될 상태는 st로 표시되는 상태

73
00:03:25,599 --> 00:03:29,040
이며 때로는 주어진 st에서 작성된 정책을 볼 수 있습니다.

74
00:03:32,080 --> 00:03:38,400
st와 ot의 차이점은 상태가 일반적으로 다음과 같이

75
00:03:35,920 --> 00:03:39,680
가정된다는 것입니다.  내가 곧 설명할 마르코비안 상태인

76
00:03:39,680 --> 00:03:44,080
반면 ot는

77
00:03:41,920 --> 00:03:46,000
해당 상태에서 발생하는 관찰

78
00:03:44,080 --> 00:03:47,360
이므로 가장 일반적으로 우리는

79
00:03:46,000 --> 00:03:48,159
정책을 조건부 및

80
00:03:47,360 --> 00:03:49,440
관찰

81
00:03:48,159 --> 00:03:51,280
로 작성하지만 때로는

82
00:03:49,440 --> 00:03:53,760
조건부 및 상태로 작성하며

83
00:03:51,280 --> 00:03:55,280
이는 보다 제한적인 특수입니다.  이 경우 상태와 관찰

84
00:03:53,760 --> 00:03:59,280
의 차이점을 설명

85
00:03:59,360 --> 00:04:02,799
하겠습니다. 이 장면을 관찰했다고 가정해 봅시다.

86
00:04:01,519 --> 00:04:06,560
거기에는 가젤을 쫓는 치타가 있습니다.

87
00:04:02,799 --> 00:04:08,400
이제 이 관찰

88
00:04:06,560 --> 00:04:09,920
은 이미지로 구성되고 이미지

89
00:04:08,400 --> 00:04:12,239
는 픽셀로 만들어집니다. 이 픽셀

90
00:04:12,239 --> 00:04:14,080
은 치타의 위치를 파악하기에 충분할 수 있습니다.  그리고

91
00:04:13,599 --> 00:04:17,120
가젤

92
00:04:14,080 --> 00:04:20,000
은 존재하거나 그렇지 않을 수도

93
00:04:17,120 --> 00:04:20,799
있지만 이미지는

94
00:04:20,799 --> 00:04:25,040
일부 시스템의 기본 물리학에 의해 생성되고 해당 시스템

95
00:04:23,759 --> 00:04:26,160
은 일종의 최소 표현을 갖는 상태를 갖습니다.

96
00:04:26,160 --> 00:04:29,840
따라서 이미지는 상태의 관찰

97
00:04:34,080 --> 00:04:37,600
이며 이 경우 예를 들어

98
00:04:36,160 --> 00:04:38,800
치타의

99
00:04:37,600 --> 00:04:42,639
위치와 가젤의 위치

100
00:04:42,639 --> 00:04:46,160
가 될 수 있으며 이제 관찰 속도가 일부 변경될 수 있는 시스템의 현재 구성을 나타냅니다.

101
00:04:47,919 --> 00:04:51,520
예를 들어 자동차

102
00:04:49,600 --> 00:04:53,199
가 치타 앞에서 운전하고

103
00:04:51,520 --> 00:04:54,880
그것을 볼 수 없는 경우 전체 상태를 정확하게 추론할 수 없기 때문에 상태

104
00:04:53,199 --> 00:04:56,880
를 추론하기에는 관찰이 불충분할

105
00:04:54,880 --> 00:04:58,320
수

106
00:04:56,880 --> 00:05:00,160
있지만 상태가 실제로 변경하지 않은

107
00:04:58,320 --> 00:05:01,759
경우 치트는 여전히 그 위치에 있습니다.  이전

108
00:05:00,160 --> 00:05:03,680
에는 이미지 픽셀

109
00:05:01,759 --> 00:05:05,840
과 관찰이 현재

110
00:05:03,680 --> 00:05:07,199
위치를 파악하기에 충분하지 않으며

111
00:05:05,840 --> 00:05:08,720
실제로

112
00:05:07,199 --> 00:05:10,560
상태

113
00:05:08,720 --> 00:05:12,639
와 관찰 상태

114
00:05:10,560 --> 00:05:14,000
의 차이를 파악하는 데 충분하지 않습니다. 시스템의 진정한 구성

115
00:05:12,639 --> 00:05:15,360
관찰

116
00:05:14,000 --> 00:05:17,840
은 해당

117
00:05:15,360 --> 00:05:21,680
상태에서 발생하는 것입니다.  상태를 보다 공식적으로 추론하기에 충분하거나 충분하지 않을 수 있습니다.

118
00:05:24,960 --> 00:05:28,720
그래픽

119
00:05:27,199 --> 00:05:31,360
모델을 사용

120
00:05:28,720 --> 00:05:33,039
하여 상태와 동작 및 관찰 사이의 관계를 나타내는 그래픽 모델을 그릴 수 있습니다.

121
00:05:36,080 --> 00:05:40,240
내가 언급한 관찰

122
00:05:38,800 --> 00:05:42,479
결과는 상태에서 발생

123
00:05:40,240 --> 00:05:44,000
하므로 매 단계마다 s에서 o로 화살표가

124
00:05:44,000 --> 00:05:48,160
표시됩니다. 따라서 정책은 관찰을 사용

125
00:05:46,479 --> 00:05:49,680
하여 동작을 선택합니다.

126
00:05:48,160 --> 00:05:51,280
o에서 화살표로

127
00:05:49,680 --> 00:05:53,199
그리고 현재

128
00:05:51,280 --> 00:05:55,520
시간 단계에서 작동 중인 상태는 다음 시간 단계의 상태를 결정

129
00:05:53,199 --> 00:05:58,319
하므로 s1과 a1

130
00:05:55,520 --> 00:05:58,319
은

131
00:05:58,639 --> 00:06:04,560
이제 이 그래픽 모델을 검사하여 s2로 이동

132
00:06:05,759 --> 00:06:11,759
합니다. 시스템에 특정 독립성이 존재한다고 결론을 내릴 수 있습니다.

133
00:06:09,280 --> 00:06:13,840
이것이 정책 파이입니다. 이것은

134
00:06:11,759 --> 00:06:14,639
sd의 전이 확률 p에

135
00:06:13,840 --> 00:06:18,479
주어진 통계 1을 더한 것입니다.

136
00:06:14,639 --> 00:06:21,840
여기서 주목해야 할 점

137
00:06:18,479 --> 00:06:21,840
은

138
00:06:22,000 --> 00:06:28,960
주어진 st 18에서 st의 p에 1을 더한

139
00:06:25,600 --> 00:06:30,080
값은 st 빼기 1과 무관

140
00:06:30,080 --> 00:06:34,160
하다는 것입니다. 따라서 현재를 알고 있는 상태의 경우  상태 그러면 이전 상태에 대한 고려 없이 다음 상태에

141
00:06:32,639 --> 00:06:35,440
대한 분포를 파악할 수 있습니다.

142
00:06:39,039 --> 00:06:43,520
즉, 미래는

143
00:06:41,600 --> 00:06:46,080
과거와 조건부로 독립적입니다.

144
00:06:43,520 --> 00:06:47,840
현재 상태를 고려할 때 이것은 매우

145
00:06:46,080 --> 00:06:49,919
중요한 독립 속성입니다.

146
00:06:47,840 --> 00:06:51,520
왜냐하면

147
00:06:51,520 --> 00:06:56,479
미래 상태에 영향을 미칠 결정을 내리려면

148
00:06:56,479 --> 00:06:59,280
현재 상태에 도달하는 방법을 고려할 필요가 없으며

149
00:06:57,919 --> 00:07:01,120
현재 상태만 고려하면 충분하기 때문입니다.

150
00:06:59,280 --> 00:07:01,599
그리고 이전

151
00:07:01,120 --> 00:07:04,960
상태

152
00:07:01,599 --> 00:07:07,199
를 잊어버릴 수 있습니다. 이것을

153
00:07:04,960 --> 00:07:08,880
markov 속성이라고 하며 markov

154
00:07:07,199 --> 00:07:10,479
속성은 강화 학습 및 순차적 의사 결정에서 매우 중요한 속성입니다.

155
00:07:11,759 --> 00:07:15,199
왜냐하면 markov 속성 없이는 최적의 정책

156
00:07:13,759 --> 00:07:16,880
을 공식화할 수 없기 때문입니다.

157
00:07:16,880 --> 00:07:22,240
전체 역사를 고려할 때

158
00:07:19,520 --> 00:07:24,160
그러나 우리의 정책이

159
00:07:24,160 --> 00:07:27,680
이 그림에서와 같이 상태가 아닌 관찰에 따라 결정된다면

160
00:07:27,680 --> 00:07:31,919
관찰도 이러한

161
00:07:35,360 --> 00:07:39,599
방식으로 조건부로 독립적인지 질문할 수

162
00:07:37,039 --> 00:07:40,800
있습니다.  미래

163
00:07:39,599 --> 00:07:42,319
에 이 질문에 대해 잠시 생각하고

164
00:07:42,319 --> 00:07:45,840
답을 쓰는 것을 고려하십시오.  주석

165
00:07:47,039 --> 00:07:50,160
에서 문제는 관찰

166
00:07:48,639 --> 00:07:52,160
이 일반적으로

167
00:07:50,160 --> 00:07:54,000
마르코프 속성을 만족하지 않는다는 것

168
00:07:52,160 --> 00:07:55,520
입니다. 즉 현재 관찰

169
00:07:54,000 --> 00:07:57,440
은 과거를 관찰하지 않고 미래를 완전히 결정하기에 충분하지 않을 수

170
00:07:58,319 --> 00:08:02,319
있으며 이는 아마도

171
00:08:00,479 --> 00:08:04,479
치타의 예에서 가장 분명합니다.

172
00:08:02,319 --> 00:08:05,759
차가 치타 앞에 있고

173
00:08:05,759 --> 00:08:08,240
이미지

174
00:08:08,240 --> 00:08:11,680
의 위치를

175
00:08:09,759 --> 00:08:14,319
볼 수 없을 때 지금은 볼 수 없기 때문에 미래에 어디로 갈지 알 수

176
00:08:11,680 --> 00:08:16,000
없지만 이전 지점에서  시간이

177
00:08:17,280 --> 00:08:20,400
지나면 치타가 어디에 있는지 기억하기 전에 차가 다른 곳에 있었는지 알 수

178
00:08:19,039 --> 00:08:22,319
있으므로 차가 막힌 경우에도

179
00:08:20,400 --> 00:08:23,520
여전히 상태를 기억할 수 있습니다.

180
00:08:22,319 --> 00:08:26,000
따라서 일반적으로 관찰을 사용하는 경우

181
00:08:23,520 --> 00:08:27,919
과거 관찰은

182
00:08:26,000 --> 00:08:28,960
실제로 추가 정보를  현재 관찰

183
00:08:27,919 --> 00:08:30,160
에서 얻을 수 있는 것 이상

184
00:08:30,160 --> 00:08:34,320
으로 의사 결정에 유용한 정보인

185
00:08:32,399 --> 00:08:36,080
반면, 상태를 직접 관찰

186
00:08:34,320 --> 00:08:37,519
하면 현재 상태는 항상 진행 중입니다.

187
00:08:37,519 --> 00:08:42,479
Markov 속성을 충족하기 때문에 필요한 모든 것을 제공하기 위해

188
00:08:42,479 --> 00:08:44,560
이 과정에서 논의할 많은 강화 학습 알고리즘

189
00:08:44,560 --> 00:08:50,399
은 실제로

190
00:08:51,760 --> 00:08:55,440
Markovian 상태를 필요로 할 것

191
00:08:53,920 --> 00:08:57,360
입니다.  특정 알고리즘

192
00:08:55,440 --> 00:08:59,200
은 비마코비안 관찰을 처리하기 위해 어떤 방식으로든 수정될 수 있습니다.

193
00:08:59,200 --> 00:09:02,800
그런 다음

194
00:09:06,800 --> 00:09:11,680
우리가 일반적으로

195
00:09:08,720 --> 00:09:13,120
s를 사용하여 상태를 표시하고

196
00:09:11,680 --> 00:09:14,560
매우 합리적인 행동을 표시하는 데 사용하는 강화 학습의 표기법을 제외하고 어떻게 할 수 있는지 설명하겠습니다.  영어

197
00:09:13,120 --> 00:09:15,360
로 된 단어의 첫 글자이기 때문에

198
00:09:15,360 --> 00:09:20,240
이러한 종류의 용어

199
00:09:27,040 --> 00:09:31,440
는 로봇 공학과 최적 제어 및 선형에 대한 배경 지식이 있는 경우 1950년대에 Richard bellman이 여러 면에서 개척한 동적 프로그래밍 연구에 의해 널리 대중화되었습니다.

200
00:09:29,200 --> 00:09:32,800
시스템

201
00:09:32,800 --> 00:09:36,959
에서는

202
00:09:34,480 --> 00:09:38,560
x가 상태를 나타내는 데 사용되고 u가 동작을 나타내는 데 사용되는 다른

203
00:09:38,560 --> 00:09:43,200
표기법에 더 익숙할 수 있습니다.  s는 정확히 동등한 용어입니다. x

204
00:09:41,680 --> 00:09:45,760
는 상태에 대해 의미가 있습니다. 왜냐하면

205
00:09:43,200 --> 00:09:47,760
그것은 일반적으로

206
00:09:45,760 --> 00:09:50,000
대수학에서 미지의 양에 사용되는 변수

207
00:09:47,760 --> 00:09:52,240
이고 u는 러시아어로 행동을 나타내는 첫 번째 단어

208
00:09:52,240 --> 00:09:55,680
이기 때문입니다.

209
00:10:01,760 --> 00:10:07,680
소비에트 연방에서 최적의 통제를 연구한 좌파

210
00:10:05,279 --> 00:10:08,800
판테라간은 맞습니다. 그래서 약간의

211
00:10:07,680 --> 00:10:11,360
용어

212
00:10:08,800 --> 00:10:12,240
이지만 이제 실제로 정책을 배울 수 있는 방법에 대해 이야기

213
00:10:12,240 --> 00:10:16,079
하고 오늘 강의에서는

214
00:10:14,560 --> 00:10:17,440
실제로 정책을 학습하는 매우 간단한 방법으로 시작하겠습니다.

215
00:10:17,440 --> 00:10:20,640
t는 매우

216
00:10:19,120 --> 00:10:21,600
정교한 강화 학습

217
00:10:20,640 --> 00:10:23,760
알고리즘을 사용해야

218
00:10:21,600 --> 00:10:25,360
하지만 대신 데이터를 활용하여 지도 학습에서

219
00:10:25,360 --> 00:10:29,120
이미지 분류기 및 기타 종류의

220
00:10:27,040 --> 00:10:32,240
모델을 배우는 것과 거의 동일한 방식으로 정책을 학습

221
00:10:32,240 --> 00:10:37,440
하므로 호랑이에게서 도망치는 보다 현실적인 예를 보겠습니다.

222
00:10:37,440 --> 00:10:41,040
우리의 일상 생활에서 중요하지만

223
00:10:41,760 --> 00:10:45,279
당신의 관찰 mi를 운전하는 다른 작업은 어떻습니까?  ght

224
00:10:44,000 --> 00:10:48,000
는 자동차 카메라의 이미지로 구성되며

225
00:10:48,000 --> 00:10:51,120
행동은

226
00:10:49,440 --> 00:10:53,600
자동차를 도로에 유지하기 위해 핸들

227
00:10:54,079 --> 00:10:58,959
을 돌리는 방법으로

228
00:10:57,120 --> 00:11:00,720
구성될 수 있습니다.

229
00:10:58,959 --> 00:11:02,880
이미지

230
00:11:00,720 --> 00:11:04,880
분류 컴퓨터 비전

231
00:11:02,880 --> 00:11:06,800
등과 같은 것 일부 레이블

232
00:11:04,880 --> 00:11:08,160
데이터를 가져오고 해당 레이블 데이터

233
00:11:06,800 --> 00:11:10,480
를 사용하여 지도 학습으로 운전 정책을

234
00:11:08,160 --> 00:11:13,279
학습

235
00:11:10,480 --> 00:11:14,959
하여 사람과 해당 운동 명령에서 이미지를 가져

236
00:11:13,279 --> 00:11:16,399
와서 이

237
00:11:14,959 --> 00:11:17,680
인간 운전자가 방향을

238
00:11:16,399 --> 00:11:19,120
바꾸도록 하겠습니다.  어떤 식으로든

239
00:11:19,839 --> 00:11:24,160
핸들을 잡고 카메라에서 본 것을 기록하고 조종 명령을 기록하고

240
00:11:24,160 --> 00:11:28,560
이미지 및 동작 튜플로 구성된 대규모 데이터 세트를 수집한

241
00:11:27,040 --> 00:11:29,040
다음 지도

242
00:11:28,560 --> 00:11:31,120
학습

243
00:11:29,040 --> 00:11:32,640
을 사용하여 지도 작성을 배우게 됩니다.

244
00:11:31,120 --> 00:11:34,959
행동에

245
00:11:32,640 --> 00:11:36,399
대한 관찰 이것을 모방 학습

246
00:11:34,959 --> 00:11:37,760
이라고 하며 때때로 행동 복제라고도 하는 초대 학습의 특정 사례입니다.

247
00:11:38,480 --> 00:11:42,880
이를 행동 복제라고 합니다.

248
00:11:41,279 --> 00:11:43,600
어떤 의미에서 우리

249
00:11:43,600 --> 00:11:48,320
는 이 인간 시연자의 행동을 복제하고 있기 때문에

250
00:11:46,720 --> 00:11:50,320
시연자들은 때때로 전문가라고도 합니다.

251
00:11:48,320 --> 00:11:51,600
왜냐하면 우리는

252
00:11:50,320 --> 00:11:54,800
그들이 컴퓨터보다 이 작업을 더 잘한다고 가정하기

253
00:11:54,800 --> 00:11:58,079
때문에 이것은 매우 간단한 접근 방식이며 우리

254
00:11:56,800 --> 00:12:00,959
는 다음과 같이 질문할 수 있습니다.  질문이 잘

255
00:11:58,079 --> 00:12:00,959
작동합니까?

256
00:12:01,120 --> 00:12:06,880
음. 그래서 이 질문

257
00:12:05,120 --> 00:12:08,480
은 아주 오랫동안 연구되어 왔습니다.

258
00:12:06,880 --> 00:12:10,560
사실 원래의

259
00:12:08,480 --> 00:12:12,079
심층 모방 모방 학습 시스템

260
00:12:10,560 --> 00:12:13,360
이나 신경 제한 학습 시스템은 오늘날

261
00:12:12,079 --> 00:12:13,920
우리에게 친숙할 무언가가 오래전

262
00:12:13,920 --> 00:12:19,200
에 제안된 것입니다.  1989년

263
00:12:17,200 --> 00:12:20,560
그것은 alvin이라고 불렀고 alvin은 자율 육상

264
00:12:19,200 --> 00:12:22,000
차량과 신경망이라고 불렀고

265
00:12:22,000 --> 00:12:27,279
현재 표준에 따르면 네트워크

266
00:12:25,040 --> 00:12:29,200
는 5개의 은닉 유닛이 있는 아주 작은 아주

267
00:12:27,279 --> 00:12:31,600
흥미로운 일을 했습니다.

268
00:12:33,760 --> 00:12:37,440
미국 전역을 가로질러 드라이브를 시도하여

269
00:12:37,519 --> 00:12:41,839
이 기본

270
00:12:40,240 --> 00:12:43,839
원리가 작동하는지 이 동작

271
00:12:41,839 --> 00:12:46,399
복제 원리

272
00:12:43,839 --> 00:12:48,000
가 일반적으로 작동하는지 물어볼 수 있습니다.  대답은 아니오입니다. 규칙적인 지도 학습이 효과가 있음에도 불구하고

273
00:12:48,000 --> 00:12:51,839
행동 복제가 잘못될 수 있는 이유에 대한 약간의 직관을 제공하기

274
00:12:51,839 --> 00:12:55,519
위해