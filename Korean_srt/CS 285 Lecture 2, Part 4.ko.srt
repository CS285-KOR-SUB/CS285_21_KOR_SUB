1
00:00:01,881 --> 00:00:03,912
이번 강의의 다음 부분에서는,

2
00:00:04,310 --> 00:00:08,992
모방 학습에 대한 작은 이슈에 대해 얘기해보려고 합니다.

3
00:00:09,251 --> 00:00:12,849
그리고 일반적인 목적들에 대해 이야기를 하고자 합니다.

4
00:00:13,194 --> 00:00:16,447
모방 학습에서 겪는 일반적인 문제가 뭐가 있을까요?

5
00:00:17,288 --> 00:00:21,712
모방학습에서는, 사람이 컴퓨터에게 움직이는 방법에 대해
데이터를 제공할 필요가 있습니다.

6
00:00:22,140 --> 00:00:25,904
예를 들면 사람이 차를 운전하는 것처럼
몇몇 환경에서는 꽤 쉽고 괜찮은 방법입니다.

7
00:00:26,385 --> 00:00:28,210
하지만 다른 환경에서는 어려운 일이 될 수도 있습니다.

8
00:00:29,579 --> 00:00:34,752
딥러닝 기반의 방법에서의 문제는 데이터가 충분할 때
잘 작동하는 경향이 있다는 점입니다.

9
00:00:34,832 --> 00:00:40,720
모방 학습을 딥러닝에 적용시켜본다면,
엄청난 양의 데이터를 제공할 필요가 있습니다.

10
00:00:40,819 --> 00:00:43,492
인간은 일부 행동을 제공하는데에는 좋지 않습니다.

11
00:00:43,516 --> 00:00:49,817
사람에겐 숲길을 걸어내려가거나 드론을 날리거나,
차를 운전하는 것은 쉽습니다.

12
00:00:50,289 --> 00:00:56,292
사람에게 수동으로 드론의 경로나 속도를 지정하는 것은
비교적으로 어렵습니다.

13
00:00:56,551 --> 00:01:03,427
그리고 휴머노이드같은 고차원적인 시스템에 대한
모든 행동을 지정하는 것은 매우 어렵습니다.

14
00:01:03,843 --> 00:01:14,109
더 나아가서는, 다양한 데이터 소스를 통합해야 하는
대규모 전자상거래에서 가격을

15
00:01:14,133 --> 00:01:15,050
동적으로 설정하는 것 같은 복잡한 상황도 있을 수 있습니다.

16
00:01:15,081 --> 00:01:20,519
이런 식으로 여러분은 인간이 최적의 행동을 제공하는 게
정말로 어려운 시나리오도 상상할 수 있습니다.

17
00:01:21,296 --> 00:01:25,271
그리고, 모방학습에서는 약간의 꺼림칙한 부분도
조금 있습니다.

18
00:01:25,403 --> 00:01:27,705
왜냐하면 사람은 알아서 배울 수 있기 때문입니다.

19
00:01:28,499 --> 00:01:33,700
컴퓨터가 사람처럼 스스로 한다면 더 이상 좋을 게 없겠지만, 사람이 어느정도 예시를 보여주는걸 필요로 한다는 것과

20
00:01:33,724 --> 00:01:36,943
사람이 스스로 배울 수 있다는 것은 명백합니다.

21
00:01:37,452 --> 00:01:42,223
만약 여러분이 스스로 배운다고 하면,
자신의 경험으로부터 무제한의 데이터를 얻을 수 있습니다.

22
00:01:42,243 --> 00:01:45,735
그리고 여러분은 그 일을 더 많이 하면서
지속적으로 발전시킬 수 있습니다.

23
00:01:46,450 --> 00:01:48,493
그리고 그게 강화학습이 목표로 하는 것입니다.

24
00:01:48,950 --> 00:01:52,421
그리고 그것을 가능하게 하기 위해,
우리는 실제로 우리의 목표를 정의해야 합니다.

25
00:01:52,445 --> 00:01:54,587
그리고 지금까지는, 우리는 이 질문을 피해왔습니다

26
00:01:54,984 --> 00:01:58,990
그래서 우리는 관찰을 행동으로 대응(mapping)시키며 용어를 정의했지만,

27
00:01:59,014 --> 00:02:03,096
실제로 좋은 대응인지 나쁜 대응인지 의미를 정의하지는 않았습니다.

28
00:02:04,339 --> 00:02:08,049
이 강의의 시작으로 돌아가서 
호랑이에 대한 시나리오를 상상해본다면,

29
00:02:08,136 --> 00:02:15,316
여러분은 아마 시연이나 데이터에 대한 걱정보단,  
잡아먹히지 않기만 바랄 것입니다.

30
00:02:16,897 --> 00:02:21,779
이 걸 수학적으로 델타 함수의 기댓값으로 표현할 수 있는데,

31
00:02:21,812 --> 00:02:24,817
델타 함수는 호랑이에게 잡아먹히면 1이 되고
그렇지 않으면, 0이 되는데

32
00:02:24,854 --> 00:02:27,863
델타 함수의 기댓값을 최소화시키면 됩니다.

33
00:02:28,519 --> 00:02:32,051
그리고 기댓값은 상태(state)의 분포에 대한 기댓값이고

34
00:02:32,075 --> 00:02:37,059
상태(state)에 대한 분포는 정책(policy)과 전이 확률(transition probabilities)에 의해 유도된 분포입니다.

35
00:02:37,092 --> 00:02:43,265
그래서 강의 초반의 그래픽 모델로 돌아가본다면, 
우리는 모델에서 나온 기댓값을 얻기만 하면 됩니다.

36
00:02:45,109 --> 00:02:48,387
우리는 이 수식을 좀 더 일반화시키면 이렇게 쓸 수 있고,

37
00:02:48,411 --> 00:02:56,676
이 델타 함수는 우리가 원하지 않는 상태와 행동의 순서를 
따르는 분포의 기댓값을 가지고 있습니다.

38
00:02:56,736 --> 00:03:02,045
일반적으로 우리는 비용함수의 합을 c로 표현하는데,

39
00:03:02,069 --> 00:03:06,527
 이는 상태(state)에만 해당될 수도 있고, 
행동(action)에만 표현될 수도 있고, 둘 다 일 수도 있습니다.

40
00:03:06,552 --> 00:03:08,630
그리고 우리는 이 기댓값을 최소화시키길 원합니다.

41
00:03:08,710 --> 00:03:14,158
그래서 델타 함수는 우리가 호랑이에게 먹힌다면, 잡아먹히는 것을 피하려고 합니다.

42
00:03:14,846 --> 00:03:20,929
그리고 문헌에서 여러분은 이것을 s_t, a_t의 
비용 함수(cost function) c로 표현할 수 있거나,

43
00:03:20,954 --> 00:03:24,957
아니면 s_t, a_t에 대한 보상 함수 (reward function) r로 
동등하게 표현할 수 있습니다.

44
00:03:25,057 --> 00:03:27,690
우리는 비용을 최소화시키고, 보상을 최대화시킵니다.

45
00:03:27,730 --> 00:03:30,330
그렇지 않으면 같은 것을 의미합니다.

46
00:03:30,496 --> 00:03:35,694
조금 돌아가서 용어에 대해 과거로 돌아가보면, 
보상에 대한 표기는 동적 프로그래밍 문헌 뿐만 아니라

47
00:03:35,718 --> 00:03:42,387
동적 프로그래밍 뿐만 아니라 
현대 강화 학습 같은 분야의 문헌에서는

48
00:03:42,411 --> 00:03:47,476
1950년대 미국에서의 동적 프로그래밍 연구에
뿌리를 두고 있습니다.

49
00:03:47,500 --> 00:03:55,881
반면에 cost에 대한 표기는 최적 제어 이론(optimal control theory)에 뿌리를 두고 있는데,

50
00:03:55,905 --> 00:03:59,506
둘 다 같은 것이고, 보상은 단지 비용의 음수일 뿐입니다.

51
00:03:59,536 --> 00:04:05,014
저는 미국인들은 삶이 보상을 가져다 주기를 기대하는 반면,

52
00:04:05,038 --> 00:04:08,044
비관적인 러시아인들은 비용과 후회를 하는 국가적인 성격의 차이를 매우 좋아합니다.

53
00:04:08,068 --> 00:04:10,496
하지만 결론적으로 같은 것을 의미합니다.

