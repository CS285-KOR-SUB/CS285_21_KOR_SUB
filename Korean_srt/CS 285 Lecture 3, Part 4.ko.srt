1
00:00:00,719 --> 00:00:05,680
자, 이제 전체 예제를 통해 neural network를 정의하고
학습시키는 전체 프로세스를 살펴보고

2
00:00:05,680 --> 00:00:07,759
이를 활용해 예측하는 것까지 진행해보겠습니다

3
00:00:07,759 --> 00:00:12,160
우리가 해야 할 작업은 neural network를 사용하여 sine wave를 모델링하는 것입니다

4
00:00:12,160 --> 00:00:16,040
그래서 이게 target인데요

5
00:00:16,160 --> 00:00:19,119
이 함수가 우리가 학습시키고자 하는 함수입니다

6
00:00:19,119 --> 00:00:23,000
먼저 neural network의 구조를 정의하는 것으로 시작하겠습니다

7
00:00:23,000 --> 00:00:25,680
우리는 module이라는 것을 활용하여 작업을 진행하려고 합니다

8
00:00:25,680 --> 00:00:34,360
PyTorch 내에서 거의 모든 neural network의
layer에 대한 기본 클래스는 nn.Module이라 불리는데요,

9
00:00:34,360 --> 00:00:40,000
이 module들은 전체 computation graph를 구성하는 기본 구성 요소입니다

10
00:00:40,000 --> 00:00:48,160
그래서 일반적으로 neural network에는 convolution layer,
fully connected layer, pooling layer와 같은 layer들이 있습니다

11
00:00:48,160 --> 00:00:57,039
이것들이 각각 nn.Module의 subclass이며 이 세 가지는 실제로 PyTorch에 이미 정의되어 있습니다

12
00:00:57,039 --> 00:00:59,160
따라서 neural network를 정의하려는 경우

13
00:00:59,160 --> 00:01:07,280
이러한 module들을 여러 개 묶어서 전체 network에 대한 자체 module로 결합하기만 하면 됩니다

14
00:01:07,680 --> 00:01:10,799
이 모델을 예로 들어 보겠습니다

15
00:01:10,799 --> 00:01:14,159
첫 번째 영상에서 보았던 건데요

16
00:01:14,159 --> 00:01:18,799
3가지 계산 작업이 있고, 그 중 처음 2개는 선형 계산 이후 비선형 계산이 따라오는 형태입니다

17
00:01:18,799 --> 00:01:23,600
그리고 마지막 1개의 계산 작업은 단순히 스칼라 결과값을 얻기 위한 선형 변환입니다

18
00:01:23,600 --> 00:01:29,280
이것이 우리가 이 문제에 사용할 구조입니다

19
00:01:30,799 --> 00:01:33,840
따라서 이 구조를 정의하기 위해

20
00:01:33,840 --> 00:01:36,400
우리는 python으로 클래스를 정의하려고 합니다

21
00:01:36,400 --> 00:01:39,200
이름은 원하는 대로 설정하면 됩니다.
일단 여기에서는 Net으로 부르도록 하죠

22
00:01:39,200 --> 00:01:44,560
중요한 것은 이것이 nn.Module의 subclass가 되어야 한다는 것입니다

23
00:01:44,799 --> 00:01:51,040
그래서 해야 하는 것은,
일반적으로 class 내에는 2가지가 필요한데요

24
00:01:51,040 --> 00:01:55,370
생성자에서는 모델의 모든 layer를 정의해야 하고

25
00:01:55,370 --> 00:02:04,079
이 forward 함수에서는 input에 어떤 작업을 해서
output으로 어떤 예측값을 반환할지를 정의해야 합니다

26
00:02:04,079 --> 00:02:09,038
그래서 생성자에서는 정의해야 할 layer가 3개 있는데요

27
00:02:09,038 --> 00:02:15,040
Layer 1 2 3를 fc1부터 fc3까지로 정의합니다

28
00:02:15,040 --> 00:02:19,640
각 Layer는 이미 정의되어 있는 nn.Linear 클래스를 활용합니다

29
00:02:19,640 --> 00:02:22,140
nn.Linear는 nn.Module의 subclass입니다

30
00:02:22,140 --> 00:02:27,440
그리고 입력과 출력이 어떻게 생겼는지 지정할 수 있습니다

31
00:02:28,239 --> 00:02:36,280
그리고 forward 함수에서는 이 모델이 실제로 예측을 하기 위해 어떤 작업을 하는지를 정의합니다

32
00:02:36,280 --> 00:02:39,120
그래서 여기에서는 linear layer를 통해 값을 보내고

33
00:02:39,120 --> 00:02:41,519
그 다음에는 ReLU를 활용한 비선형 연산

34
00:02:41,519 --> 00:02:44,959
그리고 두번째 hidden layer에서도 같은 작업을 반복하고

35
00:02:44,959 --> 00:02:51,840
마지막 layer에서는 선형 변환을 해서 output을 반환합니다

36
00:02:54,480 --> 00:03:00,879
이렇게 한 번 정의하고 나면 이 network를 instance로서 초기화할 수 있게 됩니다

37
00:03:00,879 --> 00:03:03,440
다른 python class와 마찬가지로요

38
00:03:03,440 --> 00:03:05,120
그리고 이를 print 함수로 출력해보면

39
00:03:05,120 --> 00:03:09,280
neural network가 어떻게 구성되었는지 확인할 수 있습니다

40
00:03:09,280 --> 00:03:19,560
모든 Layer와 해당 이름, input과 output 형태를 볼 수 있습니다

41
00:03:20,319 --> 00:03:24,480
그래서 만약 이 neural network를 특정 input을 넣어 확인해보고 싶다면

42
00:03:24,480 --> 00:03:28,319
직접 호출해서 input을 넣어주면 됩니다

43
00:03:28,319 --> 00:03:30,959
이를 통해 network의 변수들을 초기화하고

44
00:03:30,959 --> 00:03:33,680
직접 호출하여 x를 넘겨줍니다

45
00:03:33,680 --> 00:03:38,959
여기서 정말 주의해야 하는 것이 net.forward(x)를 호출하면 안 됩니다

46
00:03:38,959 --> 00:03:46,879
우리가 이전에 class에서도 forward 함수를 직접 호출하지는 않았듯이 말이죠

47
00:03:48,799 --> 00:03:54,720
그리고 그 이유는 바로 net()을 바로 호출할 때 사용자를 위해 설정된 초기 setup code가 실행되어서

48
00:03:54,720 --> 00:04:00,000
그 직후 net.forward()를 호출하게 되기 때문입니다

49
00:04:00,000 --> 00:04:02,879
그래서 forward 함수를 직접 호출하지 않더라도

50
00:04:02,879 --> 00:04:08,239
net module을 호출하고 input을 넣으면 됩니다

51
00:04:08,959 --> 00:04:15,120
이렇게 한 번 실행해보면 output이 tensor인 것을 확인할 수 있고

52
00:04:15,120 --> 00:04:19,358
이 tensor의 첫번째 차원이 100인 것을 확인할 수 있는데

53
00:04:19,358 --> 00:04:24,240
이는 우리가 넣어준 데이터의 개수인 100과 같습니다. tensor의 첫번째 차원은 batch dimension이기 때문입니다

54
00:04:24,240 --> 00:04:29,840
그리고 두번째 차원은 우리가 이전에 neural network에서 정의해둔 output의 크기와 동일합니다

55
00:04:31,440 --> 00:04:34,960
그럼 이번에는 학습하기 이전에 이 네트워크가 어떻게 생겼는지 시각화해봅시다

56
00:04:34,960 --> 00:04:36,880
입력이 있고 출력이 있습니다

57
00:04:36,880 --> 00:04:44,320
그리고 이것은 무작위로 초기화된 모델이 이 값들에 대해 예측한 것입니다

58
00:04:44,639 --> 00:04:52,560
이 nn.Module 내에서 일어나는 일을 좀 더 자세히 살펴보면

59
00:04:52,560 --> 00:04:58,720
module 내에서 인스턴스 변수를 정의하게 되면

60
00:04:58,720 --> 00:05:04,240
그 때 nn.Module 클래스 내에서는

61
00:05:04,240 --> 00:05:07,360
이것을 module 내의 parameter로 간주하게 됩니다

62
00:05:07,360 --> 00:05:08,960
이것이 중요한 이유는

63
00:05:08,960 --> 00:05:12,160
이러한 parameter들이 자동으로 gradient들을 추적하게 되기 때문입니다

64
00:05:12,160 --> 00:05:16,080
즉 requires_grad = true입니다

65
00:05:17,840 --> 00:05:20,320
그래서 이것을 print로 출력해보면

66
00:05:20,320 --> 00:05:23,520
named_parameters() 함수를 사용하거나

67
00:05:23,520 --> 00:05:30,240
만약 parameter의 이름을 확인할 필요 없이 값만 확인하고 싶다면 parameters()를 사용하면 됩니다

68
00:05:30,240 --> 00:05:32,720
그러면 어떤 layer와 어떤 tensor들이 이 model을 구성하는지 확인할 수 있습니다

69
00:05:32,720 --> 00:05:36,880
이 model은 3개의 fully connected layer로 구성되어 있고

70
00:05:36,880 --> 00:05:44,639
각 fully connected layer는 weight와 bias로 이루어져 있습니다

71
00:05:46,320 --> 00:05:51,680
그리고 weight와 bias의 각 parameter는 자동적으로 gradient를 저장하게 되는데

72
00:05:51,680 --> 00:05:57,280
왜냐하면 이것들이 이전에 말한 것처럼 nn.Module subclass 내에 정의되어 있기 때문입니다

73
00:05:57,280 --> 00:05:59,199
그래서 만약 print 함수를 통해 출력해보면

74
00:05:59,199 --> 00:06:01,280
grad를 가지고 있는 것을 볼 수 있는데

75
00:06:01,280 --> 00:06:02,880
다만 이 경우에는 아직 None으로 나옵니다

76
00:06:02,880 --> 00:06:06,840
그 이유는 아직 backpropagation을 하지 않았기 때문입니다

77
00:06:07,280 --> 00:06:13,199
흔하게 하는 실수로 module들의 list를 정의하려 할 때

78
00:06:13,199 --> 00:06:17,520
예를 들면 특정 개수의 layer를 가진 neural network를 정의하려고 한다면

79
00:06:17,520 --> 00:06:22,880
for 루프문을 활용해서 각 layer를 선언하려고 할 수도 있을겁니다

80
00:06:22,880 --> 00:06:27,440
그런데 이렇게 python의 내장 list 방식으로 더하면 안 됩니다

81
00:06:27,440 --> 00:06:32,319
왜냐하면 이렇게 할 경우 PyTorch가 이것들이 실제로는 sub-module임을 인지하지 못하기 때문입니다

82
00:06:32,319 --> 00:06:37,520
그리고 실제 parameter module로 간주되지 않아 gradient가 추적되지 않습니다

83
00:06:37,520 --> 00:06:42,880
그래서 이렇게 하는 것 대신 nn.ModuleList class를 활용해야 합니다

84
00:06:42,880 --> 00:06:48,800
사용하려는 layer들의 리스트를 넘겨주면

85
00:06:48,800 --> 00:06:54,479
network1과 network2 모두 gradient를 잘 추적하게 됩니다

86
00:06:58,080 --> 00:07:05,840
예측을 하고자 할 때 우리가 이전에 했던 방식은

87
00:07:07,599 --> 00:07:11,919
y = net(x)로 예측을 했는데요

88
00:07:11,919 --> 00:07:14,960
이렇게 할 경우 output은 tensor이고

89
00:07:14,960 --> 00:07:21,039
우리가 이전 영상들에서 했던 것처럼 tensor에 대해 연산을 할 수 있습니다

90
00:07:21,039 --> 00:07:26,400
그리고 자동으로 계산을 추적하고 도함수를 계산할 수 있습니다.

91
00:07:26,400 --> 00:07:33,039
예를 들어 우리가 한 예측과 target 간의 squared loss를 사용해본다고 합시다

92
00:07:33,039 --> 00:07:35,280
그리고 lost.backward()를 실행해보면

93
00:07:35,280 --> 00:07:43,599
자동으로 backpropagation을 실행하고 model의 각 weight에 gradient들을 배정해줍니다

94
00:07:43,759 --> 00:07:51,000
그래서 이제 p.grad를 활용해 model의 각 parameter를 확인할 수 있습니다

95
00:07:51,000 --> 00:07:53,199
그리고 직접 gradient descent를 구현할 수도 있습니다

96
00:07:53,199 --> 00:07:59,080
이는 기본적으로 gradient의 반대 방향으로 나아가는 것과 같습니다

97
00:07:59,520 --> 00:08:01,919
그리고 전에 이에 대해 이야기했던 것을 기억해보면

98
00:08:01,919 --> 00:08:05,039
만약 backward()를 여러번 호출하면 실제로는 gradient를 누적시키는 것이 됩니다

99
00:08:05,039 --> 00:08:08,460
따라서 이 단계를 수행한 후 gradient descent을 한다면

100
00:08:08,460 --> 00:08:11,039
항상 gradients를 0으로 설정해야 합니다

101
00:08:11,039 --> 00:08:14,960
그래야 이를 통해 다음에 새롭게 시작할 수 있게 됩니다

102
00:08:16,240 --> 00:08:19,759
이 시점에서 우리는 network를 학습시키는 데 필요한 모든 것들을 갖추었습니다

103
00:08:19,759 --> 00:08:23,590
우리는 for 루프를 사용해서 다음 작업을 반복합니다

104
00:08:23,590 --> 00:08:29,599
loss를 계산하고, backward()를 호출해서 backpropagation을 하고 gradient를 배정하고

105
00:08:29,599 --> 00:08:33,440
배정된 gradient들을 활용해서 각 parameter를 조정합니다

106
00:08:33,440 --> 00:08:36,479
그리고 gradient들을 0으로 만든 뒤 다시 시작합니다

107
00:08:36,479 --> 00:08:43,039
이렇게 우리는 sine wave를 상당히 비슷하게 모델링하는 network를 얻을 수 있습니다

108
00:08:43,039 --> 00:08:47,040
그러니까 확실히 학습을 한 것입니다

109
00:08:47,040 --> 00:08:54,210
하지만 이런 방식의 구현은 매번 작성해야 해서 성가신 일입니다

110
00:08:54,210 --> 00:09:01,440
따라서 PyTorch는 기본적으로 특정 작업을 처리하는 몇 가지 도우미 기능을 가지고 있습니다

111
00:09:01,440 --> 00:09:07,120
이것들을 추가해보고 어떻게 학습 과정을 단순화하는지 살펴봅시다

112
00:09:07,279 --> 00:09:09,519
첫번째는 loss function들입니다

113
00:09:09,519 --> 00:09:12,480
PyTorch에는 많은 내장 loss function들이 있으며

114
00:09:12,480 --> 00:09:18,320
이러한 loss function들은 layer와 마찬가지로
model 내에 존재하는 module입니다

115
00:09:18,320 --> 00:09:20,000
이것들을 통해 데이터를 전달할 수 있습니다

116
00:09:20,000 --> 00:09:24,880
loss function을 직접 구현하는 대신

117
00:09:24,880 --> 00:09:29,200
MSELoss()를 사용한다고 하면

118
00:09:29,440 --> 00:09:33,839
outputs과 targets을 전달하고

119
00:09:33,839 --> 00:09:38,080
output과 prediction 간 loss를 계산한 tensor를 얻게 됩니다

120
00:09:38,880 --> 00:09:45,200
마찬가지로 이전에 loss를 계산하던 방식을
loss_fun로 바꿔서 실행해보면

121
00:09:45,200 --> 00:09:49,360
같은 결과에 도달하는 것을 확인할 수 있습니다

122
00:09:49,360 --> 00:09:53,440
그다지 중요한 건 아니긴 합니다

123
00:09:53,440 --> 00:09:55,200
squared loss의 경우 구현하기 쉬운 편이니까요

124
00:09:55,200 --> 00:09:58,880
그러나 만약 좀 더 복잡한 loss function들로 실험을 진행하거나

125
00:09:58,880 --> 00:10:04,560
임의의 loss function으로 교체해서 어떤 것이 더 잘 동작하는지 확인하는 방식을 간편하게 만들고 싶다면

126
00:10:04,560 --> 00:10:08,720
이 방식이 확실히 따라해볼만한 좋은 예제입니다

127
00:10:10,560 --> 00:10:13,070
다음은 optimizer들입니다

128
00:10:13,070 --> 00:10:18,320
PyTorch의 optim 패키지를 활용해 내장 optimizer들을 사용할 수 있습니다

129
00:10:18,320 --> 00:10:24,880
이전처럼 직접 gradient descent을 구현하는 대신에요

130
00:10:24,880 --> 00:10:29,839
s그래서 우리가 해볼 것은 neural network를 재정의하는 것입니다

131
00:10:29,839 --> 00:10:33,120
여기 optimizer가 있고

132
00:10:33,120 --> 00:10:36,399
여기에 원하는 optimization function을 사용할 수 있습니다

133
00:10:36,399 --> 00:10:40,880
이전에 했던 것과 가장 비슷한 SGD를 사용할 수도 있고

134
00:10:40,880 --> 00:10:45,680
RMSprop를 사용할 수도 있습니다
이 경우에는 Adam을 사용해봅시다

135
00:10:45,680 --> 00:10:51,040
즉 내가 원하는 다른 optimizer들로 쉽게 교체하여 사용할 수 있습니다

136
00:10:51,040 --> 00:10:58,240
여기서 prediction을 한 다음 다시 학습을 진행하기 전에 network가 어떻게 생겼는지 확인해봅시다

137
00:10:58,240 --> 00:11:04,240
다시 초기화를 했기 때문에 임의의 값을 가지게 되는데

138
00:11:04,240 --> 00:11:08,560
이번에는 직접 gradient descent를 구현하는 대신

139
00:11:08,560 --> 00:11:11,600
이것을 optimizer로 교체했습니다

140
00:11:11,600 --> 00:11:16,560
작동 방식은 이전과 같이 loss를 계산하는 방식입니다

141
00:11:16,560 --> 00:11:25,360
그러나 optimizer를 통해 모든 최적화를 수행하므로 먼저 optimizer.zero_grad()를 호출합니다

142
00:11:25,360 --> 00:11:27,519
gradient들을 모두 0으로 만들고

143
00:11:27,519 --> 00:11:32,079
loss.backward()를 실행해서 각 parameter들에 gradient들을 넣어줍니다

144
00:11:32,079 --> 00:11:34,560
그리고 마지막으로 optimizer.step()를 실행하는데

145
00:11:34,560 --> 00:11:40,320
optimizer.step()는 이전에 사용한 이 코드와 동일한데

146
00:11:40,320 --> 00:11:44,640
각 parameters를 gradient descent를 통해 업데이트하는 방식입니다

147
00:11:44,640 --> 00:11:55,200
그리고 이 optimizer.zero_grad()는 이 코드와 기본적으로 같은 내용인데

148
00:11:55,200 --> 00:12:00,240
gradient들을 0으로 만들어서 이것들이 누적되지 않도록 해주는 역할을 합니다

149
00:12:01,440 --> 00:12:07,600
한 가지 유의해야 할 것은 optimizer를 초기화할 때

150
00:12:07,600 --> 00:12:14,240
학습하고자 하는 neural network의 모든 parameter들을 잘 넘겨주었는지 확인해야 합니다

151
00:12:14,240 --> 00:12:20,639
왜냐하면 이것이 optimizer가 gradient descent를 할 때
유일하게 parameter에 대해 인지할 수 있는 방법이기 때문입니다

152
00:12:20,639 --> 00:12:29,040
optimizer.zero_grad()나 optimizer.step를 호출하면
여기서 넘겨준 parameter들에 대해서만 업데이트가 수행됩니다

153
00:12:29,040 --> 00:12:31,920
여기 있는 것처럼 단일 neural network라면 이런 작업을 하기는 정말 쉽습니다

154
00:12:31,920 --> 00:12:34,160
그냥 net.parameters()를 써서 넘겨주면 됩니다

155
00:12:34,160 --> 00:12:39,760
하지만 동시에 여러 model를 학습시키는 작업을 해야 한다면

156
00:12:39,760 --> 00:12:46,240
실제로 optimizer에 모든 parameters를 넘겼는지 잘 확인하고 주의해야 합니다

157
00:12:48,560 --> 00:12:52,480
좋습니다. 이제 이 코드를 실행해봅시다

158
00:12:52,480 --> 00:12:56,399
우리가 이전에 했던 것과 똑같은 일을 했다는 것을 알 수 있습니다

159
00:12:56,399 --> 00:12:59,680
다만 코드가 좀 더 간결해졌습니다

160
00:12:59,920 --> 00:13:02,000
여기 우리가 한 작업에 대한 요약이 있습니다.

161
00:13:02,000 --> 00:13:05,519
편하게 원하는대로 동영상을 일시중지하거나 colab으로 돌아가 확인하면 됩니다

162
00:13:05,519 --> 00:13:12,000
다만 이 전체 학습 과정이 어떻게 이루어져 있는지 잘 이해하도록 합시다

163
00:13:12,000 --> 00:13:14,200
다음은 PyTorch의 GPU 지원에 대해 얘기해봅시다

164
00:13:14,200 --> 00:13:18,000
대부분의 경우 우리는 GPU를 활용해 학습을 진행하고 싶어합니다

165
00:13:18,000 --> 00:13:24,079
왜냐하면 GPU는 행렬 연산에 최적화되어 있어
학습을 훨씬 더 빠르게 실행할 수 있기 때문입니다

166
00:13:24,079 --> 00:13:30,480
PyTorch를 사용하면 학습 과정에서 CPU와 GPU 간에 tensor를 매우 쉽게 이동할 수 있습니다

167
00:13:30,480 --> 00:13:33,519
여기에서 그 예제를 살펴보겠습니다

168
00:13:33,519 --> 00:13:39,360
먼저 현재 컴퓨터에서 사용 가능한 GPU가 있는지 확인합니다

169
00:13:39,360 --> 00:13:44,190
여러분 중 대다수가 colab을 활용해 숙제를 하게 될 겁니다

170
00:13:44,190 --> 00:13:48,399
colab은 GPU를 무료로 사용할 수 있게 해주기 때문에 좋습니다

171
00:13:48,399 --> 00:13:52,160
colab을 사용하려는 경우 일단 runtime 탭으로 이동해서

172
00:13:52,160 --> 00:13:55,760
runtime type이 GPU로 잘 되어 있는지 확인합니다

173
00:13:55,760 --> 00:13:59,440
아닐 경우 GPU로 학습을 진행할 수 없게 됩니다

174
00:13:59,600 --> 00:14:03,360
일단 설정을 했으면

175
00:14:03,360 --> 00:14:08,240
torch.cuda_is_available()를 print 함수로 출력해서 True가 나오는지 확인해봅니다

176
00:14:08,240 --> 00:14:14,240
여기 CPU 대신 GPU에서 tensor를 초기화하는 예제가 있습니다

177
00:14:14,240 --> 00:14:18,720
기본적으로 만약 torch.ones()를 활용해 tensor를 정의하면

178
00:14:18,720 --> 00:14:22,880
CPU에 올라가게 되지만 만약 특정 device parameter를 지정해주어

179
00:14:22,880 --> 00:14:28,880
PyTorch에 우리가 대신 GPU에서 초기화하는 것을 원한다고 알려주면

180
00:14:28,959 --> 00:14:35,120
여기 CPU 이외의 다른 장치를 지정한다는 것을 알 수 있습니다.

181
00:14:36,160 --> 00:14:43,360
정말 주의해야 할 한 가지는 tensor가
실제로 동일한 장치에 있는 경우에만 작업을 수행할 수 있다는 것입니다

182
00:14:43,360 --> 00:14:48,560
따라서 이 코드는 x가 CPU에 있는데 y는 GPU에 있기 때문에 오류가 발생합니다

183
00:14:48,560 --> 00:14:56,680
이 둘을 더하려고 하면 이 오류 메시지가 나타납니다

184
00:14:58,399 --> 00:15:03,560
그래서 우리가 이것들을 더하고 싶다면 x를 GPU로 옮겨야 합니다

185
00:15:03,560 --> 00:15:08,440
원래 CPU에서 초기화된 대상에 이 작업을 하려면 to() 함수를 사용해야 합니다

186
00:15:08,440 --> 00:15:13,839
그래서 우리는 x.to()를 사용해서 x를 GPU로 옮기고 싶다고 지정합니다

187
00:15:15,279 --> 00:15:19,040
이 방법을 통해 둘을 잘 더할 수 있게 됩니다

188
00:15:20,800 --> 00:15:27,199
그리고 추적되는 gradient를 가진 tensor를 numpy array로 변환할 수 없는 것처럼

189
00:15:27,199 --> 00:15:31,920
GPU에 있는 tensor를 직접 numpy array로 변환할 수도 없습니다

190
00:15:31,920 --> 00:15:35,759
만약 그러면 에러 메세지를 보게 됩니다

191
00:15:35,839 --> 00:15:38,959
따라서 대신 numpy로 변환하려면 해야 할 일은

192
00:15:38,959 --> 00:15:44,000
먼저 CPU로 이동한 다음 numpy로 변환하는 것입니다

193
00:15:45,680 --> 00:15:50,160
그래서 이런 작업이 추가되는 것은 복잡도가 추가되는 것과 같습니다

194
00:15:50,160 --> 00:15:57,060
tensor를 numpy array로 변환하려는 경우 tensor가 잘 분리되었는지 확인해야 합니다

195
00:15:57,060 --> 00:16:01,440
tensor가 gradient를 가지고 있는지 확인하고, CPU로 이동하고, 그 다음에 numpy로 변환합니다

196
00:16:01,440 --> 00:16:06,220
이런 작업들을 하는 게 머리가 아플 수도 있습니다

197
00:16:06,220 --> 00:16:10,959
그래서 우리는 이 멋진 유틸리티 함수를 사용할 건데, 나중에 과제에서도 사용할 수 있을 겁니다

198
00:16:10,959 --> 00:16:15,759
이것들은 ptu.from_numpy()와 to_numpy() 함수입니다

199
00:16:15,759 --> 00:16:24,320
기본적으로 from_numpy()가 하는  일은 numpy array를 GPU에 있는 PyTorch tensor로 변환해주는 것입니다

200
00:16:24,320 --> 00:16:30,399
to_numpy()는 GPU에 있는 tensor를 받아 먼저 CPU로 이동한 다음 numpy로 변환합니다

201
00:16:30,399 --> 00:16:36,800
따라서 이 두 기능을 사용하면 더 이상 GPU를 사용하고 있는지 아닌지 여부에 대해 생각할 필요가 없습니다

202
00:16:36,800 --> 00:16:42,320
지원되는 경우 GPU에서 모든 tensor 작업을 수행하게 됩니다

203
00:16:44,079 --> 00:16:46,560
이제 GPU에 대해 조금 배웠습니다

204
00:16:46,560 --> 00:16:53,920
그럼 이제 배운 내용을 이전의 동일한 sine wave 학습 예제에 적용해 보겠습니다

205
00:16:54,639 --> 00:16:58,880
이것이 model 학습의 전체 흐름입니다

206
00:16:58,880 --> 00:17:02,959
일부분이 새로운 것만 제외하면 모든 것이 이전과 거의 동일합니다

207
00:17:02,959 --> 00:17:04,720
neural network를 정의하고 난 후에

208
00:17:04,720 --> 00:17:07,839
이 부분은 GPU를 사용하고 싶다면 추가해야 할 것들입니다

209
00:17:07,839 --> 00:17:09,919
기본적으로 device를 정의합니다

210
00:17:09,919 --> 00:17:15,919
사용 가능한 경우 GPU를 사용하고 그렇지 않은 경우 CPU를 사용하는 방법으로 정의하는 것이 좋은 방법입니다

211
00:17:15,919 --> 00:17:20,079
코드가 어느 쪽이든 호환 가능하게 됩니다

212
00:17:20,240 --> 00:17:28,079
그런 다음 neural network을 새로운 device로 이동합니다

213
00:17:28,079 --> 00:17:33,840
일반적으로 실무에서는 neural network만 먼저 이동하고 target을 포함한 모든 대상을 이동하지는 않습니다

214
00:17:33,840 --> 00:17:39,840
그 이유는 거대한 데이터로 작업하게 되면 한 번에 모든 것을 GPU에 넣는 것은 불가능하기 때문입니다

215
00:17:39,840 --> 00:17:44,960
대신 batch 방식으로 작업하는데

216
00:17:44,960 --> 00:17:51,679
stochastic gradient descent나 mini batch gradient descent를 하면

217
00:17:51,679 --> 00:17:59,760
gradient descent 코드의 for 반복문 내에서 각 batch를 GPU로 개별적으로 이동할 수 있습니다

218
00:17:59,760 --> 00:18:06,160
하지만 이 예제에서는 target 데이터가 충분히 작으므로 한 번에 모두 GPU로 이동합니다

219
00:18:06,480 --> 00:18:10,670
그리고 우리가 여기 학습 코드에 추가한 것은

220
00:18:10,670 --> 00:18:13,720
input 데이터를 CPU에서 GPU로 옮기는 것입니다

221
00:18:13,720 --> 00:18:21,280
따라서 일반적으로 이것은 y_target = y.to(device)와 같은 작업을 수행합니다

222
00:18:21,280 --> 00:18:26,240
앗 그게 아니라 y_target.to(device)군요

223
00:18:27,280 --> 00:18:31,440
그 밖의 것들은 이전과 대부분 같습니다

224
00:18:32,640 --> 00:18:37,600
그리고 이 코드를 실행해보면 훨씬 더 빨리 학습된다는 것을 볼 수 있습니다

225
00:18:37,600 --> 00:18:45,280
이미지와 같은 데이터나 고차원 데이터로 작업을 해보면 이 차이가 훨씬 더 상당해집니다

226
00:18:45,280 --> 00:18:47,039
지금 당장은 작은 차이지만요

227
00:18:47,039 --> 00:18:51,600
여러분은 확실히 모든 과제에 이 방법을 사용하고 싶어질 겁니다

228
00:18:55,200 --> 00:18:58,080
이제 예제를 계속 살펴봅시다

229
00:18:58,080 --> 00:19:07,039
우리는 model을 평가해볼 겁니다.
이것은 일반적으로 PyTorch model로 예측을 하는 방식입니다

230
00:19:07,039 --> 00:19:11,120
여기에 두 가지를 추가했습니다

231
00:19:11,120 --> 00:19:16,720
먼저 net.eval()을 통해 하는 작업은 network를 평가 모드로 전환하는 것입니다

232
00:19:16,720 --> 00:19:19,200
우리 예제에서는 해당하지 않습니다만

233
00:19:19,200 --> 00:19:27,200
만약 neural network가 dropout 혹은 batch norm 같이 학습 과정과 테스트 과정에서 다르게 동작해야 하는 경우

234
00:19:27,200 --> 00:19:34,400
일관성 없는 결과를 제공하지 않도록 network가 평가 모드에 있는지 확인해야 합니다

235
00:19:34,400 --> 00:19:37,360
다음에 살펴볼 것은 torch.no_grad()입니다

236
00:19:37,360 --> 00:19:40,400
이것은 예측을 할 때 사용하는 훌륭한 도우미 기능입니다

237
00:19:40,400 --> 00:19:51,360
이걸 사용하면 이 코드 블록 안에 있는 tensor들은 require_grad가 False로 설정됩니다

238
00:19:51,360 --> 00:19:59,919
평가를 하는 때에는 backpropagation을 하지 않기 때문에 gradient를 추적하는 것을 신경 쓸 필요가 없습니다

239
00:19:59,919 --> 00:20:06,720
그래서 이는 PyTorch에게 여기 안에 있는 gradient들에 대해 
신경 쓰지 않는다고 알려주어 약간의 성능 향상을 얻는 것입니다

240
00:20:06,720 --> 00:20:15,280
따라서 이 두 번째 것은 코드를 좀 더 효율적으로 만들 수 있긴 하지만 덜 중요하긴 합니다

241
00:20:15,280 --> 00:20:20,640
이 코드를 쓰지 않아도 크게 상관 없습니다. 실수로 backward() 함수를 실수로 호출하는 것만 아니면 괜찮습니다

242
00:20:20,640 --> 00:20:24,960
하지만 이 net.eval()은 특히 dropout과 같은 작업을 수행하는 경우 매우 중요합니다

243
00:20:24,960 --> 00:20:29,120
그렇지 않으면 결과가 정확하지 않을 것입니다

244
00:20:29,280 --> 00:20:32,480
이제 변수 y에 대한 예측을 가지고 있습니다

245
00:20:32,480 --> 00:20:41,280
그리고 이것들이 GPU에 있기 때문에 먼저 CPU로 변환하고
 그 다음 numpy를 호출하여 numpy arrays로 변환해야 합니다

246
00:20:42,480 --> 00:20:45,840
작업이 잘 수행되었다는 것을 알 수 있습니다

247
00:20:46,640 --> 00:20:53,520
학습과 평가 작업 후에 마지막으로 하게 될 일은 model의 weight를 저장하는 것입니다

248
00:20:53,520 --> 00:20:57,360
나중에 이 학습된 model을 사용하여 다른 작업을 수행할 수 있습니다

249
00:20:57,360 --> 00:21:05,840
PyTorch에서 모델을 저장하는 방법은 기본적으로 parameter들을 가져와서

250
00:21:05,840 --> 00:21:10,080
dictionary에 저장한 다음 특정 종류의 파일로 직렬화하여 저장합니다

251
00:21:10,080 --> 00:21:14,000
여기에서는 checkpoint 파일 경로를 특정화했습니다

252
00:21:14,000 --> 00:21:22,960
일반적으로는 PyTorch용 파일의 확장자를 .pt로 만들긴 하지만, 중요한 것이 아니라서 원하는대로 하면 됩니다

253
00:21:22,960 --> 00:21:29,360
그 다음 torch.save()를 호출하고 그 안에 dictionary를 반환하는 net.state_dict()를 넘겨주는데

254
00:21:29,360 --> 00:21:36,320
이 dictionary는 model의 parameter뿐만 아니라 예측하는 데 있어 중요한 다른 것들을 모두 제공해줍니다

255
00:21:36,320 --> 00:21:42,640
예를 들면 model에 batch norm을 적용했을 경우 기록한 running mean을 제공하는 방식입니다

256
00:21:42,640 --> 00:21:47,080
이것이 model을 저장할 때 state_dict()가 parameters()와 약간 다른 점입니다

257
00:21:47,080 --> 00:21:51,679
parameter들만 원하는 것이 아니라 model의 전체 상태를 원하는 경우에 쓸 수 있습니다

258
00:21:51,679 --> 00:21:56,159
그래서 이 dictionary를 이 경로에 저장하겠습니다

259
00:21:57,760 --> 00:22:05,600
그리고 나중에 다시 돌아와서 이 checkpoint를 model에 다시 로드하고 다시 예측하고 싶다고 가정해 보겠습니다

260
00:22:05,600 --> 00:22:07,280
먼저 할 일은 새로운 model을 초기화하는 것입니다

261
00:22:07,280 --> 00:22:12,480
이 때 이전과 동일한 구조여야만 합니다

262
00:22:12,480 --> 00:22:20,000
그런 다음 load_state_dict()를 호출하고 이전에 저장한 파일을 로드합니다

263
00:22:20,000 --> 00:22:24,880
그리고 평가를 수행하려면 eval() 함수를 호출하는 것을 잊지 말아야 합니다

264
00:22:25,360 --> 00:22:29,760
만약 이것을 실행해보면

265
00:22:30,320 --> 00:22:32,880
봅시다

266
00:22:36,960 --> 00:22:41,799
잘 실행되는 것 같습니다

267
00:22:46,640 --> 00:22:51,440
둘 다 GPU에 올리면 이제 parameter들을 비교할 수 있습니다

268
00:22:51,440 --> 00:23:01,840
그러면 로드된 checkpoint가 원래 model과 정확히 동일한 이름과 parameter들을 갖고 있음을 알 수 있습니다

269
00:23:02,000 --> 00:23:09,280
우리가 한 일을 요약하자면 우리가 정의한 loss function, optimizer, 그리고 neural net을 활용해 model을 학습하는 것으로 시작했습니다

270
00:23:09,280 --> 00:23:13,280
그리고 우리가 학습시키고 싶은 것들을 GPU로 이동시켰습니다

271
00:23:13,280 --> 00:23:19,200
또 model을 평가하기 위해 model을 평가 모드로 설정하고 gradient를 끄는 작업을 했습니다

272
00:23:19,200 --> 00:23:28,200
그런 다음 마지막으로 model을 checkpoint 파일에 저장한 후 나중에 로드해서 예측을 다시 수행하는 데 사용했습니다

