1
00:00:01,120 --> 00:00:03,040
저는 오늘 강의의 마지막 부분에서

2
00:00:03,040 --> 00:00:05,200
우리가 흥미로운 의사 결정과 문제 해결을

3
00:00:05,200 --> 00:00:06,960
하기위해 모방학습을

4
00:00:06,960 --> 00:00:09,280
사용 할 수있는 몇 가지 다른 방법들에

5
00:00:09,280 --> 00:00:11,120
대한 더 나아간 자료들에 대해

6
00:00:11,120 --> 00:00:13,679
이야기를 하고 싶습니다.

7
00:00:13,679 --> 00:00:16,160
그래서 제가 말하고자 하는

8
00:00:16,160 --> 00:00:17,680
이 다른 모방 아이디어는

9
00:00:17,680 --> 00:00:20,880
한 가지 작업에 대해

10
00:00:20,880 --> 00:00:23,199
최적이 아닌 데이터가 다른 작업에

11
00:00:23,199 --> 00:00:24,960
최적이 될 수 있는지에 대한 것 과,

12
00:00:24,960 --> 00:00:26,560
우리가 어떻게 그 관찰을

13
00:00:26,560 --> 00:00:30,000
모방 학습에 활용할 수 있는지를 의미합니다.

14
00:00:30,000 --> 00:00:31,670
여러분의 에이전트의 궤적이

15
00:00:31,670 --> 00:00:34,160
p1에 도달했다고 가정해봅시다.

16
00:00:34,160 --> 00:00:35,760
만약 여러분이 p1에 도달하는 궤적을

17
00:00:35,760 --> 00:00:37,760
많이 가지고 있다면, 여러분은 제한된 학습의

18
00:00:37,760 --> 00:00:39,200
궤적을 사용할 수 있고 p1에

19
00:00:39,200 --> 00:00:42,710
도달하는 방법을 배울 수 있습니다.

20
00:00:43,440 --> 00:00:44,870
그러나 하나는 p1에 도달하고

21
00:00:44,870 --> 00:00:46,640
다른 하나는 p2에 도달하고

22
00:00:46,640 --> 00:00:48,390
또 다른 하나는 p3에 도달하는

23
00:00:48,390 --> 00:00:49,920
각기 다른 지점에 도달하는

24
00:00:49,920 --> 00:00:51,280
다양한 선지를 가지고 있다면

25
00:00:51,280 --> 00:00:52,870
지금 당신이 겪고 있는 문제는

26
00:00:52,870 --> 00:00:54,390
다양한 포인트중 하나에 대한

27
00:00:54,390 --> 00:00:57,190
데모가 충분하지 않다는것일지도 모릅니다

28
00:00:57,190 --> 00:00:59,030
만약 당신이 p1에 대한

29
00:00:59,030 --> 00:01:00,000
데모를 사용한다면

30
00:01:00,000 --> 00:01:01,350
실질적으로 p1에 성공적으로

31
00:01:01,350 --> 00:01:04,150
도달하는것을 배우기 힘들것입니다.

32
00:01:04,150 --> 00:01:06,150
지금 도달한 기점을 기준으로

33
00:01:06,150 --> 00:01:08,320
정책(policy)를 조건화하면 어떨까요?

34
00:01:08,320 --> 00:01:09,920
각 점에 대해서 하나의 시연만

35
00:01:09,920 --> 00:01:11,680
가지고 있다고 해도말입니다

36
00:01:11,680 --> 00:01:14,000
당신은 단지 p를

37
00:01:14,000 --> 00:01:16,240
상태에 추가하기 위한

38
00:01:16,240 --> 00:01:18,150
조건부 정책 종류를 배울 수 있습니다.

39
00:01:18,150 --> 00:01:19,680
그리고 이러한 방법으로 추가정보에

40
00:01:19,680 --> 00:01:22,560
당신의 정책(policy)를 조건화 함으로써

41
00:01:22,560 --> 00:01:23,920
그 조건부 정책은 p1을 나타내는

42
00:01:23,920 --> 00:01:24,790
어떤 지점에 도달할 수 있을 것입니다

43
00:01:24,790 --> 00:01:27,920
실제로 해당 작업에 대한 데이터가

44
00:01:27,920 --> 00:01:29,920
충분하지 않더라도 일부작업을

45
00:01:29,920 --> 00:01:32,960
수행하기 위한 정책(policy)를 교육할 수 있습니다

46
00:01:32,960 --> 00:01:34,640
이 아이디어를 한 가지 혹은 다른형태로

47
00:01:34,640 --> 00:01:36,400
사용한 많은 작품들이 있습니다.

48
00:01:36,400 --> 00:01:39,040
우리는 이 목표 조건을

49
00:01:39,040 --> 00:01:40,150
행동 복제라고 부를 것입니다

50
00:01:40,150 --> 00:01:42,390
그리고 훈련하는 동안 우리는

51
00:01:42,390 --> 00:01:43,600
일반적으로 모든 다른 것들을 하는

52
00:01:43,600 --> 00:01:48,720
많은 시연들을 관찰할 것입니다.

53
00:01:48,720 --> 00:01:50,000
성공적인 사례로 취급할 것입니다.

54
00:01:50,000 --> 00:01:52,640
그 데모가 실제로 성공한 것이

55
00:01:52,640 --> 00:01:54,000
무엇이든 간에 말입니다

56
00:01:54,000 --> 00:01:56,560
이러한 내용들이 제가 간단히 말씀드리고자 하는내용입니다.

57
00:01:56,560 --> 00:01:57,920
그래서 만약 해당 데모가 s_t에 도달 했다면

58
00:01:57,920 --> 00:02:00,390
그것은 's_t'에 도달하기 위한 성공적인 시연이고,

59
00:02:00,390 --> 00:02:03,920
조건부 상태 정책 및 조건상태를

60
00:02:03,920 --> 00:02:09,030
목표에 따라 배울 것입니다.

61
00:02:09,030 --> 00:02:10,950
그래서 이것이 작동하는 방식은

62
00:02:10,950 --> 00:02:13,200
각각의 데모에서 기본적으로 도달한

63
00:02:13,200 --> 00:02:14,560
마지막 목표를 선택한 다음

64
00:02:14,560 --> 00:02:18,160
규칙적인 행동을 복제하는 것이고,

65
00:02:18,160 --> 00:02:20,310
이러한 아이디어는 몇몇 논문에 사용되어 왔습니다.

66
00:02:20,310 --> 00:02:22,230
여기에는 몇 가지 예시들이 있습니다.

67
00:02:22,230 --> 00:02:23,590
제가 처음으로 말하고자 하는 것은

68
00:02:23,590 --> 00:02:26,160
'latent plants'라 불리는 것을 놀이로 부터 배우는것입니다.

69
00:02:26,160 --> 00:02:28,080
그리고 이 논문에서 아이디어는

70
00:02:28,080 --> 00:02:29,440
특별히 아무것도 하지 않는

71
00:02:29,440 --> 00:02:31,360
인간 표본으로부터

72
00:02:31,360 --> 00:02:32,870
데이터를 수집하는 것이었습니다.

73
00:02:32,870 --> 00:02:33,360
하지만 실제로는

74
00:02:33,360 --> 00:02:35,120
다양한 행동들을 시도하고 있습니다.

75
00:02:37,510 --> 00:02:40,230
그러면 이 데이터는 기본적으로

76
00:02:40,230 --> 00:02:43,040
마지막 상태를 목표로 선택하도록 처리됩니다.

77
00:02:43,040 --> 00:02:44,560
이 논문의 실제 모델

78
00:02:44,560 --> 00:02:46,870
아키텍처는

79
00:02:46,870 --> 00:02:48,230
앞서 설명한 이유들로 인해

80
00:02:48,230 --> 00:02:49,510
약간 복잡합니다.

81
00:02:49,510 --> 00:02:50,400
이것이 작동하기 위해서는

82
00:02:50,400 --> 00:02:52,230
모방이 매우 강력할 필요가 있습니다

83
00:02:52,230 --> 00:02:53,590
즉, 전문가와 매우 정확하게

84
00:02:53,590 --> 00:02:54,400
일치시킬 필요가 있습니다.

85
00:02:54,400 --> 00:02:56,080
이것은 그것이 다중 모드와 non-markovian에 대한

86
00:02:56,080 --> 00:02:59,680
문제를 다루어야 한다는 것을 의미합니다.

87
00:02:59,680 --> 00:03:01,440
이것이 하는 방법은

88
00:03:01,440 --> 00:03:03,510
실제로 자기 회귀 이산화와

89
00:03:03,510 --> 00:03:04,640
잠재 변수 모델의

90
00:03:04,640 --> 00:03:06,480
조합을 사용하는 것입니다.

91
00:03:06,480 --> 00:03:08,400
만약 당신이 잠재 변수 모델에서

92
00:03:08,400 --> 00:03:11,680
기억한다면 우리는 정책에 추가 노이즈를 입력을 공급하고

93
00:03:11,680 --> 00:03:13,360
그 노이즈 입력은 우리가 임의의 출력 분포를

94
00:03:13,360 --> 00:03:16,310
나타낼 수 있도록 합니다.

95
00:03:16,310 --> 00:03:18,080
즉 그것은 조금더 정교해진

96
00:03:18,080 --> 00:03:19,680
모방학습 아키텍처입니다.

97
00:03:19,680 --> 00:03:21,360
그러나 전체적인 알고리즘은

98
00:03:21,360 --> 00:03:24,230
이전 슬라이드에서 제가 가지고 있던 방식을 따릅니다.

99
00:03:24,230 --> 00:03:26,150
그리고 이 정책이 훈련되면 그것은 실제로

100
00:03:26,150 --> 00:03:28,950
다양한 다른 목표에 도달할 수 있습니다.

101
00:03:28,950 --> 00:03:30,790
여기 왼쪽에 있는 이미지는

102
00:03:30,790 --> 00:03:32,870
목표와 비디오를 보여줍니다.

103
00:03:32,870 --> 00:03:34,400
오른쪽에 있는 이미지는

104
00:03:34,400 --> 00:03:36,150
실제로 그 목표에 도달하는 도달하려는 정책을 보여줍니다

105
00:03:36,150 --> 00:03:40,870
그리고 이것은 다양한 다른 훈련 데이터에 훈련되는 당일 정책입니다

106
00:03:41,120 --> 00:03:45,200
이 결과를 보고 우리가 상상 할 수 있는

107
00:03:45,200 --> 00:03:48,080
또 다른 아이디어는 만약 우리가 하나의

108
00:03:48,080 --> 00:03:50,480
특정한 작업에서 성공하기 위해 데모가 필요하지 않다면

109
00:03:50,480 --> 00:03:52,480
잘 해낼 수 있다는 것입니다 but we can

110
00:03:52,480 --> 00:03:54,080
그러나 우리는 대신 이 조건화된 트릭으로

111
00:03:54,080 --> 00:03:55,430
광범위한 다른 작업에

112
00:03:55,430 --> 00:03:57,760
데모를 사용할 수 있습니다.

113
00:03:57,760 --> 00:03:59,680
우리에게 그것들이 데모가 될 필요가 있을까요?

114
00:03:59,680 --> 00:04:03,040
대신에 우리는 나쁜상태의

115
00:04:03,040 --> 00:04:04,230
무작위 데이터를가지고 그 무작위 데이터를

116
00:04:04,230 --> 00:04:06,150
시연으로 사용할 수 있을까요?

117
00:04:06,150 --> 00:04:08,150
만약 당신이 나쁜 궤젹을 가지고 있더라도

118
00:04:08,150 --> 00:04:09,920
그 궤적이 어떤 상태에 도달한다면

119
00:04:09,920 --> 00:04:11,280
어떤 상태에 도달 했든 간에

120
00:04:11,280 --> 00:04:13,510
그것은 실제로 좋은 궤적일 것입니다.

121
00:04:13,510 --> 00:04:14,870
그리고 그것이 이 논문에 설명된 알고리즘이며

122
00:04:14,870 --> 00:04:16,790
해당 알고리즘은 반복된 supervised learning(지도학습)을 통해

123
00:04:16,790 --> 00:04:18,950
목표에 도달하는 학습입니다.

124
00:04:18,950 --> 00:04:20,320
여기서 전체적인 레시피는

125
00:04:20,320 --> 00:04:21,840
처음부터 시작하는 것입니다.

126
00:04:21,840 --> 00:04:23,440
사람이 제공한 데이터는 전혀 없습니다

127
00:04:23,440 --> 00:04:25,680
대신 무작위 정책으로 시작합니다

128
00:04:25,680 --> 00:04:28,240
랜덤 정책(policy)이 랜덤 데이터를 수집하지만

129
00:04:28,240 --> 00:04:30,320
랜덤 데이터가 도달한 목표에 따라

130
00:04:30,320 --> 00:04:32,160
다시 학습되므로

131
00:04:32,160 --> 00:04:33,750
랜덤 궤적이 s_t 상태에

132
00:04:33,750 --> 00:04:36,960
도달했다면 목표 s_t에 대한 좋은 시연입니다.

133
00:04:36,960 --> 00:04:39,440
그리고 나서 무작위보다

134
00:04:39,440 --> 00:04:40,800
더 나은 목표에 도달하기 위한

135
00:04:40,800 --> 00:04:42,160
더 나은 정책을 제공하는

136
00:04:42,160 --> 00:04:43,280
정책을 재교육하는 데 

137
00:04:43,280 --> 00:04:45,280
사용됩니다

138
00:04:45,280 --> 00:04:46,400
이 과정을 반복하면

139
00:04:46,400 --> 00:04:48,000
내부 루프 모방 학습 절차와

140
00:04:48,000 --> 00:04:49,360
이 라벨링 단계 만으로

141
00:04:49,360 --> 00:04:51,190
상당히 복잡한 강화 학습 과제를

142
00:04:51,190 --> 00:04:53,120
해결할 수 있다는 것을

143
00:04:53,120 --> 00:04:57,840
알 수 있습니다.

