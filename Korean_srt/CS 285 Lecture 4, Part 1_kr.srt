1
00:00:00,880 --> 00:00:04,524
cs285 4강에 오신 것을 환영합니다

2
00:00:05,120 --> 00:00:13,530
이번 강의에서는, 강화학습 알고리즘 정의와 기본 개념에 대한 포괄적인 소개를 진행할 예정입니다.

3
00:00:14,246 --> 00:00:16,591
몇몇 정의들과 함께 시작해보겠습니다

4
00:00:16,952 --> 00:00:22,976
먼저, 지난 모방 학습 시간 때 얘기했던 용어들을 짚어보겠습니다.

5
00:00:23,039 --> 00:00:30,559
우리는 관측 o_t가 주어졌을 때 행동 a_t를 하는 분포를 policy라고 표현할 수 있다는 것을 배웠습니다.

6
00:00:30,560 --> 00:00:40,159
우리는 이 정책을 π라고 부르고, 종종 θ라는 첨자를 넣어서 표현합니다.

7
00:00:40,160 --> 00:00:45,691
알아야할 것은 θ는 딥러닝 기반으로 정책을 표현할 때 사용되며,
이 때 정책은 딥러닝 파라미터 벡터들의 집합이라는 것을 의미합니다.

8
00:00:45,747 --> 00:00:51,858
자세한 내용은 강화학습의 종류에 따라 조만간 배울 예정입니다.

9
00:00:51,883 --> 00:00:59,119
우리는 가치 함수같은 예를 통해 정책을 직접적으로나 간접적으로 선택할 수 있습니다.

10
00:00:59,120 --> 00:01:05,880
중요하게 알아야할 정의로는 
상태는 s_t, 관측은 o_t, 행동은 a_t로 표현하기로 배웠습니다.

11
00:01:05,905 --> 00:01:17,438
모방 학습 강의 시간에 배웠던 것처럼, 관측과 상태는 행동과 관측이 정책이 되는 그래프 모델에 따라 연결될 수 있습니다.

12
00:01:17,493 --> 00:01:26,493
현재 상태와 현재 행동, 미래 상태 사이의 선들은 전이 확률이거나, 마코프 상태를 만족시키는 상태입니다.

13
00:01:26,573 --> 00:01:35,956
이 말은 현재 상태 s_t가 주어졌을 때, s_t+1은 s_t-1과 독립적이라는 뜻입니다.

14
00:01:36,059 --> 00:01:40,713
마코프 상태는 관측과 상태를 구분짓는 중요한 상태입니다.

15
00:01:40,799 --> 00:01:45,039
상태는 마코프 상태를 만족시키는 반면에, 관측 값은 그렇지 않습니다.

16
00:01:45,040 --> 00:01:55,571
모방 학습 강의에서 관측은 모든 상태를 포함하는 정보를 포함할 수도 있고, 아닐 수도 있는
확률 함수라는 것을 배웠습니다.

17
00:01:55,799 --> 00:01:58,216
이것이 관측과 상태의 주된 차이입니다.

18
00:01:58,240 --> 00:02:10,960
우리는 모든 상태(s)에 대해 접근할 수 있는 전체 관측 강화학습과
관측(o)만 접근할 수 있는 부분 관측 강화학습 알고리즘에 대해 이야기할 것입니다.

19
00:02:11,120 --> 00:02:14,168
(그림에 보이는 내용) 저 내용이 마코프 속성입니다.

20
00:02:14,480 --> 00:02:26,187
앞으로는 부분 관측 상태인지 전체 관측 상태인지에 따라 정책을 
π_θ(a_t /o_t) 나 π_θ(a_t / s_t)로 나누어 표현할 예정입니다.

21
00:02:26,215 --> 00:02:31,636
가끔 실수로 o_t인데도 s_t를 쓸 수도 있습니다.

22
00:02:31,661 --> 00:02:37,840
이런 경우엔 구분이 중요해서 따로 언급을 할 예정입니다.

23
00:02:38,019 --> 00:02:47,279
모방학습에서 우리는 관측과 행동 쌍으로 이루어진 주행 데이터를 모았습니다.

24
00:02:47,280 --> 00:02:54,399
그리고 지도학습 알고리즘을 사용해 정책이 의견을 모아 행동을 취하는 방법에 대해 이해하도록 학습시켰습니다.

25
00:02:54,400 --> 00:03:05,279
이번 강의에서는, 이런 전문가 데이터에 접근할 필요 없이 학습시키는 방법에 대해 소개하겠습니다.

26
00:03:05,280 --> 00:03:15,119
그렇게 하기 위해선 정책이 무엇을 하기를 원하는지 정의하고, 보상 함수라는 것을 통해 목적을 정의해야 합니다.

27
00:03:15,120 --> 00:03:23,119
우리는 데이터가 없더라도 그림과 같이 차를 운전하는 상황 속에서 어떤 상황이 더 좋고 나쁜지 말할 수 있습니다.

28
00:03:23,120 --> 00:03:29,635
보상 함수란 기본적으로 상태와 행동에 대한 스칼라 값의 함수를 의미합니다.

29
00:03:29,675 --> 00:03:35,279
보상함수는 때때로 상태에 의존적일 수도 있지만, 보통은 상태와 행동에 의존적입니다.

30
00:03:35,327 --> 00:03:39,148
그리고 보상함수는 어떤 상태와 행동이 더 좋은지를 말해줍니다.

31
00:03:39,219 --> 00:03:47,999
예를 들어 운전중이라고 한다면, 도로에서 빠르게 달리면 높은 보상을 줄 수 있는 상태일 겁니다.

32
00:03:48,000 --> 00:03:53,039
반면에, 다른 차에 부딪히는 상황은 낮은 보상값을 가지는 상태일 겁니다.

33
00:03:53,040 --> 00:04:05,074
강화학습에서 중요한 목표는 지금 높은 보상을 갖는 행동을 취하는 게 아니라,
나중에 높은 보상을 받도록 행동을 이끄는 것입니다.

34
00:04:05,120 --> 00:04:14,079
만약 차를 너무 빠르게 몰아 높은 보상을 받더라도 차가 부딪힌다면 보상이 낮아지게 됩니다.

35
00:04:14,080 --> 00:04:18,386
따라서 행동을 선택할 때 미래 보상을 고려해야 합니다.

36
00:04:18,410 --> 00:04:21,767
의사를 결정하는 것은 매우 어렵지만,

37
00:04:21,767 --> 00:04:30,457
나중에 큰 보상을 받을 수 있는 지금 행동을 선택하는 것이 강화학습의 핵심입니다.

38
00:04:30,516 --> 00:04:44,478
상태(a), 보상(r), 전이 확률(p)은 우리가 마코프 결정 과정(MDP)이라고 부르는 것을 결정하고,
마코프 결정 과정은 마코프 상태에 대한 결정 과정을 의미합니다.

39
00:04:44,479 --> 00:04:50,417
마코프 결정 과정의 완전한 공식적인 정의를 만들어 봅시다.

40
00:04:50,442 --> 00:04:53,695
"마코프 연쇄" 부터 시작해보겠습니다.

41
00:04:53,720 --> 00:05:02,959
마코프 연쇄는 마코프 연쇄의 상위 개념인 확률적 과정의 연구를 개척한 andrei markov에서 유래됐습니다.

42
00:05:02,960 --> 00:05:12,420
마코프 연쇄는 상태 S와 전이 함수 T 2개로만 이루어진 간단한 정의를 가집니다.

43
00:05:12,515 --> 00:05:17,758
S는 이산적이거나 연속적일 수 있는 집합입니다.

44
00:05:17,759 --> 00:05:23,359
따라서 S는 유한한 크기를 갖는 이산적 상태일 수도 있고,

45
00:05:23,360 --> 00:05:31,918
실제 세계와 유사한 연속적인 상태일 수도 있습니다.

46
00:05:31,919 --> 00:05:42,055
T는 전이 연산자로, 전이 확률이나 조건부 확률 분포를 지정하는 함수로 표현될 수 있습니다.

47
00:05:42,080 --> 00:05:51,976
마코프 연쇄 상태에서 T는 
s_t가 주어졌을 때, s_t+1 일 확률을 나타냅니다.

48
00:05:52,001 --> 00:06:00,478
연산자라 불리는 이유는 시간 t일 때, 각 상태 확률을 벡터로 표현하기 때문입니다.

49
00:06:00,479 --> 00:06:09,487
N개의 상태가 있다고 가정했을 때, 우리는 이를 i번째 상태 확률에 대해 μ(t, i)으로 부를 수 있습니다.

50
00:06:09,600 --> 00:06:19,439
전체 벡터는 μ(t)로 부를 수 있고, 우리는 전이확률을 행렬로 표현할 수 있습니다.

51
00:06:19,440 --> 00:06:25,463
행렬 i,j 번째 성분은 j상태일 때 i 상태에 대한 확률입니다.

52
00:06:25,536 --> 00:06:42,863
그러면 우리는 상태 확률 μ_t 와 T의 곱을 통해
다음 스텝 상태 확률 벡터 μ_(t+1)를 그림처럼 단순하게 표현할 수 있게 됩니다.

53
00:06:42,888 --> 00:06:50,784
이는 약간의 선형대수를 이용해 확률의 연쇄 법칙을 표현한 방법입니다.

54
00:06:50,809 --> 00:07:07,039
하지만 여기서 볼 수 있듯이, μ에 적용된 T는 선형 연산자로, 
상태 확률의 현재 벡터에 적용하면 다음 벡터가 생성되므로, 이 것이 우리가 전이 연산자라 부르는 이유입니다.

55
00:07:07,432 --> 00:07:12,047
여기에 마코프 연쇄에 대한 그림이 있습니다.

56
00:07:12,080 --> 00:07:17,840
그림에 표현된 선은 전이 확률입니다.

57
00:07:18,319 --> 00:07:33,360
물론, 마코프 연쇄에서의 상태는 현재 상태 t가 주어졌을 때,
 t-1의 상태와 t+1가 조건부 독립이라는 마코프 속성을 만족시킨다는 것을 의미합니다.

58
00:07:34,080 --> 00:07:38,799
마코프 연쇄 그 자체는 의사 결정 문제를 특정하지는 못합니다.

59
00:07:38,800 --> 00:07:42,375
왜냐하면 여기엔 행동에 대한 개념이 없기 때문입니다.

60
00:07:42,400 --> 00:07:49,439
따라서 행동에 대해 넘어가기 위해, 마코프 연쇄를 결정 과정으로 변환이 필요합니다.

61
00:07:49,440 --> 00:07:54,511
이 방법은 1950년대 이후에 발견되었습니다.

62
00:07:54,741 --> 00:08:03,375
마코프 결정 과정은 마코프 연쇄에서 추가로 행동 공간과 보상 함수가 더해집니다.

63
00:08:03,400 --> 00:08:07,484
우리는 이산적이거나 연속적인 집합의 *상태 공간*을 갖고 있고,

64
00:08:07,508 --> 00:08:11,758
이산적이거나 연속적인 집합의 *행동 공간*을 갖고 있습니다.
we have an action space which is also a discrete or continuous set

65
00:08:11,759 --> 00:08:24,959
그래프 모델은 이제 상태와 행동을 모두 포함하고 전이 확률은 상태와 동작 모두에 따라 결정됩니다.
이를 p(s_t+1/s_t,a_t)로 표현한 걸 볼 수 있습니다.

66
00:08:24,960 --> 00:08:31,800
T는 여전히 전이 연산자로 불리고 있지만, 이제는 행렬(2차원)이 아니라 텐서(3차원)입니다.

67
00:08:31,825 --> 00:08:37,440
왜냐하면 다음 상태와 현재 상태, 그리고 현재 행동이라는 3개의 차원을 가지고 있기 때문입니다.

68
00:08:37,888 --> 00:08:46,918
하지만 우리는 여전히 선형대수 기법을 사용할 수 있습니다.

69
00:08:46,943 --> 00:08:53,518
μ_(t,j)가 t시간일 때 상태 j의 확률을 의미한다고 가정하면, 우리는 행동을 취하는 벡터도 얻을 수 있고,

70
00:08:53,543 --> 00:09:02,811
T를 T(i,j,k)로 쓰면 j 상태에서 k 행동을 취하고, i 상태로 들어가는 확률로 표현할 수 있습니다.

71
00:09:02,836 --> 00:09:20,375
그러면 현재 상태 확률과 현재 행동 확률, 전이 확률의 선형 함수로
다음 단계의 상태 확률 μ(t+1,i)를 선형 변환으로 표현할 수 있습니다.

72
00:09:20,415 --> 00:09:32,880
그러면 이 전이 연산자는 텐서 형태이더라도 현재 상태와 상태 확률을 
다음 단계의 상태 확률로 변환시켜주는 선형 연산자가 되는 것이 됩니다.

73
00:09:33,519 --> 00:09:43,571
보상 함수는 행동 공간 내에서 상태의 행렬 곱셈 연산을 통해 실수값으로 대응시킵니다.

74
00:09:43,795 --> 00:09:48,317
그리고 이게 강화학습의 목표를 정의할 수 있도록 합니다.

75
00:09:48,731 --> 00:09:58,494
우리는 R(s_t, a_t)를 보상이라 부르고, 지금부터는 소개할 몇 장에 걸쳐 강화학습의 목적인
전체 보상을 최대화시키는 것에 대해 소개하겠습니다.

76
00:09:58,810 --> 00:10:11,015
소개하기 전에, 마코프 결정 과정을 확장시켜서 관측의 개념을 가져올 수 있도록
부분 관찰 마코프 결정 과정(POMDP)에 대해서도 정의해보겠습니다.

77
00:10:11,040 --> 00:10:24,310
부분 관찰 의사 결정(POMDP)은 관측 공간 o와 관측 가능성 e 두개의 추가 용어로 정의를 확장시킵니다.

78
00:10:24,570 --> 00:10:31,494
다시 처음부터 말하면, s는 상태 공간, a는 행동 공간, o는 관측 공간입니다.

79
00:10:31,519 --> 00:10:39,999
그림을 보면 MDP에서 대한 것과 마찬 가지로, 상태에 따라 관찰이 달라지는 걸 볼 수 있습니다.

80
00:10:40,000 --> 00:10:50,615
우리는 전과 마찬가지로 전이 연산자와 출력 확률 p(o_t / s_t), 보상 함수를 가지고 있습니다.

81
00:10:50,640 --> 00:11:09,200
보상 함수는 여전히 상태와 행동으로부터 실수로 대응시키며, 보상 함수의 규칙은 전체 관측이 아니라 
일반적으로 진짜 상태에 대한 접근 없이 결정을 내려야 하는 POMDP나 PALMDP에서의 최종 상태입니다.

82
00:11:09,440 --> 00:11:18,239
이렇게 마코프 연쇄와 마코프 결정 과정 (MDP), 부분 의사 결정 과정(POMDP)의
수학적 정의에 대해 정의했습니다.

83
00:11:18,240 --> 00:11:22,615
이제 강화학습에 관한 목적을 정의해봅시다.

84
00:11:22,640 --> 00:11:28,141
강화학습에서는 정책을 결정짓는 어떤 대상들을 학습하게 될 것입니다.

85
00:11:28,165 --> 00:11:35,119
일단은 우리가 정책을 직접적으로 배우고, 암묵적으로 표현할 수 있는 
다른 방법들이 있는지는 나중에 살펴보도록 하겠습니다.

86
00:11:35,120 --> 00:11:39,585
하지만 지금은 π_θ(a/s) 에 대해 배워보도록 하겠습니다.

87
00:11:39,931 --> 00:11:48,769
부분 관찰에 관한 케이스는 나중에 얘기하도록 하고, 지금은 정책이
S와 정책의 파라미터에 대응되는 θ가 주어졌다고 가정해봅시다.

88
00:11:48,794 --> 00:11:55,199
만약 정책이 깊은 신경망이면, θ는 신경망의 파라미터를 의미할 것입니다.

89
00:11:55,200 --> 00:12:06,624
상태가 정책에 입력되면 행동이 출력되고, 행동과 상태는 전이 확률로 들어가는데, 
이는 다음 상태를 만드는 물리 법칙이 됩니다.

90
00:12:06,649 --> 00:12:11,015
이 과정을 우리는 제어라고 부릅니다.

91
00:12:11,040 --> 00:12:24,239
이 과정에서 궤적(일련의 과정)에 대한 확률 분포를 기록할 수 있고, 
궤적은 s_t,a_t 에 이르기 까지의 상태와 행동의 연속들입니다.

92
00:12:24,240 --> 00:12:44,879
제어 문제를 무한한 지평선까지 확장할 거지만, 유한하다고 가정하면 시작하기가 쉽기 때문에
지금은 일단 의사 결정 문제가 T 단계까지 지속되는 유한한 지평선으로 가정해보겠습니다.

93
00:12:44,880 --> 00:13:02,799
행동과 상태의 결합 확률 분포를 써보고 정책 π_θ에 따르는 것을 의미하는 첨자 θ를 붙인다면, 
우리가 이미 정의한 확률 분포의 관점에서 연쇄 법칙에 따라 분해할 수 있습니다.

94
00:13:02,800 --> 00:13:13,975
마코프 연쇄와 MDP, PALMDP를 정의할 때 초기 상태 분포 p(s1)를 가지고 있다는 걸 숨겨두었는데,
사실 이 모든 것들은 초기 상태 분포 p(s1)를 가지고 있습니다.

95
00:13:14,000 --> 00:13:29,415
시간 순대로 행동 확률 p(a_t/s_t)와 다음 상태의 전이 확률 p(s_t+1/s_t,a_t)을 곱하면, 
아까 말했던 확률의 연쇄 법칙이 유도되는 것을 볼 수 있습니다.

96
00:13:29,440 --> 00:13:41,278
물론, 확률의 연쇄법칙 속에 우리는 지난 변수들에 대한 조건들이 필요하지만,
s_(t-1) s_(t-2) 등에 대한 의존도를 낮추기 위해 마코프 특성을 이용합니다.

97
00:13:41,279 --> 00:13:47,269
왜냐하면 s_t가 주어졌을 때, s_(t+1)이 s_(t-1)과 조건부 독립이라는 걸 알기 때문입니다.

98
00:13:47,294 --> 00:14:01,020
우리는 궤적의 분포를 정의할 수 있지만,
표기상의 간결함을 위해 s1부터 s_t,a_t까지의 분포를 p(τ)로 표현하기도 합니다.

99
00:14:01,045 --> 00:14:09,760
그래서 τ는 궤적의 약어고, 상태와 행동의 연속성을 의미합니다.

100
00:14:10,320 --> 00:14:21,433
궤적 분포를 정의한 후에는 강화학습에 대한 목적을 정의할 수 있고,
궤적 분포 아래에서 목표를 예상값으로 정의할 수 있습니다.

101
00:14:21,458 --> 00:14:34,398
그래서 강화학습의 목표는 궤적에 대한 보상의 기댓값의 합이 최대가 되는 정책의 파라미터 θ를 찾는 것입니다.

102
00:14:34,399 --> 00:14:41,999
그래서 우리는 기대에서 가장 높은 보상을 만드는 궤적을 지닌 정책을 만들길 원합니다.

103
00:14:42,000 --> 00:14:52,716
기댓값은 정책의 확률과 초기 상태 분포를 설명할 수 있습니다.

104
00:14:52,741 --> 00:14:58,159
이것이 우리가 연구할 강화 학습 목표의 정의입니다.

105
00:14:58,160 --> 00:15:04,648
물론 이에 대한 몇 가지 변형이 있으며 다음 강의 일부에서 가장 기본적인 버전으로 유도할 것입니다.

106
00:15:04,673 --> 00:15:12,135
잠시 영상을 멈추고 주관적으로 주의 깊게 보면서 이 수식이 무엇을 의미하는지 확실히 이해하기를 바랍니다.

107
00:15:12,160 --> 00:15:19,156
보상의 합이 무엇을 의미하는지, 기댓값을 궤적 분포로 가져가는 것이 무엇을 의미하는지,

108
00:15:19,279 --> 00:15:27,141
궤적 분포란 무엇이고, 정책 파라미터인 π_θ 선택에 따라 어떻게 영향을 받는지에 대해서 말입니다.

109
00:15:27,166 --> 00:15:33,278
왜냐하면 이 부분이 불분명하다면 앞으로 이어질 강의에 대해 상당히 이해하기 어려울 것이기 때문입니다.

110
00:15:33,279 --> 00:15:43,120
그러니 이것에 대해 잠시 생각해 보시고 궤적 분포에 대해 질문이 있으면,
반드시 비디오에 코멘트를 적어 주시길 바랍니다.

111
00:15:44,054 --> 00:15:46,978
계속 진행해보겠습니다.

112
00:15:47,003 --> 00:16:07,254
구조 분포의 인수분해에서 알아챌 수 있었던 것 중 하나는 MDP 과정에서
우리가 가졌던 대상의 관점에서 정의되어 있음에도 불구하고, 마코프 연쇄로 해석될 수 있다는 것입니다.

113
00:16:07,279 --> 00:16:12,879
이 것을 마코프 연쇄로 해석하려면, 일종의 연장된 상태 공간을 정의하는게 필요합니다.

114
00:16:12,880 --> 00:16:20,800
그래서 원래의 상태 공간은 S이지만 행동을 가지고 있고, 이 행동들은 마코프 결정 과정으로 만듭니다.

115
00:16:21,199 --> 00:16:30,159
우리는 이 행동이 정책에 근거한 상태에 달려있다는 것을 알고 있기 때문에,
π_θ(a_t/s_t)는 우리가 상태에 따라 행동의 분포를 얻을 수 있게 해줍니다.

116
00:16:30,160 --> 00:16:36,039
따라서 우리가 할 수 있는 것은 행동과 상태를 묶어 다음의 상태로 분류하는 것입니다.

117
00:16:36,064 --> 00:16:58,639
마코프 연쇄로부터 실제로 증강된 상태 p(s_t+1, a_t+1 /s_t,a_t)는
간단하게 마코프 결정 과정과 정책에서 전이 연산자의 산물입니다.

118
00:17:01,120 --> 00:17:08,879
이것은 우리가 목표를 약간 다른 방식으로 정의할 수 있게 해주어,
나중의 유도 수식으로부터 사용하기 편리하게 해줍니다.

119
00:17:08,880 --> 00:17:17,119
지금까지 저는 목표를 궤적 분포를 따르는 보상의 합으로 정의해왔습니다.

120
00:17:17,120 --> 00:17:30,159
하지만 우리의 분포는 증강된 상태가 마코프 연쇄를 따르고,
이 전이 연산자는 MDP 전이와 정책의 곱이라는 점을 기억해야 합니다.

121
00:17:30,160 --> 00:17:43,321
그래서 우리는 기댓값의 선형성으로 인해 마코프 연쇄에 따라 그 시간 단계에서의 보상을 
실제 가능한 상태들의 기댓값의 합으로 목표를 쓸 수 있습니다.

122
00:17:43,346 --> 00:17:53,279
이 것은 오로지 기댓값의 선형성만을 이용해 T 시간까지의 기댓값의 합 Eτ~r(s_t,a_t) 을 얻은 것입니다.

123
00:17:53,280 --> 00:18:01,279
그리고 기댓값 안에서 (s_t,a_t)에 의존하는 것 뿐만 아니라, 다른 변수들을 경계화(marginalize)시킬 수 있고,

124
00:18:01,280 --> 00:18:10,639
기댓값의 합 E~p_θ(s_t,a_t)[r(s_t,a_t)] 으로 표현됩니다.

125
00:18:10,640 --> 00:18:19,599
원래의 목적을 수학적으로 다시 적는 건 쓸모 없어 보인다고 생각할 수도 있습니다.

126
00:18:19,600 --> 00:18:25,119
하지만, 우리가 수식을 무한대로 확장하면서 꽤 유용하다는 것이 밝혀졌습니다.

127
00:18:25,120 --> 00:18:36,379
유한한 시간 내에서 발생하는 마코프 연쇄 p_θ(s_t,a_t)는 다른 단계들과
이격(marginalize)시키기만 하는 것으로도 얻어집니다.

128
00:18:36,404 --> 00:18:44,094
하지만 T가 무한할 때와 같은 경우에도 사용할 수 있을까요?

129
00:18:44,784 --> 00:18:49,376
T가 무한하다고 가정했을 때 첫번쨰로 일어날 수 있는 일은 목표가 잘못 정의될 수 있다는 것입니다.

130
00:18:49,401 --> 00:18:58,061
예를 들어 보상이 항상 양수라면, 시간이 무한으로 갈수록 보상도 무한으로 수렴하게 될 것입니다.

131
00:18:58,086 --> 00:19:14,345
따라서 목적을 유한하게 정의할 방법이 필요하게 되고, 일반적인 방법은 아니지만
실제로 편리하게 잘 되는 몇몇 방법들이 존재하는데, 그 중 하나는 평균 보상 공식화라는 것입니다

132
00:19:14,370 --> 00:19:19,199
보통은 기댓값의 합을 취하는데, 이를 T로 나누는 것입니다.

133
00:19:19,200 --> 00:19:27,759
T로 나눠 전체 보상을 일정화시키면 일반적이지만, 최댓값으로 바꾸진 못하더라도, 무한으로 갈 순 있습니다.

134
00:19:27,760 --> 00:19:40,695
나중에 배우게 되겠지만 할인율(discount factor)같이 잘 정해진 수가 있다면, 쉽게 유한하게 만드는 방법도 있습니다.

135
00:19:40,720 --> 00:19:45,952
우리가 무한한 경우에 대해 어떻게 정의할 수 있는지 얘기해봅시다.

136
00:19:46,000 --> 00:19:54,953
우리는 이전까지 마코프 연쇄에 대해 다뤄왔고, 증강된 마코프 연쇄는 전이연산자를 가지고 있다고 했습니다.

137
00:19:54,977 --> 00:20:06,615
이는 벡터 (s_t+1, a_t+1)를 (s_t, a_t) 벡터에 적용된 선형 연산 T라는 것을 의미하고,
T는 상태, 행동의 전이 연산자입니다.

138
00:20:06,640 --> 00:20:18,080
좀 더 일반화하여서 k 시간 동안을 생략하여, (s_t+k, a_t+k)를 T^k, (s_t, a_t)로 표현할 수도 있습니다.

139
00:20:18,400 --> 00:20:33,950
물어볼 수 있는 질문은 상태 행동의 이격 분포 p(s_t,a_t)는 k가 무한대로 갈 때,
일정한 분포로 수렴하냐, 하나의 분포로 수렴하냐는 것입니다.

140
00:20:33,975 --> 00:20:51,839
만약 특정 확률 분포를  Tμ로 쓸 수 있고, 이상적이라는 기술적인 전제 하에 이게 사실이라고 한다면,
이상적인(ergodic) 연쇄는 정적 분포가 존재한다는 것을 보여줄 수 있습니다.

141
00:20:51,840 --> 00:21:04,597
직관적으로 주기적이라는 것은 마코프 연쇄가 주기적이지 않다는 것으로 들리는 것을 의미하며, 이상적이라는 것은 대략적으로 말해서 모든 상태가 0이 아닌 모든 확률로 다른 모든 상태에 도달할 수 있다는 것을 의미합니다.

142
00:21:04,622 --> 00:21:11,678
이상적인 가정은 절대 닿을 수 없는 MDP 상황 중 한 부분에서 시작하는 상황을 방지해주기 때문에 중요합니다.

143
00:21:11,679 --> 00:21:21,104
만약에 이게 사실이여서 절대 닿을 수 없는 부분이 있어 도달하지 못하는 결과가 생긴다면,
시작하는 곳이 중요해진다는 의미가 되고, 정적 분포는 존재하지 않는다는 것을 의미하기 때문입니다.

144
00:21:21,144 --> 00:21:31,677
하지만 이런 경우가 아니라면, 어떤 상태에서 다른 상태까지 조금의 기회가 존재하기 때문에,
충분히 시간이 주어지면 정적 분포를 볼 수 있을 것입니다.

145
00:21:31,840 --> 00:21:40,239
정적 분포는 μ = Tμ 라는 방정식이 항상 성립해야 합니다. 그렇지 않으면 정적 분포가 아닙니다.

146
00:21:40,240 --> 00:21:50,158
정적임(stationary)의 뜻은 충분한 시간 T동안 전이를 해도 결국 다다를 수 있다는 전이의 전후가 같다는 것을 의미합니다.

147
00:21:50,159 --> 00:22:01,999
여러분도 정적 분포를 단순하게 이 방정식을 재나열하는 것만으로도 (τ-I)μ = 0 이라는 것을 풀어볼 수 있을 것입니다.

148
00:22:02,000 --> 00:22:09,199
기억할 것은 μ는 0~1 사이의 양수인 벡터이고, 합이 1이라는 확률 분포라는 점입니다.

149
00:22:09,200 --> 00:22:19,199
따라서 μ를 찾는 방법 중 하나는 T를 이용하여 고유값(eigenvalue)과 고유벡터(eigenvector)를 찾는 것입니다.

150
00:22:19,200 --> 00:22:28,880
이를 이용하면 μ는 고유값이 1인 T의 고유벡터 되고, 불규칙하다는 가정 하에서는 항상 이상적으로 존재합니다.

151
00:22:29,039 --> 00:22:45,424
따라서 우리가 마코프 연쇄를 충분한 시간동안 진행하여 T가 무한대로 갈수록, μ를 정착시킬수 있을 테고, 경계의 한계값은 결국 정적 분포 수식에 의해 지배됩니다.

152
00:22:45,449 --> 00:22:56,465
만약에 초기에 유한하게 μ_1, μ_2, μ_3 등이 있다면, 무한대에 갈수록 정적 분포에 가까워지는 것을 볼 수 있을 것입니다.

153
00:22:56,490 --> 00:23:02,162
이는 평균 보상을 넣는다면, 1/T를 얻을 수 있다는 의미입니다.

154
00:23:02,188 --> 00:23:10,639
이는 T가 무한대로 갈수록 정적 분포 아래에서 보상의 기댓값으로 한계를 얻을 수 있고,

155
00:23:10,640 --> 00:23:18,400
T가 무한으로 가는 경우에서 강화학습의 목적을 정의할 수 있도록 해줍니다.

156
00:23:18,559 --> 00:23:31,440
이번 강의에서 아마도 방금의 유도에 대해 받아들여야 할 게 많아서, 잠시 멈춰 생각해보는게 좋을 것 같습니다. 그리고 만약 이해하지 못 했거나, 질문이 있다면 댓글 달아주면 감사하겠습니다.

157
00:23:33,520 --> 00:23:50,079
이번 장에서 설명하고 싶었던 마지막 부분은 많은 강화학습 방법들의 이면에 있는 기본 원리를 이해하는데 매우 중요한 것으로, 강화학습은 기댓값을 정말로 최적화시키냐는 것입니다.

158
00:23:50,080 --> 00:24:00,479
우리가 높은 보상으로 이어지는 행동을 선택하는 관점에서의 강화 학습을 이야기하지만, 보상의 기댓값에 대해서는 걱정되는게 많습니다.

159
00:24:00,480 --> 00:24:14,534
기댓값에 대해 흥미로운 점은 기댓값이 함수 자체가 매우 불연속적인 경우일지라도 분포의 파라미터에 따라 연속적일 수도 있다는 것입니다.

160
00:24:14,561 --> 00:24:32,107
이는 강화학습 알고리즘이 게임의 승패같이 미분이 불가능해보이는 이진 보상을 최적화하기 위해,
어떻게 경사하강법과 같은 매끄러운 최적화 방법을 사용할 수 있는 이유를 이해하는 데 중요한 사실입니다.

161
00:24:32,332 --> 00:24:35,338
작게 예를 들어 설명해보겠습니다.

162
00:24:35,760 --> 00:24:47,278
여러분이 산을 내려가고 있다면 보상은 +1이고, 길에 머무르면 0, 길에서 벗어나거나 산에서 떨어지면  -1이라고 생각해봅시다.

163
00:24:47,279 --> 00:24:53,839
길에 머무르는 것과 도로에서 벗어나는 것 사이에는 비연속성이 있어, 보상 함수는 비연속적으로 보입니다.

164
00:24:53,840 --> 00:24:59,941
여러분이 차의 입장에서 보상 함수를 최적화 하고 싶다면,

165
00:24:59,965 --> 00:25:10,959
보상이 연속적이지 않거나, 차의 입장에서 덜 미분가능한 함수이기 때문에,
최적화 문제는 기울기 기반의 방식으로는 풀수 없어보입니다.

166
00:25:10,960 --> 00:25:21,599
하지만 추상적으로 말해서 떨어질지 안 떨어질지 선택하는 행동에 대한 확률 분포를 갖고 있다면

167
00:25:21,600 --> 00:25:34,240
여러분은 떨어질지 안 떨어질지에 대한 이진 행동을 가지고 있는 것이고,
이는 떨어질 확률 θ 하나와 떨어지지 않는 확률 1-θ를 갖는 베르누이 랜덤 변수입니다.

168
00:25:34,559 --> 00:25:42,079
흥미로운 점은 π_θ에 대한 보상의 기댓값이 이제 θ에서는 매끄러워졌다는 것입니다.

169
00:25:42,080 --> 00:25:49,678
왜냐하면 -1의 보상을 갖는 떨어지는 확률도 가지고 있고, 길에 있을 때는 1-θ의 확률을 갖고 있기 때문입니다.

170
00:25:49,679 --> 00:26:00,719
이제 보상은 각각 1-θ, θ가 되고, θ는 완벽하게 미분이 가능하고 매끄럽습니다.

171
00:26:00,720 --> 00:26:16,693
이 것은 강화학습이 최적화시킬 수 있다는 것을 설명하는 계속 해서 나올 매우 중요한 속성으로,
보상이 희박하거나 매끄럽지 못해 보이는 함수처럼 보일지라도,

172
00:26:16,717 --> 00:26:25,359
매끄럽고 미분 가능한 확률 분포 하에서는 그 자체로 매끄럽고 미분가능하므로
강화학습이 최적화시킬 수 있다는 것을 설명하는 매우 중요한 특성입니다.

173
00:26:25,360 --> 00:26:29,840
(다음 강의에 계속)

