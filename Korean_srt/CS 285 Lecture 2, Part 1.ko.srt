1
00:00:00,719 --> 00:00:05,200
안녕하세요. 오늘 cs285의 두 번째 강의에 오신 것을 환영합니다.

2
00:00:05,200 --> 00:00:09,200
우리는

3
00:00:06,240 --> 00:00:10,800
행동의 지도 학습에 대해 이야기할

4
00:00:09,200 --> 00:00:13,120
것이므로

5
00:00:13,120 --> 00:00:16,960
규칙적인 지도 학습 문제가 있는 경우 약간의 용어와 표기법으로 시작

6
00:00:16,960 --> 00:00:20,240
하겠습니다.

7
00:00:20,240 --> 00:00:24,640
우리는 이미지의 객체를 인식하고 싶을 수

8
00:00:22,240 --> 00:00:27,039
있으므로 심층 신경망을 통과하는 일부 입력 이미지를 가질 수

9
00:00:27,039 --> 00:00:31,840
있고 출력은 레이블

10
00:00:29,199 --> 00:00:32,320
이므로 여기서 사용할 용어는

11
00:00:32,320 --> 00:00:36,079
일종의 강화 학습 용어가 될 것입니다.

12
00:00:33,840 --> 00:00:37,200
표준 지도 학습 예제에 대해

13
00:00:37,200 --> 00:00:41,120
우리의 강화 용어를 사용하는 것부터 점진적으로 작업하여

14
00:00:41,760 --> 00:00:46,160
이를 강화

15
00:00:44,480 --> 00:00:48,879
학습 문제

16
00:00:46,160 --> 00:00:49,920
로 전환하여 관찰을 위해 입력을 o

17
00:00:49,920 --> 00:00:53,920
라고 부르고 행동을 위해 출력을 호출할 것입니다.

18
00:00:51,520 --> 00:00:56,000
지금은 입력이 이미지

19
00:00:53,920 --> 00:00:57,920
이고 출력이

20
00:00:56,000 --> 00:00:59,280
중간에 있는 신경망 레이블 또는

21
00:00:57,920 --> 00:01:00,079
일반적으로 해당 맵을 원하는 모델의 종류입니다.

22
00:01:00,079 --> 00:01:03,520
행동에 대한 관찰을

23
00:01:03,520 --> 00:01:07,600
정책이라고 부를

24
00:01:07,600 --> 00:01:12,479
것이며 아래첨자 ta

25
00:01:10,240 --> 00:01:12,960
는 해당 정책의 매개변수를 나타내는 문자 pi로 표시할 것이므로 신경망

26
00:01:12,960 --> 00:01:16,960
에서 ta는 해당 신경망의 가중치를 나타내

27
00:01:14,880 --> 00:01:19,759
므로 입력이 있습니다.

28
00:01:16,960 --> 00:01:20,080
o 출력 a와 그들 사이의 매핑

29
00:01:19,759 --> 00:01:21,759
pi

30
00:01:20,080 --> 00:01:23,200
아래 첨자 ta는 주어진 o에 대한 분포를 제공합니다.

31
00:01:26,400 --> 00:01:30,479
이제 강화 학습에서 물론

32
00:01:28,320 --> 00:01:33,200
우리는

33
00:01:30,479 --> 00:01:35,200
순차적 의사 결정 문제에 관심이 있으므로

34
00:01:33,200 --> 00:01:37,200
이러한 모든 입력 및 출력

35
00:01:35,200 --> 00:01:39,119
이 특정 시점에서 발생하므로 우리는

36
00:01:37,200 --> 00:01:41,200
일반적으로 첨자

37
00:01:39,119 --> 00:01:43,600
t를 사용하여 강화 학습에서 일반적으로 발생하는 시간 단계를 나타냅니다.

38
00:01:43,600 --> 00:01:47,119
우리는 이산 시간 문제를 다루므로

39
00:01:47,119 --> 00:01:50,960
시간이 작은 이산

40
00:01:49,040 --> 00:01:51,600
단계로 나뉘고 t는 어느 단계를 나타내는 정수라고 가정

41
00:01:51,600 --> 00:01:55,119
합니다.  o를 관찰하고

42
00:01:54,159 --> 00:01:58,320
어느 단계에서

43
00:01:55,119 --> 00:02:00,240
a를 방출합니까? 이제 pita

44
00:01:58,320 --> 00:02:02,880
는 조건부 ot에서 분포를 제공

45
00:02:02,880 --> 00:02:06,960
하며 물론

46
00:02:04,880 --> 00:02:09,440
강화 학습의 일반 지도 학습과

47
00:02:06,960 --> 00:02:10,080
달리  e 한 단계의 출력은 다음 단계

48
00:02:09,440 --> 00:02:13,599
의 입력

49
00:02:10,080 --> 00:02:17,200
에 영향을 미치므로 at은 ot

50
00:02:13,599 --> 00:02:19,520
에 1을 더한 영향을 미치므로 예를 들어

51
00:02:17,200 --> 00:02:20,560
호랑이를 인식하지 못하면

52
00:02:19,520 --> 00:02:23,120
다음 단계

53
00:02:23,120 --> 00:02:26,400
에서 호랑이가 많을 것 같은 바람직하지 않은 것을 볼 수 있습니다.

54
00:02:26,400 --> 00:02:32,640
이 기본 아이디어를 확장

55
00:02:32,640 --> 00:02:36,239
하여 제어에 대한 정책을 학습할 수 있으므로

56
00:02:34,800 --> 00:02:37,120
레이블을 출력하는 대신

57
00:02:37,120 --> 00:02:40,239
작업과 훨씬 비슷하게 보이는 것을 출력

58
00:02:38,879 --> 00:02:40,959
할 수 있지만 여전히 소프트 맥스 분포를 사용할 수 있는 개별 작업일

59
00:02:40,239 --> 00:02:43,200
수 있습니다.

60
00:02:40,959 --> 00:02:44,560
예를

61
00:02:43,200 --> 00:02:45,920
들어 호랑이를 볼 때

62
00:02:44,560 --> 00:02:48,400
이산 옵션 세트에서 선택할 수

63
00:02:45,920 --> 00:02:50,239
있지만

64
00:02:50,239 --> 00:02:54,560
pita

65
00:02:55,440 --> 00:02:59,200
가 다변량 정규 또는 가우스 분포의 평균 및 분산과 같은 일부 연속 분포의 매개 변수를 출력하는 연속 작업을 가질 수도 있습니다.

66
00:03:01,599 --> 00:03:06,560
용어를 요약하기 위해 ot

67
00:03:04,480 --> 00:03:09,920
는 관찰

68
00:03:06,560 --> 00:03:13,280
을 나타내고 는 행동을 나타내고

69
00:03:09,920 --> 00:03:16,080
pi는 주어진 ot에서 첨자 ta

70
00:03:13,280 --> 00:03:16,080
가 이제 정책입니다.

71
00:03:19,680 --> 00:03:25,599
강화 학습

72
00:03:21,120 --> 00:03:27,519
에서 많이 보게 될 상태는 st로 표시되는 상태

73
00:03:25,599 --> 00:03:29,040
이며 때로는 주어진 st에서 작성된 정책을 볼 수 있습니다.

74
00:03:32,080 --> 00:03:38,400
st와 ot의 차이점은 상태가 일반적으로 다음과 같이

75
00:03:35,920 --> 00:03:39,680
가정된다는 것입니다.  내가 곧 설명할 마르코비안 상태인

76
00:03:39,680 --> 00:03:44,080
반면 ot는

77
00:03:41,920 --> 00:03:46,000
해당 상태에서 발생하는 관찰

78
00:03:44,080 --> 00:03:47,360
이므로 가장 일반적으로 우리는

79
00:03:46,000 --> 00:03:48,159
정책을 조건부 및

80
00:03:47,360 --> 00:03:49,440
관찰

81
00:03:48,159 --> 00:03:51,280
로 작성하지만 때로는

82
00:03:49,440 --> 00:03:53,760
조건부 및 상태로 작성하며

83
00:03:51,280 --> 00:03:55,280
이는 보다 제한적인 특수입니다.  이 경우 상태와 관찰

84
00:03:53,760 --> 00:03:59,280
의 차이점을 설명

85
00:03:59,360 --> 00:04:02,799
하겠습니다. 이 장면을 관찰했다고 가정해 봅시다.

86
00:04:01,519 --> 00:04:06,560
거기에는 가젤을 쫓는 치타가 있습니다.

87
00:04:02,799 --> 00:04:08,400
이제 이 관찰

88
00:04:06,560 --> 00:04:09,920
은 이미지로 구성되고 이미지

89
00:04:08,400 --> 00:04:12,239
는 픽셀로 만들어집니다. 이 픽셀

90
00:04:12,239 --> 00:04:14,080
은 치타의 위치를 파악하기에 충분할 수 있습니다.  그리고

91
00:04:13,599 --> 00:04:17,120
가젤

92
00:04:14,080 --> 00:04:20,000
은 존재하거나 그렇지 않을 수도

93
00:04:17,120 --> 00:04:20,799
있지만 이미지는

94
00:04:20,799 --> 00:04:25,040
일부 시스템의 기본 물리학에 의해 생성되고 해당 시스템

95
00:04:23,759 --> 00:04:26,160
은 일종의 최소 표현을 갖는 상태를 갖습니다.

96
00:04:26,160 --> 00:04:29,840
따라서 이미지는 상태의 관찰

97
00:04:34,080 --> 00:04:37,600
이며 이 경우 예를 들어

98
00:04:36,160 --> 00:04:38,800
치타의

99
00:04:37,600 --> 00:04:42,639
위치와 가젤의 위치

100
00:04:42,639 --> 00:04:46,160
가 될 수 있으며 이제 관찰 속도가 일부 변경될 수 있는 시스템의 현재 구성을 나타냅니다.

101
00:04:47,919 --> 00:04:51,520
예를 들어 자동차

102
00:04:49,600 --> 00:04:53,199
가 치타 앞에서 운전하고

103
00:04:51,520 --> 00:04:54,880
그것을 볼 수 없는 경우 전체 상태를 정확하게 추론할 수 없기 때문에 상태

104
00:04:53,199 --> 00:04:56,880
를 추론하기에는 관찰이 불충분할

105
00:04:54,880 --> 00:04:58,320
수

106
00:04:56,880 --> 00:05:00,160
있지만 상태가 실제로 변경하지 않은

107
00:04:58,320 --> 00:05:01,759
경우 치트는 여전히 그 위치에 있습니다.  이전

108
00:05:00,160 --> 00:05:03,680
에는 이미지 픽셀

109
00:05:01,759 --> 00:05:05,840
과 관찰이 현재

110
00:05:03,680 --> 00:05:07,199
위치를 파악하기에 충분하지 않으며

111
00:05:05,840 --> 00:05:08,720
실제로

112
00:05:07,199 --> 00:05:10,560
상태

113
00:05:08,720 --> 00:05:12,639
와 관찰 상태

114
00:05:10,560 --> 00:05:14,000
의 차이를 파악하는 데 충분하지 않습니다. 시스템의 진정한 구성

115
00:05:12,639 --> 00:05:15,360
관찰

116
00:05:14,000 --> 00:05:17,840
은 해당

117
00:05:15,360 --> 00:05:21,680
상태에서 발생하는 것입니다.  상태를 보다 공식적으로 추론하기에 충분하거나 충분하지 않을 수 있습니다.

118
00:05:24,960 --> 00:05:28,720
그래픽

119
00:05:27,199 --> 00:05:31,360
모델을 사용

120
00:05:28,720 --> 00:05:33,039
하여 상태와 동작 및 관찰 사이의 관계를 나타내는 그래픽 모델을 그릴 수 있습니다.

121
00:05:36,080 --> 00:05:40,240
내가 언급한 관찰

122
00:05:38,800 --> 00:05:42,479
결과는 상태에서 발생

123
00:05:40,240 --> 00:05:44,000
하므로 매 단계마다 s에서 o로 화살표가

124
00:05:44,000 --> 00:05:48,160
표시됩니다. 따라서 정책은 관찰을 사용

125
00:05:46,479 --> 00:05:49,680
하여 동작을 선택합니다.

126
00:05:48,160 --> 00:05:51,280
o에서 화살표로

127
00:05:49,680 --> 00:05:53,199
그리고 현재

128
00:05:51,280 --> 00:05:55,520
시간 단계에서 작동 중인 상태는 다음 시간 단계의 상태를 결정

129
00:05:53,199 --> 00:05:58,319
하므로 s1과 a1

130
00:05:55,520 --> 00:05:58,319
은

131
00:05:58,639 --> 00:06:04,560
이제 이 그래픽 모델을 검사하여 s2로 이동

132
00:06:05,759 --> 00:06:11,759
합니다. 시스템에 특정 독립성이 존재한다고 결론을 내릴 수 있습니다.

133
00:06:09,280 --> 00:06:13,840
이것이 정책 파이입니다. 이것은

134
00:06:11,759 --> 00:06:14,639
sd의 전이 확률 p에

135
00:06:13,840 --> 00:06:18,479
주어진 통계 1을 더한 것입니다.

136
00:06:14,639 --> 00:06:21,840
여기서 주목해야 할 점

137
00:06:18,479 --> 00:06:21,840
은

138
00:06:22,000 --> 00:06:28,960
주어진 st 18에서 st의 p에 1을 더한

139
00:06:25,600 --> 00:06:30,080
값은 st 빼기 1과 무관

140
00:06:30,080 --> 00:06:34,160
하다는 것입니다. 따라서 현재를 알고 있는 상태의 경우  상태 그러면 이전 상태에 대한 고려 없이 다음 상태에

141
00:06:32,639 --> 00:06:35,440
대한 분포를 파악할 수 있습니다.

142
00:06:39,039 --> 00:06:43,520
즉, 미래는

143
00:06:41,600 --> 00:06:46,080
과거와 조건부로 독립적입니다.

144
00:06:43,520 --> 00:06:47,840
현재 상태를 고려할 때 이것은 매우

145
00:06:46,080 --> 00:06:49,919
중요한 독립 속성입니다.

146
00:06:47,840 --> 00:06:51,520
왜냐하면

147
00:06:51,520 --> 00:06:56,479
미래 상태에 영향을 미칠 결정을 내리려면

148
00:06:56,479 --> 00:06:59,280
현재 상태에 도달하는 방법을 고려할 필요가 없으며

149
00:06:57,919 --> 00:07:01,120
현재 상태만 고려하면 충분하기 때문입니다.

150
00:06:59,280 --> 00:07:01,599
그리고 이전

151
00:07:01,120 --> 00:07:04,960
상태

152
00:07:01,599 --> 00:07:07,199
를 잊어버릴 수 있습니다. 이것을

153
00:07:04,960 --> 00:07:08,880
markov 속성이라고 하며 markov

154
00:07:07,199 --> 00:07:10,479
속성은 강화 학습 및 순차적 의사 결정에서 매우 중요한 속성입니다.

155
00:07:11,759 --> 00:07:15,199
왜냐하면 markov 속성 없이는 최적의 정책

156
00:07:13,759 --> 00:07:16,880
을 공식화할 수 없기 때문입니다.

157
00:07:16,880 --> 00:07:22,240
전체 역사를 고려할 때

158
00:07:19,520 --> 00:07:24,160
그러나 우리의 정책이

159
00:07:24,160 --> 00:07:27,680
이 그림에서와 같이 상태가 아닌 관찰에 따라 결정된다면

160
00:07:27,680 --> 00:07:31,919
관찰도 이러한

161
00:07:35,360 --> 00:07:39,599
방식으로 조건부로 독립적인지 질문할 수

162
00:07:37,039 --> 00:07:40,800
있습니다.  미래

163
00:07:39,599 --> 00:07:42,319
에 이 질문에 대해 잠시 생각하고

164
00:07:42,319 --> 00:07:45,840
답을 쓰는 것을 고려하십시오.  주석

165
00:07:47,039 --> 00:07:50,160
에서 문제는 관찰

166
00:07:48,639 --> 00:07:52,160
이 일반적으로

167
00:07:50,160 --> 00:07:54,000
마르코프 속성을 만족하지 않는다는 것

168
00:07:52,160 --> 00:07:55,520
입니다. 즉 현재 관찰

169
00:07:54,000 --> 00:07:57,440
은 과거를 관찰하지 않고 미래를 완전히 결정하기에 충분하지 않을 수

170
00:07:58,319 --> 00:08:02,319
있으며 이는 아마도

171
00:08:00,479 --> 00:08:04,479
치타의 예에서 가장 분명합니다.

172
00:08:02,319 --> 00:08:05,759
차가 치타 앞에 있고

173
00:08:05,759 --> 00:08:08,240
이미지

174
00:08:08,240 --> 00:08:11,680
의 위치를

175
00:08:09,759 --> 00:08:14,319
볼 수 없을 때 지금은 볼 수 없기 때문에 미래에 어디로 갈지 알 수

176
00:08:11,680 --> 00:08:16,000
없지만 이전 지점에서  시간이

177
00:08:17,280 --> 00:08:20,400
지나면 치타가 어디에 있는지 기억하기 전에 차가 다른 곳에 있었는지 알 수

178
00:08:19,039 --> 00:08:22,319
있으므로 차가 막힌 경우에도

179
00:08:20,400 --> 00:08:23,520
여전히 상태를 기억할 수 있습니다.

180
00:08:22,319 --> 00:08:26,000
따라서 일반적으로 관찰을 사용하는 경우

181
00:08:23,520 --> 00:08:27,919
과거 관찰은

182
00:08:26,000 --> 00:08:28,960
실제로 추가 정보를  현재 관찰

183
00:08:27,919 --> 00:08:30,160
에서 얻을 수 있는 것 이상

184
00:08:30,160 --> 00:08:34,320
으로 의사 결정에 유용한 정보인

185
00:08:32,399 --> 00:08:36,080
반면, 상태를 직접 관찰

186
00:08:34,320 --> 00:08:37,519
하면 현재 상태는 항상 진행 중입니다.

187
00:08:37,519 --> 00:08:42,479
Markov 속성을 충족하기 때문에 필요한 모든 것을 제공하기 위해

188
00:08:42,479 --> 00:08:44,560
이 과정에서 논의할 많은 강화 학습 알고리즘

189
00:08:44,560 --> 00:08:50,399
은 실제로

190
00:08:51,760 --> 00:08:55,440
Markovian 상태를 필요로 할 것

191
00:08:53,920 --> 00:08:57,360
입니다.  특정 알고리즘

192
00:08:55,440 --> 00:08:59,200
은 비마코비안 관찰을 처리하기 위해 어떤 방식으로든 수정될 수 있습니다.

193
00:08:59,200 --> 00:09:02,800
그런 다음

194
00:09:06,800 --> 00:09:11,680
우리가 일반적으로

195
00:09:08,720 --> 00:09:13,120
s를 사용하여 상태를 표시하고

196
00:09:11,680 --> 00:09:14,560
매우 합리적인 행동을 표시하는 데 사용하는 강화 학습의 표기법을 제외하고 어떻게 할 수 있는지 설명하겠습니다.  영어

197
00:09:13,120 --> 00:09:15,360
로 된 단어의 첫 글자이기 때문에

198
00:09:15,360 --> 00:09:20,240
이러한 종류의 용어

199
00:09:27,040 --> 00:09:31,440
는 로봇 공학과 최적 제어 및 선형에 대한 배경 지식이 있는 경우 1950년대에 Richard bellman이 여러 면에서 개척한 동적 프로그래밍 연구에 의해 널리 대중화되었습니다.

200
00:09:29,200 --> 00:09:32,800
시스템

201
00:09:32,800 --> 00:09:36,959
에서는

202
00:09:34,480 --> 00:09:38,560
x가 상태를 나타내는 데 사용되고 u가 동작을 나타내는 데 사용되는 다른

203
00:09:38,560 --> 00:09:43,200
표기법에 더 익숙할 수 있습니다.  s는 정확히 동등한 용어입니다. x

204
00:09:41,680 --> 00:09:45,760
는 상태에 대해 의미가 있습니다. 왜냐하면

205
00:09:43,200 --> 00:09:47,760
그것은 일반적으로

206
00:09:45,760 --> 00:09:50,000
대수학에서 미지의 양에 사용되는 변수

207
00:09:47,760 --> 00:09:52,240
이고 u는 러시아어로 행동을 나타내는 첫 번째 단어

208
00:09:52,240 --> 00:09:55,680
이기 때문입니다.

209
00:10:01,760 --> 00:10:07,680
소비에트 연방에서 최적의 통제를 연구한 좌파

210
00:10:05,279 --> 00:10:08,800
판테라간은 맞습니다. 그래서 약간의

211
00:10:07,680 --> 00:10:11,360
용어

212
00:10:08,800 --> 00:10:12,240
이지만 이제 실제로 정책을 배울 수 있는 방법에 대해 이야기

213
00:10:12,240 --> 00:10:16,079
하고 오늘 강의에서는

214
00:10:14,560 --> 00:10:17,440
실제로 정책을 학습하는 매우 간단한 방법으로 시작하겠습니다.

215
00:10:17,440 --> 00:10:20,640
t는 매우

216
00:10:19,120 --> 00:10:21,600
정교한 강화 학습

217
00:10:20,640 --> 00:10:23,760
알고리즘을 사용해야

218
00:10:21,600 --> 00:10:25,360
하지만 대신 데이터를 활용하여 지도 학습에서

219
00:10:25,360 --> 00:10:29,120
이미지 분류기 및 기타 종류의

220
00:10:27,040 --> 00:10:32,240
모델을 배우는 것과 거의 동일한 방식으로 정책을 학습

221
00:10:32,240 --> 00:10:37,440
하므로 호랑이에게서 도망치는 보다 현실적인 예를 보겠습니다.

222
00:10:37,440 --> 00:10:41,040
우리의 일상 생활에서 중요하지만

223
00:10:41,760 --> 00:10:45,279
당신의 관찰 mi를 운전하는 다른 작업은 어떻습니까?  ght

224
00:10:44,000 --> 00:10:48,000
는 자동차 카메라의 이미지로 구성되며

225
00:10:48,000 --> 00:10:51,120
행동은

226
00:10:49,440 --> 00:10:53,600
자동차를 도로에 유지하기 위해 핸들

227
00:10:54,079 --> 00:10:58,959
을 돌리는 방법으로

228
00:10:57,120 --> 00:11:00,720
구성될 수 있습니다.

229
00:10:58,959 --> 00:11:02,880
이미지

230
00:11:00,720 --> 00:11:04,880
분류 컴퓨터 비전

231
00:11:02,880 --> 00:11:06,800
등과 같은 것 일부 레이블

232
00:11:04,880 --> 00:11:08,160
데이터를 가져오고 해당 레이블 데이터

233
00:11:06,800 --> 00:11:10,480
를 사용하여 지도 학습으로 운전 정책을

234
00:11:08,160 --> 00:11:13,279
학습

235
00:11:10,480 --> 00:11:14,959
하여 사람과 해당 운동 명령에서 이미지를 가져

236
00:11:13,279 --> 00:11:16,399
와서 이

237
00:11:14,959 --> 00:11:17,680
인간 운전자가 방향을

238
00:11:16,399 --> 00:11:19,120
바꾸도록 하겠습니다.  어떤 식으로든

239
00:11:19,839 --> 00:11:24,160
핸들을 잡고 카메라에서 본 것을 기록하고 조종 명령을 기록하고

240
00:11:24,160 --> 00:11:28,560
이미지 및 동작 튜플로 구성된 대규모 데이터 세트를 수집한

241
00:11:27,040 --> 00:11:29,040
다음 지도

242
00:11:28,560 --> 00:11:31,120
학습

243
00:11:29,040 --> 00:11:32,640
을 사용하여 지도 작성을 배우게 됩니다.

244
00:11:31,120 --> 00:11:34,959
행동에

245
00:11:32,640 --> 00:11:36,399
대한 관찰 이것을 모방 학습

246
00:11:34,959 --> 00:11:37,760
이라고 하며 때때로 행동 복제라고도 하는 초대 학습의 특정 사례입니다.

247
00:11:38,480 --> 00:11:42,880
이를 행동 복제라고 합니다.

248
00:11:41,279 --> 00:11:43,600
어떤 의미에서 우리

249
00:11:43,600 --> 00:11:48,320
는 이 인간 시연자의 행동을 복제하고 있기 때문에

250
00:11:46,720 --> 00:11:50,320
시연자들은 때때로 전문가라고도 합니다.

251
00:11:48,320 --> 00:11:51,600
왜냐하면 우리는

252
00:11:50,320 --> 00:11:54,800
그들이 컴퓨터보다 이 작업을 더 잘한다고 가정하기

253
00:11:54,800 --> 00:11:58,079
때문에 이것은 매우 간단한 접근 방식이며 우리

254
00:11:56,800 --> 00:12:00,959
는 다음과 같이 질문할 수 있습니다.  질문이 잘

255
00:11:58,079 --> 00:12:00,959
작동합니까?

256
00:12:01,120 --> 00:12:06,880
음. 그래서 이 질문

257
00:12:05,120 --> 00:12:08,480
은 아주 오랫동안 연구되어 왔습니다.

258
00:12:06,880 --> 00:12:10,560
사실 원래의

259
00:12:08,480 --> 00:12:12,079
심층 모방 모방 학습 시스템

260
00:12:10,560 --> 00:12:13,360
이나 신경 제한 학습 시스템은 오늘날

261
00:12:12,079 --> 00:12:13,920
우리에게 친숙할 무언가가 오래전

262
00:12:13,920 --> 00:12:19,200
에 제안된 것입니다.  1989년

263
00:12:17,200 --> 00:12:20,560
그것은 alvin이라고 불렀고 alvin은 자율 육상

264
00:12:19,200 --> 00:12:22,000
차량과 신경망이라고 불렀고

265
00:12:22,000 --> 00:12:27,279
현재 표준에 따르면 네트워크

266
00:12:25,040 --> 00:12:29,200
는 5개의 은닉 유닛이 있는 아주 작은 아주

267
00:12:27,279 --> 00:12:31,600
흥미로운 일을 했습니다.

268
00:12:33,760 --> 00:12:37,440
미국 전역을 가로질러 드라이브를 시도하여

269
00:12:37,519 --> 00:12:41,839
이 기본

270
00:12:40,240 --> 00:12:43,839
원리가 작동하는지 이 동작

271
00:12:41,839 --> 00:12:46,399
복제 원리

272
00:12:43,830 --> 00:12:49,600
일반적으로 그렇지 않습니다.

273
00:12:49,600 --> 00:12:55,510
지도학습에 비해 왜 행동 복제가 안되는 지 설명을 하겠습니다.

274
00:12:55,510 --> 00:13:00,000
제어 문제에 대한 추상적인 사진으로 시작을 해보겠습니다.

275
00:13:00,000 --> 00:13:05,760
이번 강의에서는 많은 사진을 사용할 계획입니다.

276
00:13:05,760 --> 00:13:13,200
사진에서의 좌측의 축은 상태이고, 실제로 이 축은 1차원이 아닙니다.

277
00:13:13,200 --> 00:13:17,360
그렇지만, 설명을 돕기 위해 1차원이라 가정하겠습니다.

278
00:13:17,360 --> 00:13:18,720
그리고 우측의 축은 시간을 나타냅니다.

279
00:13:18,720 --> 00:13:25,830
그래서 이 검은 곡선은 다른 상태의 값이 있는 모든 시점의 시간 궤적을 나타냅니다.

280
00:13:25,830 --> 00:13:29,680
그리고 이 검은 궤적이 우리의 학습 데이터를 나타낸다고 가정해 봅시다

281
00:13:29,680 --> 00:13:39,680
우리는 이 학습 데이터가 S에서 A로 정책을 학습하기 위해 사용되는 것으로 간주할 것입니다.

282
00:13:39,680 --> 00:13:43,920
앞으로 빨간색 선으로 해당 정책이 실행될 때 어떤 상태를 가지게 되는 지 그릴 것입니다.

283
00:13:43,920 --> 00:13:47,270
처음에 정책은 학습 데이터에 매우 근접하게 유지됩니다.

284
00:13:47,270 --> 00:13:51,120
왜냐하면 우리는 잘 학습된 거대한 뉴럴 네트워크를 사용했기 때문입니다.

285
00:13:51,120 --> 00:13:52,630
하지만 약간의 오차는 발생하게 됩니다.

286
00:13:52,630 --> 00:13:57,830
모든 뉴럴 네트워크는 약간의 오차가 발생하게 되고, 이것은 기본적으로 불가피한 요소입니다.

287
00:13:57,830 --> 00:14:07,120
문제는 이 모델이 작은 오차를 가졌음에도. 훈련 받은 상태와는 다른 상태에 있다는 것입니다.

288
00:14:07,120 --> 00:14:11,190
이러한 문제는 더 큰 오차를 발생하게 되는데,

289
00:14:11,190 --> 00:14:15,600
이는 학습된 상태와는 다른 상태에 있는데, 다른 상태에 대한 학습이 되지 않았기 때문입니다.

290
00:14:15,600 --> 00:14:20,880
그리고 이러한 오차들이 복합적으로 작용함에 따라 상태는 점점 달라지고, 오차는 점점 커집니다.

291
00:14:20,880 --> 00:14:27,920
결국에는 학습된 행동과는 매우 다른 행동을 하게 됩니다.

292
00:14:27,920 --> 00:14:29,360
자동차 시나리오를 예시로 들자면,

293
00:14:29,360 --> 00:14:33,360
자동차가 아주 조금 왼쪽으로 움직여 학습되지 않은 것을 보게 될 것이고,

294
00:14:33,360 --> 00:14:37,040
여기서 조금 더 왼쪽으로 가게 됨으로써 결국 도로를 벗어나게 됩니다.

295
00:14:37,040 --> 00:14:44,320
그리고 우리는 이 현상을 더 공식적으로 설명하는 것을 나중에 강의에서 볼 것입니다.

296
00:14:44,320 --> 00:14:45,680
하지만 이것은 실제로 잘 작동합니다.

297
00:14:45,680 --> 00:14:47,760
가끔 꽤 효과적이기도 합니다.

298
00:14:47,760 --> 00:14:52,880
이 비디오들은 Nvidia에서 2016년에 발표한 논문에서 수집되었습니다

299
00:14:52,880 --> 00:14:56,000
초기 시스템에 많은 어려움을 겪었다는 것을 볼 수 있습니다.

300
00:14:56,000 --> 00:15:02,070
길에서 벗어나거나 원뿔콘을 쓰러트리는 등 예상치 못한 행동을 하곤 합니다.

301
00:15:02,070 --> 00:15:04,800
하지만 많은 데이터를 수집하고 약간의 트릭을 사용한 후

302
00:15:04,800 --> 00:15:18,560
그들은 실제로 원뿔 사이를 자율적으로 운전하고, 도로 위에서 꽤나 합리적인 행동을 하는 시스템을 만들게 되었습니다.

303
00:15:18,560 --> 00:15:26,320
왜 그럴듯한 정책을 훈련시키기 위해 행동 복제 방법을 사용할 수 있는 지

304
00:15:26,320 --> 00:15:30,390
Part 2에서 좀 더 자세히 논의할 것입니다.

305
00:15:30,390 --> 00:15:34,000
하지만 제가 지금 간단히 언급하고 싶은 것 중 하나는

306
00:15:34,000 --> 00:15:39,680
Nvidia가 발표한 이 논문에서 해당 문제를 다루기 위해 사용된 특별한 기술입니다.

307
00:15:39,680 --> 00:15:43,920
Nvidia의 논문에서 시스템에 대한 설명을 보면,

308
00:15:43,920 --> 00:15:48,070
우리가 예상했던 방법과 비슷한 방법을 사용했습니다.

309
00:15:48,070 --> 00:15:53,190
CNN 계층이 있고, 그 CNN 계층이 조향각을 생성합니다.

310
00:15:53,190 --> 00:16:00,070
자동차는 생성된 조향각을 조정하고, 다시 CNN이 카메라로부터 입력을 받게 됩니다.

311
00:16:00,070 --> 00:16:03,120
하지만 여기서 여러분이 여기서 확인해야될 점은

312
00:16:03,120 --> 00:16:06,000
전방, 좌측, 우측 카메라가 있다는 것입니다.

313
00:16:06,000 --> 00:16:19,040
이 카메라들은 논문에서 사용된 기술 중 중요한 것은 3개의 측면 카메라 이미지를 동시에 녹화하는 것입니다.

314
00:16:19,040 --> 00:16:22,720
전방 이미지는 자동차가 가지는 조향각과는 무관하게 관리됩니다.

315
00:16:22,720 --> 00:16:28,320
좌측 카메라 이미지는 오른쪽으로 방향을 트는 조향각을 관리합니다.

316
00:16:28,320 --> 00:16:34,000
즉, 도로의 좌측으로 도로를 벗어나는 이미지를 보게 되었다면 우측으로 각도를 잡아야 합니다.

317
00:16:34,000 --> 00:16:39,440
그에 상응하여 우측 카메라 이미지는 왼쪽으로 방향을 트는 조향각을 관리합니다.

318
00:16:39,440 --> 00:16:45,830
이제 이 특별한 기술이 어떻게 차를 운전하는 특별한 경우에 이 표류 문제를 완화시킬지 상상할 수 있을 것입니다.

319
00:16:45,830 --> 00:16:51,270
왜냐하면 이 좌측/우측 이미지들은 본질적으로 정책에 작은 오차들을 바로잡는 방법을 가르치기 때문입니다.

320
00:16:51,270 --> 00:16:55,270
그리고 오차를 수정하게 된다면 더 큰 오차를 계산하지 않게 될 것입니다.

321
00:16:55,270 --> 00:16:59,360
이것은 일반적인 원리에서의 특별한 경우입니다.

322
00:16:59,360 --> 00:17:03,360
보다 일반적인 원칙은 궤도의 오차가 복합적으로 계산되지만,

323
00:17:03,360 --> 00:17:07,910
작은 오차를 피드백할 수 있도록 학습 데이터를 수정할 수 있다면

324
00:17:07,910 --> 00:17:14,950
정책이 이러한 피드백 과정을 학습하고 안정화할 수 있다는 것입니다.

325
00:17:14,950 --> 00:17:19,830
물론 이러한 것을 하기 위한 많은 다른 방법들이 있으나, 나중에 이 코스에서 논의할 것입니다.

326
00:17:19,830 --> 00:17:24,880
예를 들어, 최적의 피드백 컨트롤러를 데모를 통하여 학습하고,

327
00:17:24,880 --> 00:17:31,600
이를 관리자(supervision)으로 사용한다면 꽤나 안정성을 계승한 정책을 학습할 수 있을 것입니다.

328
00:17:31,600 --> 00:17:35,670
혹은 고의적으로 실수를 저지르고, 그 실수를 바로잡는 방법을 사용할 수도 있습니다.

329
00:17:35,670 --> 00:17:39,840
그래서 우리가 이 문제를 해결하기 위해 사용할 수 있는 여러 방법들이 있습니다.

330
00:17:39,840 --> 00:17:48,160
하지만 우리가 조금 더 일반적인 해답을 도출하기 위해 요구하는 것은

331
00:17:48,160 --> 00:17:57,360
이 흐름 뒤에 있는 근본적인 수학적 원리가 무엇인가 하는 것입니다.

332
00:17:57,360 --> 00:18:01,120
우리가 정책을 실행하게 되면,

333
00:18:01,120 --> 00:18:03,910
주어진 o_t에서 pi_theta a_t를 추출합니다.

334
00:18:03,910 --> 00:18:08,080
그리고 이 추출된 분포는 특정 데이터 분포에 의해 학습되었습니다.

335
00:18:08,080 --> 00:18:11,670
그리고 우리는 그러한 분포를 P_data(O_t) 라고 부를 것입니다.

336
00:18:11,670 --> 00:18:18,160
이것은 기본적으로 우리의 훈련 데이터에 나타난 관찰의 분포입니다.

337
00:18:18,160 --> 00:18:24,960
우리는 지도학습을 통하여, 특정 학습 분포에 대해 특정 모델을 훈련한다면

338
00:18:24,960 --> 00:18:27,280
낮은 학습 오류를 얻으며 과적합 되지 않는다는 것을 배웠습니다.

339
00:18:27,280 --> 00:18:35,030
또한 동일한 분포에서 도출된 테스트 데이터인 경우, 낮은 테스트 오류를 얻을 것으로 예상할 수 있습니다.

340
00:18:35,030 --> 00:18:42,400
만약, 훈련 데이터와 동일한 분포에서 관측된 새로운 관찰이 기존 관찰과는 동일하지 않더라도

341
00:18:42,400 --> 00:18:48,640
우리는 학습된 정책이 새로운 관찰에 대하여 올바른 행동을 생산할 것을 기대할 것입니다.

342
00:18:48,640 --> 00:18:55,280
그러나 우리가 정책을 실제로 실행할 때, 우리가 보는 관찰에 대한 분포는 다릅니다.

343
00:18:55,280 --> 00:19:05,280
왜냐하면 정책이 관찰에 대하여 다른 행동을 수행하기 때문에 관찰의 분포가 달라집니다.

344
00:19:05,280 --> 00:19:08,960
마지막에는, P_(O_t) 는 P_data(O_t)와 달라지게 되고,

345
00:19:08,960 --> 00:19:15,030
이것이 복합적인 오차의 원인이다.

346
00:19:15,030 --> 00:19:19,360
그래서 우리는 P_data(O_t)와 P_(O_t)를 같게 만드려고 합니다.

347
00:19:19,360 --> 00:19:30,400
이게 가능하다면 우리는 정책이 지도학습의 표준 결과로부터 좋은 행동을 만들어 낼 것이라는 것을 압니다.

348
00:19:30,400 --> 00:19:36,160
이 두 개의 값을 같게 만드는 방법은 정책을 완벽하게 만드는 것입니다.

349
00:19:36,160 --> 00:19:42,880
만약 정책이 완벽하고, 오차가 없다면 이 분포들은 서로 일치할 것입니다.

350
00:19:42,880 --> 00:19:45,840
그렇지만, 당연하게도 매우 힘든 작업입니다.

351
00:19:45,840 --> 00:19:53,760
만약 우리가 정책에 대해 생각하는 대신에 우리의 데이터 분포에 대해서 더 생각한다면 어떨까요?

352
00:19:53,760 --> 00:20:02,150
그러니 어떤 방법으로 정책을 바꾸지 말고 실제로 우리의 데이터를 바꿔서 이 분포의 불일치 문제를 해결해보도록 합시다.

353
00:20:02,150 --> 00:20:08,480
이것은 DAgger라고 불리는 방법의 기본적인 아이디어입니다.

354
00:20:08,480 --> 00:20:11,440
DAgger는 "Dataset Aggregation"의 약자입니다.

355
00:20:11,440 --> 00:20:18,790
DAgger에서는, 훈련 데이터를 P_data(O_t) 에서 수집하는 대신 P_(O_t)에서 수집하는 것이 목표입니다.

356
00:20:18,790 --> 00:20:22,550
왜냐하면 우리는  P_(O_t) 분포에서의 관찰-행동 쌍을 가지고 있고

357
00:20:22,550 --> 00:20:31,760
이 쌍들을 학습한다면 데이터 분포의 불일치 문제가 사라지기 때문입니다.

358
00:20:31,760 --> 00:20:33,030
DAgger가 이걸 어떻게 해내는지 보여드리죠

359
00:20:33,030 --> 00:20:36,400
주어진 o_t에서 pi_theta a_t를 추출한 정책을 실행할 것이고,

360
00:20:36,400 --> 00:20:38,640
주어진 o_t에서 pi_theta a_t를 추출한 정책을 실행할 것이고,

361
00:20:38,640 --> 00:20:40,960
이 정책은 P_(O_t)로부터 샘플들을 생산할 것입니다.

362
00:20:40,960 --> 00:20:47,120
그리고 우리는 그 관찰들에서 추가적인 라벨을 요청할 것입니다.

363
00:20:47,120 --> 00:20:52,720
첫번째 단계는 인공적인 데이터 셋으로부터 학습을 하여 정책을 초기화하는 것입니다.

364
00:20:52,720 --> 00:20:58,400
그런 다음 관찰의 추가적인 데이터 셋을 수집하기 위해 정책을 실행할 것입니다.

365
00:20:58,400 --> 00:21:00,000
여기서 그 데이터 셋을 D_pi라 하고,

366
00:21:00,000 --> 00:21:03,520
이러한 관찰들은 P_(O_t) 로부터 추출됩니다.

367
00:21:03,520 --> 00:21:08,080
그러면 우리는 인공적으로 이 모든 관찰에 최적의 행동으로 라벨을 붙이도록 요청할 것입니다.

368
00:21:08,080 --> 00:21:12,960
그래서 누군가는 말 그대로 기계가 만들어낸 관찰을 지켜볼 것입니다.

369
00:21:12,960 --> 00:21:18,880
그리고 기계에게 해당 관찰로부터 취할 수 있는 최적의 행동이 무엇인지 표기합니다.

370
00:21:18,880 --> 00:21:19,840
그리고 나서 우리는 기존의 데이터 셋과 병합을 할 것입니다.

371
00:21:19,840 --> 00:21:25,670
우리는 이러한 데이터 셋을 병합하고 정책을 다시 학습시킬 것입니다.

372
00:21:25,670 --> 00:21:30,480
이 병합된 데이터 셋에 대해 정책을 다시 학습할 때 정책이 변경됩니다.

373
00:21:30,480 --> 00:21:37,520
이것은 D_pi 데이터 셋이 P_로부터 왔음에도 불구하고, thetha가 다르고, P_ 또한 다르다는 것을 의미합니다.

374
00:21:37,520 --> 00:21:42,000
그래서 우리는 이 과정을 반복해야 합니다.

375
00:21:42,000 --> 00:21:51,840
이러한 알고리즘을 충분히 반복한다면 추가적인 Converge를 가진다는 것이 ROSS의 논문에 설명되어 있습니다.

376
00:21:51,840 --> 00:21:59,910
최종 데이터 셋은 정책과 점근적으로 근사한 분포에서 가져오게 됩니다.

377
00:21:59,910 --> 00:22:03,670
여기 DAgger으로 학습된 정책의 예가 있습니다.

378
00:22:03,670 --> 00:22:05,280
숲 속을 드론을 날립니다.

379
00:22:05,280 --> 00:22:07,030
이 정책은 실제로 딥러닝을 사용하지 않습니다.

380
00:22:07,030 --> 00:22:10,320
실제로는 선형 이미지 특징들을 사용합니다.

381
00:22:10,320 --> 00:22:13,760
하지만 그 이후의 연구는 딥러닝에 대해서도 수행됐습니다.

382
00:22:13,760 --> 00:22:18,080
그래서 이 드론은 처음에 멀리 있는 숲을 잘 헤쳐나갈 수 없었습니다.

383
00:22:18,080 --> 00:22:22,080
하지만 몇번의 인공적인 라벨링 작업 이후로는

384
00:22:22,080 --> 00:22:24,640
꽤 능숙하게 숲을 항해할 수 있었습니다.

385
00:22:24,640 --> 00:22:29,280
그럼 DAgger의 문제는 무엇일까요?

386
00:22:29,280 --> 00:22:32,960
왜 우리는 항상 모방 학습에 이 알고리즘을 사용하지 않을까요?

387
00:22:32,960 --> 00:22:37,440
DAgger에 관한 많은 문제들은 3단계에서 비롯됩니다.

388
00:22:37,440 --> 00:22:40,240
문제에 따라 약간 다릅니다만,

389
00:22:40,240 --> 00:22:47,760
많은 경우, D_pi에 최적의 조치를 인공적으로 표시하는 것은 실제로 상당히 부담스러운 일일 수 있습니다.

390
00:22:47,760 --> 00:22:49,840
스스로 이것을 한다고 상상해 보세요

391
00:22:49,840 --> 00:22:53,760
드론이 숲을 날아가는 영상을 보고 있다고 상상해보세요.

392
00:22:53,760 --> 00:22:56,480
그리고 드론이 최적의 동작을 제공하도록 조종해야 합니다.

393
00:22:56,480 --> 00:22:59,840
실시간으로 드론에 영향을 주지 않으면서 말이죠.

394
00:22:59,840 --> 00:23:02,320
사람에게는 매우 부자연스러운 일입니다.

395
00:23:02,320 --> 00:23:05,120
왜냐하면 인간은 단순히 관측을 행동으로 연결하지 않기 때문입니다.

396
00:23:05,120 --> 00:23:07,030
우리는 실제로 피드팩을 필요로 합니다.

397
00:23:07,030 --> 00:23:08,240
우리는 우리의 행동의 효과를 지켜보고 그에 따라 보상합니다.

398
00:23:08,240 --> 00:23:12,720
우라기 우리의 행동의 효과를 볼 수 없을 때 이것을 하는 것은 약간 어려울 수 있습니다

399
00:23:12,720 --> 00:23:14,400
물론 이것은 상황마다 다릅니다

400
00:23:14,400 --> 00:23:17,910
일부 도메인에서는 관찰에 대한 최적의 행동을 제공합니다.

401
00:23:17,910 --> 00:23:21,520
추상적인 의사 결정 문제와 같이 합리적으로 간단할 수 있습니다.

402
00:23:21,520 --> 00:23:26,240
예를 들어, 재고 조사와 관련된 연구를 하고 있다고 한다면,

403
00:23:26,240 --> 00:23:28,480
it's easy to ask an expert you know if

404
00:23:28,480 --> 00:23:29,440
your warehouse is in this state and your

405
00:23:29,440 --> 00:23:31,440
prices are this

406
00:23:31,440 --> 00:23:33,440
how should you change the prices but

407
00:23:33,440 --> 00:23:39,030
하지만, 이 이미지를 보면 운전대를 얼마나 돌려야 하는지 물어보는 것은 비교적 어렵습니다.

408
00:23:39,030 --> 00:23:42,880
고생하셨습니다.