1
00:12:43,830 --> 00:12:49,600
일반적으로 그렇지 않습니다.

2
00:12:48,000 --> 00:12:55,510
지도학습에 비해 왜 행동 복제가 안되는 지 설명을 하겠습니다.

3
00:12:53,270 --> 00:13:00,000
제어 문제에 대한 추상적인 사진으로 시작을 해보겠습니다.

4
00:12:56,720 --> 00:13:05,760
이번 강의에서는 많은 사진을 사용할 계획입니다.

5
00:13:01,920 --> 00:13:13,200
사진에서의 좌측의 축은 상태이고, 실제로 이 축은 1차원이 아닙니다.

6
00:13:12,000 --> 00:13:17,360
그렇지만, 설명을 돕기 위해 1차원이라 가정하겠습니다.

7
00:13:15,270 --> 00:13:18,720
그리고 우측의 축은 시간을 나타냅니다.

8
00:13:17,360 --> 00:13:25,830
그래서 이 검은 곡선은 다른 상태의 값이 있는 모든 시점의 시간 궤적을 나타냅니다.

9
00:13:24,160 --> 00:13:29,680
그리고 이 검은 궤적이 우리의 학습 데이터를 나타낸다고 가정해 봅시다

10
00:13:28,070 --> 00:13:39,680
우리는 이 학습 데이터가 S에서 A로 정책을 학습하기 위해 사용되는 것으로 간주할 것입니다.

11
00:13:37,200 --> 00:13:43,920
앞으로 빨간색 선으로 해당 정책이 실행될 때 어떤 상태를 가지게 되는 지 그릴 것입니다.

12
00:13:41,830 --> 00:13:47,270
처음에 정책은 학습 데이터에 매우 근접하게 유지됩니다.

13
00:13:45,600 --> 00:13:51,120
왜냐하면 우리는 잘 학습된 거대한 뉴럴 네트워크를 사용했기 때문입니다.

14
00:13:48,720 --> 00:13:52,630
하지만 약간의 오차는 발생하게 됩니다.

15
00:13:51,120 --> 00:13:57,830
모든 뉴럴 네트워크는 약간의 오차가 발생하게 되고, 이것은 기본적으로 불가피한 요소입니다.

16
00:13:55,760 --> 00:14:07,120
문제는 이 모델이 작은 오차를 가졌음에도. 훈련 받은 상태와는 다른 상태에 있다는 것입니다.

17
00:14:05,600 --> 00:14:11,190
이러한 문제는 더 큰 오차를 발생하게 되는데,

18
00:14:09,680 --> 00:14:15,600
이는 학습된 상태와는 다른 상태에 있는데, 다른 상태에 대한 학습이 되지 않았기 때문입니다.

19
00:14:13,120 --> 00:14:20,880
그리고 이러한 오차들이 복합적으로 작용함에 따라 상태는 점점 달라지고, 오차는 점점 커집니다.

20
00:14:19,040 --> 00:14:27,920
결국에는 학습된 행동과는 매우 다른 행동을 하게 됩니다.

21
00:14:26,160 --> 00:14:29,360
자동차 시나리오를 예시로 들자면,

22
00:14:27,920 --> 00:14:33,360
자동차가 아주 조금 왼쪽으로 움직여 학습되지 않은 것을 보게 될 것이고,

23
00:14:31,920 --> 00:14:37,040
여기서 조금 더 왼쪽으로 가게 됨으로써 결국 도로를 벗어나게 됩니다.

24
00:14:35,360 --> 00:14:44,320
그리고 우리는 이 현상을 더 공식적으로 설명하는 것을 나중에 강의에서 볼 것입니다.

25
00:14:41,920 --> 00:14:45,680
하지만 이것은 실제로 잘 작동합니다.

26
00:14:44,320 --> 00:14:47,760
가끔 꽤 효과적이기도 합니다.

27
00:14:45,680 --> 00:14:52,880
이 비디오들은 Nvidia에서 2016년에 발표한 논문에서 수집되었습니다

28
00:14:51,510 --> 00:14:56,000
초기 시스템에 많은 어려움을 겪었다는 것을 볼 수 있습니다.

29
00:14:54,070 --> 00:15:02,070
길에서 벗어나거나 원뿔콘을 쓰러트리는 등 예상치 못한 행동을 하곤 합니다.

30
00:14:59,440 --> 00:15:04,800
하지만 많은 데이터를 수집하고 약간의 트릭을 사용한 후

31
00:15:03,600 --> 00:15:18,560
그들은 실제로 원뿔 사이를 자율적으로 운전하고, 도로 위에서 꽤나 합리적인 행동을 하는 시스템을 만들게 되었습니다.

32
00:15:15,830 --> 00:15:26,320
왜 그럴듯한 정책을 훈련시키기 위해 행동 복제 방법을 사용할 수 있는 지

33
00:15:23,680 --> 00:15:30,390
Part 2에서 좀 더 자세히 논의할 것입니다.

34
00:15:28,240 --> 00:15:34,000
하지만 제가 지금 간단히 언급하고 싶은 것 중 하나는

35
00:15:31,920 --> 00:15:39,680
Nvidia가 발표한 이 논문에서 해당 문제를 다루기 위해 사용된 특별한 기술입니다.

36
00:15:37,750 --> 00:15:43,920
Nvidia의 논문에서 시스템에 대한 설명을 보면,

37
00:15:41,750 --> 00:15:48,070
우리가 예상했던 방법과 비슷한 방법을 사용했습니다.

38
00:15:45,120 --> 00:15:53,190
CNN 계층이 있고, 그 CNN 계층이 조향각을 생성합니다.

39
00:15:50,160 --> 00:16:00,070
자동차는 생성된 조향각을 조정하고, 다시 CNN이 카메라로부터 입력을 받게 됩니다.

40
00:15:56,800 --> 00:16:03,120
하지만 여기서 여러분이 여기서 확인해야될 점은

41
00:16:01,270 --> 00:16:06,000
전방, 좌측, 우측 카메라가 있다는 것입니다.

42
00:16:03,600 --> 00:16:19,040
이 카메라들은 논문에서 사용된 기술 중 중요한 것은 3개의 측면 카메라 이미지를 동시에 녹화하는 것입니다.

43
00:16:17,440 --> 00:16:22,720
전방 이미지는 자동차가 가지는 조향각과는 무관하게 관리됩니다.

44
00:16:20,720 --> 00:16:28,320
좌측 카메라 이미지는 오른쪽으로 방향을 트는 조향각을 관리합니다.

45
00:16:27,680 --> 00:16:34,000
즉, 도로의 좌측으로 도로를 벗어나는 이미지를 보게 되었다면 우측으로 각도를 잡아야 합니다.

46
00:16:32,160 --> 00:16:39,440
그에 상응하여 우측 카메라 이미지는 왼쪽으로 방향을 트는 조향각을 관리합니다.

47
00:16:37,680 --> 00:16:45,830
이제 이 특별한 기술이 어떻게 차를 운전하는 특별한 경우에 이 표류 문제를 완화시킬지 상상할 수 있을 것입니다.

48
00:16:43,680 --> 00:16:51,270
왜냐하면 이 좌측/우측 이미지들은 본질적으로 정책에 작은 오차들을 바로잡는 방법을 가르치기 때문입니다.

49
00:16:50,160 --> 00:16:55,270
그리고 오차를 수정하게 된다면 더 큰 오차를 계산하지 않게 될 것입니다.

50
00:16:53,830 --> 00:16:59,360
이것은 일반적인 원리에서의 특별한 경우입니다.

51
00:16:56,800 --> 00:17:03,360
보다 일반적인 원칙은 궤도의 오차가 복합적으로 계산되지만,

52
00:17:01,600 --> 00:17:07,910
작은 오차를 피드백할 수 있도록 학습 데이터를 수정할 수 있다면

53
00:17:05,910 --> 00:17:14,950
정책이 이러한 피드백 과정을 학습하고 안정화할 수 있다는 것입니다.

54
00:17:13,670 --> 00:17:19,830
물론 이러한 것을 하기 위한 많은 다른 방법들이 있으나, 나중에 이 코스에서 논의할 것입니다.

55
00:17:17,120 --> 00:17:24,880
예를 들어, 최적의 피드백 컨트롤러를 데모를 통하여 학습하고,

56
00:17:22,950 --> 00:17:31,600
이를 관리자(supervision)으로 사용한다면 꽤나 안정성을 계승한 정책을 학습할 수 있을 것입니다.

57
00:17:30,080 --> 00:17:35,670
혹은 고의적으로 실수를 저지르고, 그 실수를 바로잡는 방법을 사용할 수도 있습니다.

58
00:17:34,400 --> 00:17:39,840
그래서 우리가 이 문제를 해결하기 위해 사용할 수 있는 여러 방법들이 있습니다.

59
00:17:40,790 --> 00:17:48,160
하지만 우리가 조금 더 일반적인 해답을 도출하기 위해 요구하는 것은

60
00:17:46,790 --> 00:17:57,360
이 흐름 뒤에 있는 근본적인 수학적 원리가 무엇인가 하는 것입니다.

61
00:17:54,790 --> 00:18:01,120
우리가 정책을 실행하게 되면,

62
00:17:57,360 --> 00:18:03,910
주어진 o_t에서 pi_theta a_t를 추출합니다.

63
00:18:01,120 --> 00:18:08,080
그리고 이 추출된 분포는 특정 데이터 분포에 의해 학습되었습니다.

64
00:18:07,440 --> 00:18:11,670
그리고 우리는 그러한 분포를 P_data(O_t) 라고 부를 것입니다.

65
00:18:10,320 --> 00:18:18,160
이것은 기본적으로 우리의 훈련 데이터에 나타난 관찰의 분포입니다.

66
00:18:15,840 --> 00:18:24,960
우리는 지도학습을 통하여, 특정 학습 분포에 대해 특정 모델을 훈련한다면

67
00:18:22,720 --> 00:18:27,280
낮은 학습 오류를 얻으며 과적합 되지 않는다는 것을 배웠습니다.

68
00:18:24,960 --> 00:18:35,030
또한 동일한 분포에서 도출된 테스트 데이터인 경우, 낮은 테스트 오류를 얻을 것으로 예상할 수 있습니다.

69
00:18:32,240 --> 00:18:42,400
만약, 훈련 데이터와 동일한 분포에서 관측된 새로운 관찰이 기존 관찰과는 동일하지 않더라도

70
00:18:40,240 --> 00:18:48,640
우리는 학습된 정책이 새로운 관찰에 대하여 올바른 행동을 생산할 것을 기대할 것입니다.

71
00:18:45,910 --> 00:18:55,280
그러나 우리가 정책을 실제로 실행할 때, 우리가 보는 관찰에 대한 분포는 다릅니다.

72
00:18:53,600 --> 00:19:05,280
왜냐하면 정책이 관찰에 대하여 다른 행동을 수행하기 때문에 관찰의 분포가 달라집니다.

73
00:19:01,670 --> 00:19:08,960
마지막에는, P_(O_t) 는 P_data(O_t)와 달라지게 되고,

74
00:19:05,280 --> 00:19:15,030
이것이 복합적인 오차의 원인이다.

75
00:19:12,880 --> 00:19:19,360
그래서 우리는 P_data(O_t)와 P_(O_t)를 같게 만드려고 합니다.

76
00:19:16,160 --> 00:19:30,400
이게 가능하다면 우리는 정책이 지도학습의 표준 결과로부터 좋은 행동을 만들어 낼 것이라는 것을 압니다.

77
00:19:27,600 --> 00:19:36,160
이 두 개의 값을 같게 만드는 방법은 정책을 완벽하게 만드는 것입니다.

78
00:19:34,480 --> 00:19:42,880
만약 정책이 완벽하고, 오차가 없다면 이 분포들은 서로 일치할 것입니다.

79
00:19:39,670 --> 00:19:45,840
그렇지만, 당연하게도 매우 힘든 작업입니다.

80
00:19:42,880 --> 00:19:53,760
만약 우리가 정책에 대해 생각하는 대신에 우리의 데이터 분포에 대해서 더 생각한다면 어떨까요?

81
00:19:51,440 --> 00:20:02,150
그러니 어떤 방법으로 정책을 바꾸지 말고 실제로 우리의 데이터를 바꿔서 이 분포의 불일치 문제를 해결해보도록 합시다.

82
00:20:00,550 --> 00:20:08,480
이것은 DAgger라고 불리는 방법의 기본적인 아이디어입니다.

83
00:20:04,790 --> 00:20:11,440
DAgger는 "Dataset Aggregation"의 약자입니다.

84
00:20:08,480 --> 00:20:18,790
DAgger에서는, 훈련 데이터를 P_data(O_t) 에서 수집하는 대신 P_(O_t)에서 수집하는 것이 목표입니다.

85
00:20:16,400 --> 00:20:22,550
왜냐하면 우리는  P_(O_t) 분포에서의 관찰-행동 쌍을 가지고 있고

86
00:20:19,670 --> 00:20:31,760
이 쌍들을 학습한다면 데이터 분포의 불일치 문제가 사라지기 때문입니다.

87
00:20:29,120 --> 00:20:33,030
DAgger가 이걸 어떻게 해내는지 보여드리죠

88
00:20:31,760 --> 00:20:36,400
주어진 o_t에서 pi_theta a_t를 추출한 정책을 실행할 것이고,

89
00:20:33,030 --> 00:20:38,640
주어진 o_t에서 pi_theta a_t를 추출한 정책을 실행할 것이고,

90
00:20:36,400 --> 00:20:40,960
이 정책은 P_(O_t)로부터 샘플들을 생산할 것입니다.

91
00:20:38,640 --> 00:20:47,120
그리고 우리는 그 관찰들에서 추가적인 라벨을 요청할 것입니다.

92
00:20:44,000 --> 00:20:52,720
첫번째 단계는 인공적인 데이터 셋으로부터 학습을 하여 정책을 초기화하는 것입니다.

93
00:20:50,480 --> 00:20:58,400
그런 다음 관찰의 추가적인 데이터 셋을 수집하기 위해 정책을 실행할 것입니다.

94
00:20:55,200 --> 00:21:00,000
여기서 그 데이터 셋을 D_pi라 하고,

95
00:20:58,400 --> 00:21:03,520
이러한 관찰들은 P_(O_t) 로부터 추출됩니다.

96
00:21:00,000 --> 00:21:08,080
그러면 우리는 인공적으로 이 모든 관찰에 최적의 행동으로 라벨을 붙이도록 요청할 것입니다.

97
00:21:06,320 --> 00:21:12,960
그래서 누군가는 말 그대로 기계가 만들어낸 관찰을 지켜볼 것입니다.

98
00:21:11,280 --> 00:21:18,880
그리고 기계에게 해당 관찰로부터 취할 수 있는 최적의 행동이 무엇인지 표기합니다.

99
00:21:15,440 --> 00:21:19,840
그리고 나서 우리는 기존의 데이터 셋과 병합을 할 것입니다.

100
00:21:18,880 --> 00:21:25,670
우리는 이러한 데이터 셋을 병합하고 정책을 다시 학습시킬 것입니다.

101
00:21:24,000 --> 00:21:30,480
이 병합된 데이터 셋에 대해 정책을 다시 학습할 때 정책이 변경됩니다.

102
00:21:28,880 --> 00:21:37,520
이것은 D_pi 데이터 셋이 P_로부터 왔음에도 불구하고, thetha가 다르고, P_ 또한 다르다는 것을 의미합니다.

103
00:21:35,440 --> 00:21:42,000
그래서 우리는 이 과정을 반복해야 합니다.

104
00:21:39,840 --> 00:21:51,840
이러한 알고리즘을 충분히 반복한다면 추가적인 Converge를 가진다는 것이 ROSS의 논문에 설명되어 있습니다.

105
00:21:49,840 --> 00:21:59,910
최종 데이터 셋은 정책과 점근적으로 근사한 분포에서 가져오게 됩니다.

106
00:21:56,790 --> 00:22:03,670
여기 DAgger으로 학습된 정책의 예가 있습니다.

107
00:22:00,240 --> 00:22:05,280
숲 속을 드론을 날립니다.

108
00:22:03,670 --> 00:22:07,030
이 정책은 실제로 딥러닝을 사용하지 않습니다.

109
00:22:05,280 --> 00:22:10,320
실제로는 선형 이미지 특징들을 사용합니다.

110
00:22:08,790 --> 00:22:13,760
하지만 그 이후의 연구는 딥러닝에 대해서도 수행됐습니다.

111
00:22:12,000 --> 00:22:18,080
그래서 이 드론은 처음에 멀리 있는 숲을 잘 헤쳐나갈 수 없었습니다.

112
00:22:15,760 --> 00:22:22,080
하지만 몇번의 인공적인 라벨링 작업 이후로는

113
00:22:20,080 --> 00:22:24,640
꽤 능숙하게 숲을 항해할 수 있었습니다.

114
00:22:25,440 --> 00:22:29,280
그럼 DAgger의 문제는 무엇일까요?

115
00:22:27,670 --> 00:22:32,960
왜 우리는 항상 모방 학습에 이 알고리즘을 사용하지 않을까요?

116
00:22:31,200 --> 00:22:37,440
DAgger에 관한 많은 문제들은 3단계에서 비롯됩니다.

117
00:22:35,670 --> 00:22:40,240
문제에 따라 약간 다릅니다만,

118
00:22:37,440 --> 00:22:47,760
많은 경우, D_pi에 최적의 조치를 인공적으로 표시하는 것은 실제로 상당히 부담스러운 일일 수 있습니다.

119
00:22:45,200 --> 00:22:49,840
스스로 이것을 한다고 상상해 보세요

120
00:22:47,760 --> 00:22:53,760
드론이 숲을 날아가는 영상을 보고 있다고 상상해보세요.

121
00:22:51,280 --> 00:22:56,480
그리고 드론이 최적의 동작을 제공하도록 조종해야 합니다.

122
00:22:55,200 --> 00:22:59,840
실시간으로 드론에 영향을 주지 않으면서 말이죠.

123
00:22:58,150 --> 00:23:02,320
사람에게는 매우 부자연스러운 일입니다.

124
00:22:59,840 --> 00:23:05,120
왜냐하면 인간은 단순히 관측을 행동으로 연결하지 않기 때문입니다.

125
00:23:03,520 --> 00:23:07,030
우리는 실제로 피드팩을 필요로 합니다.

126
00:23:05,120 --> 00:23:08,240
우리는 우리의 행동의 효과를 지켜보고 그에 따라 보상합니다.

127
00:23:07,030 --> 00:23:12,720
우라기 우리의 행동의 효과를 볼 수 없을 때 이것을 하는 것은 약간 어려울 수 있습니다

128
00:23:11,120 --> 00:23:14,400
물론 이것은 상황마다 다릅니다

129
00:23:12,720 --> 00:23:17,910
일부 도메인에서는 관찰에 대한 최적의 행동을 제공합니다.

130
00:23:16,080 --> 00:23:21,520
추상적인 의사 결정 문제와 같이 합리적으로 간단할 수 있습니다.

131
00:23:20,400 --> 00:23:26,240
예를 들어, 재고 조사와 관련된 연구를 하고 있다고 한다면,

132
00:23:24,240 --> 00:23:28,480
it's easy to ask an expert you know if

133
00:23:26,240 --> 00:23:29,440
your warehouse is in this state and your

134
00:23:28,480 --> 00:23:31,440
prices are this

135
00:23:29,440 --> 00:23:33,440
how should you change the prices but

136
00:23:31,440 --> 00:23:39,030
하지만, 이 이미지를 보면 운전대를 얼마나 돌려야 하는지 물어보는 것은 비교적 어렵습니다.

137
00:23:39,200 --> 00:23:42,880
고생하셨습니다.

