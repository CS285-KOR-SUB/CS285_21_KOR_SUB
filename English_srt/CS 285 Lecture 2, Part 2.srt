1
00:00:01,040 --> 00:00:03,360
okay so we saw that we can address the

2
00:00:03,439 --> 00:00:04,960
distributional shift problem

3
00:00:04,960 --> 00:00:06,900
using the dagger algorithm but the

4
00:00:06,960 --> 00:00:08,360
dagger algorithm does have a few

5
00:00:08,400 --> 00:00:09,280
practical

6
00:00:09,360 --> 00:00:10,840
challenges that make it a little tricky

7
00:00:10,880 --> 00:00:12,180
to use because of this additional

8
00:00:12,240 --> 00:00:14,879
labeling step

9
00:00:14,880 --> 00:00:17,200
can we make deep imitation learning work

10
00:00:17,279 --> 00:00:18,359
without having to collect

11
00:00:18,400 --> 00:00:21,320
more data so dagger addresses the

12
00:00:21,359 --> 00:00:22,680
problem of distributional

13
00:00:22,720 --> 00:00:26,080
shift or drift but what if our model is

14
00:00:26,080 --> 00:00:26,800
so good

15
00:00:26,880 --> 00:00:28,320
that it doesn't have this drifting

16
00:00:28,400 --> 00:00:30,660
problem what if it's so accurate

17
00:00:30,720 --> 00:00:33,719
that it doesn't deviate far enough

18
00:00:33,760 --> 00:00:38,700
from p data to ot and still stays good

19
00:00:38,719 --> 00:00:39,920
well in order to do that we need to

20
00:00:40,000 --> 00:00:43,059
mimic the x-ray behavior very accurately

21
00:00:43,120 --> 00:00:44,460
but we have to also make sure that we

22
00:00:44,559 --> 00:00:46,879
don't overfit in practice there are a

23
00:00:46,960 --> 00:00:48,860
few things we can do to facilitate this

24
00:00:48,879 --> 00:00:51,320
that can actually work empirically

25
00:00:51,360 --> 00:00:52,279
although they do not have the

26
00:00:52,320 --> 00:00:56,079
theoretical guarantees the dagger has

27
00:00:56,239 --> 00:00:57,680
to try to understand what we can do we

28
00:00:57,760 --> 00:00:59,259
could first ask

29
00:00:59,280 --> 00:01:01,279
why might we fail to fit the expert data

30
00:01:01,359 --> 00:01:04,419
why might our model not be perfect

31
00:01:04,479 --> 00:01:07,059
there are a few possible reasons one

32
00:01:07,119 --> 00:01:08,100
reason is that

33
00:01:08,159 --> 00:01:10,400
even if the observation is fully

34
00:01:10,400 --> 00:01:11,959
markovian meaning that the observation

35
00:01:12,000 --> 00:01:14,100
is enough to perfectly ignore infer the

36
00:01:14,159 --> 00:01:15,120
state

37
00:01:15,195 --> 00:01:16,695
the humans behavior might still be

38
00:01:16,720 --> 00:01:19,200
non-markovian

39
00:01:19,200 --> 00:01:21,920
a second possibility is that if we have

40
00:01:22,000 --> 00:01:23,500
continuous actions

41
00:01:23,520 --> 00:01:25,160
the demonstrator's behavior might be

42
00:01:25,200 --> 00:01:26,480
multimodal

43
00:01:26,560 --> 00:01:28,040
meaning that they might

44
00:01:28,072 --> 00:01:29,792
inconsistently select from among

45
00:01:29,840 --> 00:01:31,160
multiple different

46
00:01:31,200 --> 00:01:33,300
modes in the distribution making it hard

47
00:01:33,360 --> 00:01:35,540
to imitate

48
00:01:35,600 --> 00:01:38,760
let's start with the first problem

49
00:01:38,840 --> 00:01:42,560
so typically we would learn a

50
00:01:42,560 --> 00:01:45,300
markovian policy that's a jargon that

51
00:01:45,360 --> 00:01:47,540
means that our policy depends only on ot

52
00:01:47,600 --> 00:01:51,840
not an ot minus one or ot minus two

53
00:01:52,479 --> 00:01:55,940
and a person might not actually act this

54
00:01:56,000 --> 00:01:56,599
way so

55
00:01:56,640 --> 00:01:58,799
being markovian basically means if we

56
00:01:58,799 --> 00:02:00,360
see the same thing twice

57
00:02:00,399 --> 00:02:02,520
we do the same thing twice regardless of

58
00:02:02,560 --> 00:02:04,320
what happened before

59
00:02:04,320 --> 00:02:06,280
in some cases humans do behave this way

60
00:02:06,320 --> 00:02:07,639
but in many cases they don't so if

61
00:02:07,680 --> 00:02:09,020
you're driving a car

62
00:02:09,039 --> 00:02:11,039
and you just had someone cut you off

63
00:02:11,120 --> 00:02:12,159
you're a little bit

64
00:02:12,239 --> 00:02:14,880
kind of shaken maybe you're a little

65
00:02:14,959 --> 00:02:15,679
anxious

66
00:02:15,680 --> 00:02:16,840
you'll react a little differently to

67
00:02:16,879 --> 00:02:18,600
whatever happens next than if that

68
00:02:18,640 --> 00:02:20,360
person hadn't done that

69
00:02:20,400 --> 00:02:22,540
more generally humans are just not very

70
00:02:22,560 --> 00:02:23,760
consistent

71
00:02:23,840 --> 00:02:26,140
and whatever strategy you're using to

72
00:02:26,160 --> 00:02:27,020
control

73
00:02:27,040 --> 00:02:29,479
 the system might be non-markovian

74
00:02:29,520 --> 00:02:30,800
even if there exists an optimal

75
00:02:30,879 --> 00:02:34,000
markovian strategy

76
00:02:34,319 --> 00:02:36,160
so it's often very unnatural for human

77
00:02:36,239 --> 00:02:38,079
demonstrators to be

78
00:02:38,160 --> 00:02:40,800
perfect markovian experts to take the

79
00:02:40,879 --> 00:02:41,919
same actions

80
00:02:42,000 --> 00:02:43,500
in the same state if they see the same

81
00:02:43,519 --> 00:02:45,940
state twice

82
00:02:46,000 --> 00:02:48,560
so maybe a person acts more like this

83
00:02:48,640 --> 00:02:49,700
maybe they they actually have a

84
00:02:49,760 --> 00:02:51,199
distribution over actions

85
00:02:51,200 --> 00:02:53,500
condition o1 through ot conditioning the

86
00:02:53,599 --> 00:02:56,320
entire history of observations

87
00:02:56,400 --> 00:02:57,860
and of course if this is how your expert

88
00:02:57,920 --> 00:02:59,719
is behaving then you won't be able to

89
00:02:59,760 --> 00:03:01,440
reproduce their behavior perfectly

90
00:03:01,519 --> 00:03:05,840
without any markovian policy

91
00:03:06,959 --> 00:03:09,220
now we could use the whole history in

92
00:03:09,280 --> 00:03:10,660
imitation

93
00:03:10,720 --> 00:03:12,519
so instead of just conditioning our

94
00:03:12,560 --> 00:03:14,280
model the current image that the car is

95
00:03:14,319 --> 00:03:16,060
observing we can condition on the entire

96
00:03:16,080 --> 00:03:18,520
history of images

97
00:03:18,560 --> 00:03:19,859
there are a few practical challenges

98
00:03:19,920 --> 00:03:22,120
with this though first

99
00:03:22,159 --> 00:03:23,880
if we simply concatenate all the images

100
00:03:23,920 --> 00:03:26,459
together we might have a variable number

101
00:03:26,480 --> 00:03:28,099
of images depending on how long our

102
00:03:28,159 --> 00:03:29,540
history is and it's

103
00:03:29,599 --> 00:03:31,399
not obvious how to input that into a

104
00:03:31,440 --> 00:03:33,540
standard convolutional neural network

105
00:03:33,599 --> 00:03:35,440
and also if the number of images is

106
00:03:35,440 --> 00:03:37,040
large we might end up with too many

107
00:03:37,040 --> 00:03:38,620
weights

108
00:03:38,640 --> 00:03:41,880
so a common solution to this issue

109
00:03:41,920 --> 00:03:43,619
is to employ some kind of recurrent

110
00:03:43,680 --> 00:03:46,240
network one very popular architecture

111
00:03:46,319 --> 00:03:47,000
for this

112
00:03:47,040 --> 00:03:49,160
is to use a convolutional encoder a

113
00:03:49,200 --> 00:03:50,520
convolutional neural network

114
00:03:50,560 --> 00:03:52,660
that reads in the image and turns that

115
00:03:52,720 --> 00:03:53,680
image into

116
00:03:53,760 --> 00:03:56,000
a RNN state and then we have a

117
00:03:56,095 --> 00:03:58,375
recurrent neural network

118
00:03:58,400 --> 00:04:00,539
a recurrent neural network backbone that

119
00:04:00,560 --> 00:04:01,380
encodes

120
00:04:01,439 --> 00:04:03,260
an arbitrary length sequence of these

121
00:04:03,280 --> 00:04:04,520
encodings

122
00:04:04,560 --> 00:04:05,900
into the current state which is then

123
00:04:05,920 --> 00:04:08,400
used to produce the action

124
00:04:08,400 --> 00:04:10,380
now in practice we would typically use

125
00:04:10,400 --> 00:04:12,080
something like a LSTM cell

126
00:04:12,080 --> 00:04:14,080
for actually representing this RNN we're

127
00:04:14,080 --> 00:04:15,220
not going to cover neural network

128
00:04:15,280 --> 00:04:16,659
architectures in great detail in this

129
00:04:16,720 --> 00:04:17,300
course

130
00:04:17,359 --> 00:04:19,520
so if you want to learn about RNNs and

131
00:04:19,600 --> 00:04:20,660
LSTM cells

132
00:04:20,720 --> 00:04:22,500
i would encourage you to do a little bit

133
00:04:22,560 --> 00:04:24,400
of your own reading the main point that

134
00:04:24,400 --> 00:04:25,560
i want to make is that these kind of

135
00:04:25,600 --> 00:04:26,960
LSTM

136
00:04:26,960 --> 00:04:29,279
or RNN type architectures can greatly

137
00:04:29,360 --> 00:04:32,380
mitigate the non-markovian problem

138
00:04:32,479 --> 00:04:33,600
so typically we would have shared

139
00:04:33,680 --> 00:04:35,520
weights for a combat encoder and then

140
00:04:35,600 --> 00:04:38,720
have a RNN

141
00:04:40,160 --> 00:04:42,660
all right now one little aside that I

142
00:04:42,720 --> 00:04:44,540
want to mention briefly is

143
00:04:44,560 --> 00:04:46,980
you know in general this RNN approach

144
00:04:47,040 --> 00:04:49,200
would mitigate the non-markovian policy

145
00:04:49,280 --> 00:04:49,980
issue

146
00:04:50,000 --> 00:04:51,660
but there are some reasons why it might

147
00:04:51,680 --> 00:04:54,080
also work poorly

148
00:04:54,160 --> 00:04:58,039
so to illustrate a potential issue

149
00:04:58,080 --> 00:05:00,540
with an imitation learning in general

150
00:05:00,639 --> 00:05:01,420
but that might

151
00:05:01,440 --> 00:05:03,120
be especially important when dealing

152
00:05:03,120 --> 00:05:05,520
with histories let's consider the

153
00:05:05,520 --> 00:05:06,639
following

154
00:05:06,720 --> 00:05:11,000
scenario let's say that you are

155
00:05:11,039 --> 00:05:14,379
driving the car and you have a camera

156
00:05:14,400 --> 00:05:16,400
that's inside the car so the camera can

157
00:05:16,479 --> 00:05:17,960
see out the front windshield

158
00:05:18,000 --> 00:05:20,140
and it also sees the dashboard and this

159
00:05:20,160 --> 00:05:22,180
car has a funny brake indicator on the

160
00:05:22,240 --> 00:05:23,620
dashboard I know most cars don't have

161
00:05:23,680 --> 00:05:25,120
this but let's say that it does so every

162
00:05:25,120 --> 00:05:26,319
time you step on the brake

163
00:05:26,320 --> 00:05:27,840
there's a little light that lights up on

164
00:05:27,919 --> 00:05:29,840
the dashboard

165
00:05:29,840 --> 00:05:31,799
now if there's a person standing in the

166
00:05:31,840 --> 00:05:33,119
road you really need to press on the

167
00:05:33,199 --> 00:05:33,860
brake

168
00:05:33,919 --> 00:05:35,320
and let's say that your data contains

169
00:05:35,360 --> 00:05:37,079
these kinds of examples

170
00:05:37,120 --> 00:05:40,420
but in all those examples when you hit

171
00:05:40,479 --> 00:05:41,500
the brake

172
00:05:41,520 --> 00:05:43,540
the light on the dashboard lights up and

173
00:05:43,600 --> 00:05:45,820
there's a person in front of the car

174
00:05:45,840 --> 00:05:49,080
so now your neural network model has to

175
00:05:49,120 --> 00:05:50,039
figure out

176
00:05:50,080 --> 00:05:52,560
is the braking caused by the presence of

177
00:05:52,639 --> 00:05:53,300
a person

178
00:05:53,360 --> 00:05:56,000
or by the light going off and it's very

179
00:05:56,000 --> 00:05:57,600
easy for the neural network to associate

180
00:05:57,600 --> 00:05:58,679
the light with the brake

181
00:05:58,800 --> 00:06:00,400
because the light always turns on when

182
00:06:00,479 --> 00:06:02,339
you're stepping on the brake

183
00:06:02,400 --> 00:06:04,560
it's a very consistent cue whereas the

184
00:06:04,639 --> 00:06:06,080
person standing in front of the car

185
00:06:06,160 --> 00:06:07,640
is a little more complicated to figure

186
00:06:07,680 --> 00:06:09,840
out

187
00:06:10,319 --> 00:06:12,760
if the brake light was obscured if

188
00:06:12,800 --> 00:06:14,139
information was removed from the

189
00:06:14,160 --> 00:06:14,960
observation

190
00:06:15,039 --> 00:06:18,159
then this issue would go away so we have

191
00:06:18,160 --> 00:06:19,680
kind of a funny conundrum here we're

192
00:06:19,759 --> 00:06:21,240
adding more observation

193
00:06:21,280 --> 00:06:23,780
adding this brake light actually makes

194
00:06:23,840 --> 00:06:25,259
imitation learning harder

195
00:06:25,280 --> 00:06:27,680
because it creates a kind of causal

196
00:06:27,759 --> 00:06:30,020
confusion problem it creates a situation

197
00:06:30,080 --> 00:06:30,719
where

198
00:06:30,720 --> 00:06:33,840
the cause and effect relationship

199
00:06:33,919 --> 00:06:35,420
between parts of your observation and

200
00:06:35,440 --> 00:06:36,700
your actions

201
00:06:36,720 --> 00:06:38,720
become difficult to deduce from just the

202
00:06:38,800 --> 00:06:41,119
data so the braking was caused by the

203
00:06:41,120 --> 00:06:42,319
presence of the person

204
00:06:42,319 --> 00:06:43,859
and the light went off because of the

205
00:06:43,919 --> 00:06:45,720
braking but the model might not be able

206
00:06:45,759 --> 00:06:47,100
to understand this I might instead

207
00:06:47,120 --> 00:06:48,780
conclude that the braking

208
00:06:48,800 --> 00:06:52,080
should be caused by the light

209
00:06:53,599 --> 00:06:55,220
so if you want to learn more about this

210
00:06:55,280 --> 00:06:57,100
phenomena there's a paper by pim dahan

211
00:06:57,120 --> 00:06:58,759
called causal confusion and imitation

212
00:06:58,800 --> 00:06:59,720
learning

213
00:06:59,759 --> 00:07:01,499
now this is not specific to

214
00:07:01,520 --> 00:07:03,160
markovian or non-markovian policies this

215
00:07:03,199 --> 00:07:04,500
can happen in all sorts of invitation

216
00:07:04,560 --> 00:07:06,339
learning scenarios

217
00:07:06,400 --> 00:07:08,620
but two questions that I might ask you

218
00:07:08,639 --> 00:07:10,680
to ponder a question one

219
00:07:10,720 --> 00:07:14,160
does including history make it causal confusion

220
00:07:14,160 --> 00:07:16,540
or make it worse I'd encourage you to

221
00:07:16,639 --> 00:07:17,780
think about this question

222
00:07:17,840 --> 00:07:19,240
and consider writing your answer in the

223
00:07:19,280 --> 00:07:21,380
comments

224
00:07:21,440 --> 00:07:24,000
question two can dagger mitigate causal

225
00:07:24,080 --> 00:07:25,999
confusion so if you use dagger

226
00:07:26,000 --> 00:07:29,560
will that fix this issue and if so why

227
00:07:29,599 --> 00:07:31,280
so I won't give you the answer this in

228
00:07:31,360 --> 00:07:32,880
the lecture but

229
00:07:32,880 --> 00:07:34,080
we could discuss this more in the

230
00:07:34,080 --> 00:07:36,880
discussion section

231
00:07:37,759 --> 00:07:40,400
okay so that's non-markovian behavior

232
00:07:40,400 --> 00:07:41,960
now let's talk about the other potential

233
00:07:42,000 --> 00:07:42,539
reason

234
00:07:42,560 --> 00:07:44,540
why we might fail to perfectly fit the

235
00:07:44,639 --> 00:07:45,780
expert behavior

236
00:07:45,847 --> 00:07:49,247
which is multimodal behavior so

237
00:07:49,280 --> 00:07:51,380
let's say that you're flying that drone

238
00:07:51,440 --> 00:07:53,280
in that video

239
00:07:53,280 --> 00:07:56,059
and you need to fly around a tree and

240
00:07:56,080 --> 00:07:57,699
maybe the human expert

241
00:07:57,759 --> 00:07:59,559
sometimes flies around the tree going

242
00:07:59,599 --> 00:08:01,740
left and sometimes flies around the tree

243
00:08:01,759 --> 00:08:04,140
going right

244
00:08:04,720 --> 00:08:07,239
now if your actions are discrete if you

245
00:08:07,280 --> 00:08:08,960
just have a discrete left action right

246
00:08:09,039 --> 00:08:10,519
action and straight action

247
00:08:10,560 --> 00:08:13,860
this is not a problem because a soft max

248
00:08:13,919 --> 00:08:14,899
distribution

249
00:08:14,960 --> 00:08:17,100
of the sort that we typically use when

250
00:08:17,199 --> 00:08:18,400
we want neural networks

251
00:08:18,400 --> 00:08:20,319
to output categorical distributions

252
00:08:20,400 --> 00:08:22,240
could easily capture the notion

253
00:08:22,319 --> 00:08:23,820
that the left action has high

254
00:08:23,840 --> 00:08:25,439
probability and the right action has

255
00:08:25,520 --> 00:08:26,540
high probability

256
00:08:26,639 --> 00:08:27,820
but the straight action is low

257
00:08:27,840 --> 00:08:30,700
probability so that's not a problem

258
00:08:30,720 --> 00:08:33,140
but if we have continuous actions then

259
00:08:33,200 --> 00:08:34,579
we would typically

260
00:08:34,640 --> 00:08:37,180
parametrize the output distribution as a

261
00:08:37,279 --> 00:08:38,799
multivariate normal distribution a

262
00:08:38,880 --> 00:08:39,580
gaussian

263
00:08:39,599 --> 00:08:42,297
determined by its mean and variance

264
00:08:42,880 --> 00:08:44,400
now if you just get to pick one mean and

265
00:08:44,480 --> 00:08:46,780
one variance how do you put that mean

266
00:08:46,800 --> 00:08:47,679
and variance

267
00:08:47,680 --> 00:08:49,220
to model the fact that you can go left

268
00:08:49,279 --> 00:08:50,680
and you can go right we should never go

269
00:08:50,720 --> 00:08:52,400
straight

270
00:08:52,480 --> 00:08:54,800
that causes a big problem right because

271
00:08:54,800 --> 00:08:55,540
essentially

272
00:08:55,600 --> 00:08:58,620
you average together the possibilities

273
00:08:58,640 --> 00:09:00,280
and you get exactly the wrong thing you

274
00:09:00,320 --> 00:09:02,480
get exactly the behavior going straight

275
00:09:02,480 --> 00:09:03,999
because that's the average between left

276
00:09:04,080 --> 00:09:06,399
and right so this is why multimodal

277
00:09:06,399 --> 00:09:08,300
behavior can cause problems

278
00:09:08,399 --> 00:09:09,919
but in general humans do tend to be

279
00:09:10,000 --> 00:09:11,479
multimodal humans tend to

280
00:09:11,519 --> 00:09:13,260
exhibit very complicated distributions

281
00:09:13,279 --> 00:09:14,500
in their demonstrations

282
00:09:14,560 --> 00:09:15,939
unless they're performing very simple

283
00:09:16,000 --> 00:09:17,780
tasks and without continuous actions that

284
00:09:17,839 --> 00:09:18,480
can be a big

285
00:09:18,480 --> 00:09:20,880
problem so there are a few possible

286
00:09:20,880 --> 00:09:22,060
solutions

287
00:09:22,080 --> 00:09:25,299
one solution is not to use

288
00:09:25,360 --> 00:09:28,320
a single gaussian output distribution

289
00:09:28,399 --> 00:09:30,320
instead of outputting the parameters of

290
00:09:30,320 --> 00:09:32,280
one gaussian a mean and a variance

291
00:09:32,320 --> 00:09:34,060
you can instead output a mixture of

292
00:09:34,080 --> 00:09:35,760
gaussians multiple means and multiple

293
00:09:35,760 --> 00:09:36,519
variances

294
00:09:36,560 --> 00:09:39,420
which could capture multiple modes you

295
00:09:39,519 --> 00:09:41,060
could also use a latent variable model

296
00:09:41,120 --> 00:09:42,500
which is a more sophisticated way to

297
00:09:42,560 --> 00:09:43,140
represent

298
00:09:43,200 --> 00:09:44,980
a complex probability distribution in

299
00:09:45,040 --> 00:09:46,860
continuous spaces

300
00:09:46,880 --> 00:09:48,140
and you could also do what's called

301
00:09:48,160 --> 00:09:50,180
autoregressive discretization

302
00:09:50,240 --> 00:09:51,640
so I'll talk about each of these three

303
00:09:51,680 --> 00:09:53,400
next

304
00:09:53,440 --> 00:09:55,560
first let's talk about mixtures of

305
00:09:55,600 --> 00:09:57,360
gaussians these are sometimes called

306
00:09:57,360 --> 00:10:00,480
mixture density networks so the idea is

307
00:10:00,560 --> 00:10:00,860
that

308
00:10:00,880 --> 00:10:03,300
instead of outputting mu and sigma the

309
00:10:03,360 --> 00:10:05,160
mean and variance of one gaussian

310
00:10:05,200 --> 00:10:08,560
you output n mu's and n sigmas

311
00:10:08,560 --> 00:10:11,320
and also nws and then the probability of

312
00:10:11,360 --> 00:10:12,320
a given o

313
00:10:12,320 --> 00:10:14,260
is given by a gaussian mixture a sum

314
00:10:14,320 --> 00:10:15,719
over each of those

315
00:10:15,760 --> 00:10:19,440
n of wi times a gaussian with mean ui

316
00:10:19,519 --> 00:10:22,580
and covariance sigma i

317
00:10:22,640 --> 00:10:25,620
so it's a very simple approach now

318
00:10:25,680 --> 00:10:27,120
some of the trade-offs with gaussian

319
00:10:27,120 --> 00:10:29,039
mixture

320
00:10:29,120 --> 00:10:31,820
mixture models for this are that you

321
00:10:31,839 --> 00:10:33,840
need more output parameters

322
00:10:33,920 --> 00:10:37,359
and

323
00:10:37,440 --> 00:10:38,920
the ability to model multimodal

324
00:10:38,959 --> 00:10:41,040
distributions in very high dimensions

325
00:10:41,120 --> 00:10:42,579
can be challenging so the higher the

326
00:10:42,640 --> 00:10:44,059
dimensionality is the more mixture

327
00:10:44,079 --> 00:10:44,860
elements you need

328
00:10:44,959 --> 00:10:46,100
and in general for arbitrary

329
00:10:46,160 --> 00:10:47,980
distributions the number of mixture

330
00:10:48,000 --> 00:10:49,660
elements needed to model them well

331
00:10:49,680 --> 00:10:51,320
in theory increases exponentially with

332
00:10:51,360 --> 00:10:53,100
dimensionality so

333
00:10:53,120 --> 00:10:54,720
if you're controlling the gas and brake

334
00:10:54,800 --> 00:10:56,240
and steering of a car that's just two

335
00:10:56,320 --> 00:10:56,980
dimensions

336
00:10:57,040 --> 00:10:58,880
that's not a problem but if you're

337
00:10:58,880 --> 00:11:00,859
controlling all of the joints of a

338
00:11:00,895 --> 00:11:02,875
humanoid robot which might be hundreds

339
00:11:02,959 --> 00:11:05,360
or all the prices of all the products on

340
00:11:05,360 --> 00:11:07,280
amazon which might be millions

341
00:11:07,360 --> 00:11:09,640
then gaussian mixtures might not be such

342
00:11:09,680 --> 00:11:12,240
a good choice

343
00:11:12,320 --> 00:11:14,800
a more sophisticated but more difficult

344
00:11:14,800 --> 00:11:15,560
to implement

345
00:11:15,600 --> 00:11:18,080
option is a latent variable model so in

346
00:11:18,160 --> 00:11:19,660
a latent variable model

347
00:11:19,680 --> 00:11:22,100
the output distribution is gaussian

348
00:11:22,160 --> 00:11:23,140
still

349
00:11:23,200 --> 00:11:25,380
but in addition to inputting the image

350
00:11:25,440 --> 00:11:26,879
we also input

351
00:11:26,880 --> 00:11:30,040
a latent variable into our model which

352
00:11:30,079 --> 00:11:31,380
might be drawn from some prior

353
00:11:31,440 --> 00:11:32,400
distribution

354
00:11:32,480 --> 00:11:34,780
so essentially our model takes in an

355
00:11:34,800 --> 00:11:36,459
image and some noise

356
00:11:36,480 --> 00:11:38,940
and turns that image and noise into a

357
00:11:38,959 --> 00:11:41,040
gaussian distribution of reactions

358
00:11:41,040 --> 00:11:42,640
for different noise inputs it might

359
00:11:42,640 --> 00:11:44,580
produce different gaussian distributions

360
00:11:44,640 --> 00:11:46,439
and in theory you can actually show that

361
00:11:46,480 --> 00:11:47,780
such a model can represent

362
00:11:47,839 --> 00:11:50,840
arbitrary distributions however training

363
00:11:50,880 --> 00:11:51,360
such a

364
00:11:51,440 --> 00:11:54,119
model can be a little tricky so I'm not

365
00:11:54,160 --> 00:11:55,500
going to go into detail about the math

366
00:11:55,519 --> 00:11:56,740
of how to train these latent variable

367
00:11:56,800 --> 00:11:57,359
models

368
00:11:57,440 --> 00:11:58,900
although we will have a lecture on

369
00:11:58,959 --> 00:11:59,940
something called variational

370
00:12:00,000 --> 00:12:00,959
autoencoders

371
00:12:01,040 --> 00:12:02,920
towards the second half of the class but

372
00:12:02,959 --> 00:12:04,300
for now if you're interested in latent

373
00:12:04,320 --> 00:12:05,580
variable models

374
00:12:05,600 --> 00:12:07,640
consider looking up conditional

375
00:12:07,680 --> 00:12:09,420
variational autoencoders

376
00:12:09,440 --> 00:12:12,320
normalizing flows or stein variational

377
00:12:12,320 --> 00:12:15,120
gradient descent

378
00:12:16,000 --> 00:12:18,460
the last option I'll cover which i think

379
00:12:18,480 --> 00:12:19,680
strikes a good balance

380
00:12:19,760 --> 00:12:22,460
between simplicity and expressivity is

381
00:12:22,480 --> 00:12:24,080
autoregressive discretization

382
00:12:24,160 --> 00:12:26,460
so a mixture of gaussians is very simple

383
00:12:26,480 --> 00:12:28,519
but has difficulty with very complex

384
00:12:28,560 --> 00:12:29,500
distributions

385
00:12:29,519 --> 00:12:31,119
a latent variable model is very

386
00:12:31,200 --> 00:12:33,440
expressive but more complex to implement

387
00:12:33,519 --> 00:12:35,440
another regressive discretization is

388
00:12:35,440 --> 00:12:37,019
perhaps a nice middle ground between the

389
00:12:37,040 --> 00:12:37,500
two

390
00:12:37,519 --> 00:12:39,580
it can represent arbitrary distributions

391
00:12:39,600 --> 00:12:40,860
but in my opinion it's quite a bit

392
00:12:40,880 --> 00:12:41,720
easier to use

393
00:12:41,760 --> 00:12:45,039
than latent variable models so the idea

394
00:12:45,120 --> 00:12:46,559
in auto regressive discretization is

395
00:12:46,639 --> 00:12:48,860
the following remember how if we have

396
00:12:48,959 --> 00:12:50,100
discrete actions

397
00:12:50,160 --> 00:12:51,960
this multimodality problem is not an

398
00:12:52,000 --> 00:12:54,080
issue because of discrete actions

399
00:12:54,160 --> 00:12:56,420
a soft max categorical distribution can

400
00:12:56,480 --> 00:12:58,640
easily represent any distribution

401
00:12:58,720 --> 00:13:01,040
however if you have continuous actions

402
00:13:01,040 --> 00:13:03,360
discretizing them can be challenging

403
00:13:03,440 --> 00:13:05,280
because in general the number of bins

404
00:13:05,360 --> 00:13:06,820
that you need for discretizing an

405
00:13:06,880 --> 00:13:08,720
n-dimensional action space is

406
00:13:08,720 --> 00:13:10,319
exponential in n

407
00:13:10,320 --> 00:13:11,480
so if you have only two action

408
00:13:11,519 --> 00:13:13,640
dimensions like steering and gas and

409
00:13:13,680 --> 00:13:14,199
brake

410
00:13:14,240 --> 00:13:16,339
that's easy to do but if you have many

411
00:13:16,399 --> 00:13:17,760
more dimensions this quickly becomes

412
00:13:17,839 --> 00:13:19,279
impractical

413
00:13:19,360 --> 00:13:21,220
autoregressive discretization

414
00:13:21,279 --> 00:13:23,900
discretizes one dimension at a time

415
00:13:23,920 --> 00:13:25,560
but can still represent arbitrary

416
00:13:25,600 --> 00:13:27,720
distributions by using a club

417
00:13:27,760 --> 00:13:31,120
a clever neural network trick

418
00:13:31,600 --> 00:13:35,359
so first we will discretize the first

419
00:13:35,360 --> 00:13:36,900
dimension of the action so we'll have a

420
00:13:36,959 --> 00:13:38,739
neural net that takes in the image

421
00:13:38,800 --> 00:13:41,759
and outputs a discretization

422
00:13:41,839 --> 00:13:44,860
of the first action dimension

423
00:13:44,959 --> 00:13:49,720
then we will sample from the softmax

424
00:13:49,760 --> 00:13:51,920
and then we'll have a value for the

425
00:13:51,920 --> 00:13:53,900
first action dimension

426
00:13:53,920 --> 00:13:55,540
and then we'll feed this value into

427
00:13:55,600 --> 00:13:57,380
another neural net that's going to

428
00:13:57,440 --> 00:13:58,819
output a distribution

429
00:13:58,880 --> 00:14:01,440
over the second action dimension we'll

430
00:14:01,519 --> 00:14:03,399
sample that and repeat

431
00:14:03,440 --> 00:14:06,339
so we discretize one dimension at a time

432
00:14:06,399 --> 00:14:07,760
which means that we never need to incur

433
00:14:07,839 --> 00:14:09,700
that exponential cost

434
00:14:09,760 --> 00:14:10,780
but because we're modeling the

435
00:14:10,800 --> 00:14:12,399
distribution over the next dimension

436
00:14:12,480 --> 00:14:14,380
condition on the previous one

437
00:14:14,399 --> 00:14:16,439
then by the chain rule of probability we

438
00:14:16,480 --> 00:14:17,820
can actually represent a full joint

439
00:14:17,839 --> 00:14:18,700
distribution

440
00:14:18,720 --> 00:14:21,600
over all of the action dimensions so

441
00:14:21,600 --> 00:14:23,600
this is an easy trick to implement

442
00:14:23,600 --> 00:14:25,560
and it actually ends up being quite

443
00:14:25,600 --> 00:14:28,460
powerful in practice

444
00:14:28,959 --> 00:14:33,340
okay so to recap we talked about how

445
00:14:33,360 --> 00:14:36,160
just imitation learning by itself is

446
00:14:36,240 --> 00:14:37,759
just behavioral cloning by itself is

447
00:14:37,760 --> 00:14:38,880
often but not always

448
00:14:38,959 --> 00:14:40,820
insufficient because of the distribution

449
00:14:40,880 --> 00:14:42,299
mismatch problem

450
00:14:42,320 --> 00:14:45,139
but sometimes it works well it can work

451
00:14:45,199 --> 00:14:46,599
well for instance

452
00:14:46,639 --> 00:14:48,399
as in that example with the nvidia paper

453
00:14:48,399 --> 00:14:50,380
where we use some trick to stabilize the

454
00:14:50,399 --> 00:14:51,619
system

455
00:14:51,680 --> 00:14:53,440
or more generally if our samples come

456
00:14:53,519 --> 00:14:56,180
from a stable trajectory distribution

457
00:14:56,240 --> 00:14:58,459
or if we add some additional on policy

458
00:14:58,480 --> 00:15:00,880
data for instance using dagger

459
00:15:00,959 --> 00:15:04,440
but also we can do a wide range of

460
00:15:04,480 --> 00:15:06,199
different tricks we can add a wide range

461
00:15:06,240 --> 00:15:07,120
of different tricks

462
00:15:07,199 --> 00:15:09,080
to improve our model so that it fits the

463
00:15:09,120 --> 00:15:10,840
data much more accurately

464
00:15:10,880 --> 00:15:12,180
and while this in theory doesn't

465
00:15:12,240 --> 00:15:13,680
alleviate the distributional shift

466
00:15:13,680 --> 00:15:14,480
problem

467
00:15:14,480 --> 00:15:16,880
in practice it can actually make even

468
00:15:16,880 --> 00:15:18,399
naive behavior cloning work

469
00:15:18,480 --> 00:15:20,379
if we're careful about tricks for

470
00:15:20,399 --> 00:15:22,039
handling non-markovian

471
00:15:22,079 --> 00:15:27,839
policies and for handling multimodality

