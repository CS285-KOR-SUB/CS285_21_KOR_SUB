1
00:00:01,040 --> 00:00:04,960
okay so we saw that we can address the
distributional shift problem

2
00:00:04,960 --> 00:00:06,730
using the DAgger algorithm

3
00:00:06,755 --> 00:00:09,739
but the DAgger algorithm does
have a few practical challenges

4
00:00:09,764 --> 00:00:14,879
that make it a little tricky to use
because of this additional labeling step

5
00:00:14,880 --> 00:00:20,069
can we make deep imitation learning
work without having to collect more data

6
00:00:20,093 --> 00:00:25,020
so dagger addresses the
problem of distributional shift or drift

7
00:00:25,045 --> 00:00:26,997
but what if our model is so good

8
00:00:27,022 --> 00:00:29,029
that it doesn't have
this drifting problem

9
00:00:29,054 --> 00:00:30,660
what if it's so accurate

10
00:00:30,720 --> 00:00:38,700
that it doesn't deviate far enough
from p data ot and still stays good

11
00:00:38,719 --> 00:00:43,059
well in order to do that we need to
mimic the expert behavior very accurately

12
00:00:43,120 --> 00:00:46,063
but we have to also make sure that
we don't overfit

13
00:00:46,088 --> 00:00:48,860
in practice there are a
few things we can do to facilitate this

14
00:00:48,879 --> 00:00:51,498
that can actually work empirically

15
00:00:51,523 --> 00:00:56,079
although they do not have the theoretical
guarantees the dagger has

16
00:00:56,364 --> 00:00:59,259
to try to understand what we can do we
could first ask

17
00:00:59,280 --> 00:01:01,334
why might we fail to fit the expert data

18
00:01:01,359 --> 00:01:04,469
why might our model not be perfect

19
00:01:04,494 --> 00:01:06,660
there are a few possible reasons

20
00:01:06,685 --> 00:01:11,168
one reason is that
even if the observation is fully markovian

21
00:01:11,193 --> 00:01:15,120
meaning that the observation is
enough to perfectly ignore infer the state

22
00:01:15,195 --> 00:01:19,200
the humans behavior might still be
non-markovian

23
00:01:19,200 --> 00:01:23,500
a second possibility is that if we have
continuous actions

24
00:01:23,520 --> 00:01:26,535
the demonstrator's behavior might be
multimodal

25
00:01:26,560 --> 00:01:31,160
meaning that they might inconsistently
select from among multiple different

26
00:01:31,200 --> 00:01:35,540
modes in the distribution making it hard
to imitate

27
00:01:35,600 --> 00:01:38,760
let's start with the first problem

28
00:01:38,840 --> 00:01:43,878
so typically we would learn
a markovian policy

29
00:01:43,903 --> 00:01:47,575
that's a jargon that
means that our policy depends only on ot

30
00:01:47,600 --> 00:01:51,840
not an ot minus one or ot minus two

31
00:01:52,479 --> 00:01:56,399
and a person might not
actually act this way

32
00:01:56,424 --> 00:02:00,360
so being markovian basically means if we
see the same thing twice

33
00:02:00,399 --> 00:02:04,320
we do the same thing twice regardless of
what happened before

34
00:02:04,320 --> 00:02:06,421
in some cases humans do behave this way

35
00:02:06,446 --> 00:02:09,020
but in many cases they don't so if
you're driving a car

36
00:02:09,039 --> 00:02:11,039
and you just had someone cut you off

37
00:02:11,064 --> 00:02:13,940
you're a little bit kind of shaken

38
00:02:13,964 --> 00:02:16,840
maybe you're a little anxious you'll
react a little differently to

39
00:02:16,879 --> 00:02:20,360
whatever happens next than if that
person hadn't done that

40
00:02:20,400 --> 00:02:23,760
more generally humans are
just not very consistent

41
00:02:23,840 --> 00:02:27,020
and whatever strategy
you're using to control

42
00:02:27,040 --> 00:02:34,000
the system might be non-markovian even if
there exists an optimal markovian strategy

43
00:02:34,319 --> 00:02:37,326
so it's often very unnatural
for human demonstrators

44
00:02:37,351 --> 00:02:41,975
to be perfect markovian experts to take the
same actions

45
00:02:42,000 --> 00:02:45,940
in the same state if they see the same
state twice

46
00:02:46,000 --> 00:02:48,560
so maybe a person acts more like this

47
00:02:48,640 --> 00:02:51,199
maybe they they actually have a
distribution over actions

48
00:02:51,200 --> 00:02:56,320
condition o1 through ot conditioning the
entire history of observations

49
00:02:56,400 --> 00:03:01,494
and of course if this is how your expert is behaving then
you won't be able to reproduce their behavior perfectly

50
00:03:01,519 --> 00:03:05,840
without any markovian policy

51
00:03:06,959 --> 00:03:10,660
now we could use the whole history in
imitation

52
00:03:10,720 --> 00:03:15,012
so instead of just conditioning our
model the current image that the car is observing

53
00:03:15,037 --> 00:03:18,520
we can condition on the entire
history of images

54
00:03:18,560 --> 00:03:21,210
there are a few practical challenges
with this though

55
00:03:21,235 --> 00:03:25,014
first if we simply concatenate
all the images together

56
00:03:25,039 --> 00:03:29,540
we might have a variable number of images
depending on how long our history is and it's

57
00:03:29,599 --> 00:03:33,540
not obvious how to input that into a
standard convolutional neural network

58
00:03:33,599 --> 00:03:38,620
and also if the number of images is large
we might end up with too many weights

59
00:03:38,640 --> 00:03:44,948
so a common solution to this issue
is to employ some kind of recurrent network

60
00:03:44,973 --> 00:03:50,520
one very popular architecture for this is to use a
convolutional encoder a convolutional neural network

61
00:03:50,560 --> 00:03:55,622
that reads in the image and turns that
image into a RNN state

62
00:03:55,647 --> 00:04:01,380
and then we have a recurrent neural network a
recurrent neural network backbone that encodes

63
00:04:01,439 --> 00:04:04,520
an arbitrary length sequence
of these encodings

64
00:04:04,560 --> 00:04:08,400
into the current state which is then
used to produce the action

65
00:04:08,400 --> 00:04:13,986
now in practice we would typically use
something like a LSTM cell for actually representing this RNN

66
00:04:14,011 --> 00:04:17,334
we're not going to cover
neural network architectures in great detail in this course

67
00:04:17,359 --> 00:04:23,714
so if you want to learn about RNNs and LSTM
cells i would encourage you to do a little bit of your own reading

68
00:04:23,739 --> 00:04:32,454
the main point that i want to make is that these kind of LSTM or RNN
type architectures can greatly mitigate the non-markovian problem

69
00:04:32,479 --> 00:04:38,720
so typically we would have shared weights
for a common encoder and then have a RNN

70
00:04:40,160 --> 00:04:44,540
all right now one little aside that I
want to mention briefly is

71
00:04:44,560 --> 00:04:49,980
you know in general this RNN approach would
mitigate the non-markovian policy issue

72
00:04:50,000 --> 00:04:54,080
but there are some reasons why it might
also work poorly

73
00:04:54,160 --> 00:04:58,039
so to illustrate a potential issue

74
00:04:58,080 --> 00:05:00,540
with imitation learning in general

75
00:05:00,639 --> 00:05:04,439
but that might be especially important
when dealing with histories

76
00:05:04,464 --> 00:05:08,137
let's consider the
following scenario

77
00:05:08,162 --> 00:05:12,517
let's say that you are driving the car

78
00:05:12,542 --> 00:05:18,120
and you have a camera that's inside the car
so the camera can see out the front windshield

79
00:05:18,145 --> 00:05:23,913
and it also sees the dashboard and this car has a funny
brake indicator on the dashboard I know most cars don't have this

80
00:05:23,938 --> 00:05:29,840
but let's say that it does so every time you step on the
brake there's a little light that lights up on the dashboard

81
00:05:29,840 --> 00:05:33,997
now if there's a person standing in the
road you really need to press on the brake

82
00:05:34,022 --> 00:05:37,198
and let's say that your data contains
these kinds of examples

83
00:05:37,223 --> 00:05:41,500
but in all those examples
when you hit the brake

84
00:05:41,520 --> 00:05:45,820
the light on the dashboard lights up and
there's a person in front of the car

85
00:05:45,840 --> 00:05:48,887
so now your neural network model

86
00:05:48,912 --> 00:05:53,300
has to figure out is the braking caused by
the presence of a person

87
00:05:53,360 --> 00:05:55,438
or by the light going off

88
00:05:55,463 --> 00:05:58,858
and it's very easy for the neural network
to associate the light with the brake

89
00:05:58,883 --> 00:06:02,339
because the light always turns on when
you're stepping on the brake

90
00:06:02,400 --> 00:06:06,279
it's a very consistent cue whereas the
person standing in front of the car

91
00:06:06,304 --> 00:06:09,840
is a little more complicated to figure
out

92
00:06:10,319 --> 00:06:15,127
if the brake light was obscured if
information was removed from the observation

93
00:06:15,152 --> 00:06:17,302
then this issue would go away

94
00:06:17,786 --> 00:06:21,240
so we have kind of a funny conundrum
here we're adding more observation

95
00:06:21,280 --> 00:06:23,145
adding this brake light

96
00:06:23,170 --> 00:06:25,343
actually makes imitation learning harder

97
00:06:25,368 --> 00:06:28,962
because it creates a kind of causal confusion problem

98
00:06:28,987 --> 00:06:36,700
it creates a situation where the cause and effect relationship
between parts of your observation

99
00:06:36,720 --> 00:06:39,624
and your actions become difficult to deduce from just the data

100
00:06:39,649 --> 00:06:42,319
so the braking was caused by the
presence of the person

101
00:06:42,319 --> 00:06:44,698
and the light went off because of the braking

102
00:06:44,723 --> 00:06:47,944
but the model might not be able
to understand this I might instead conclude

103
00:06:47,969 --> 00:06:52,080
that the braking
should be caused by the light

104
00:06:53,599 --> 00:06:59,720
so if you want to learn more about this phenomena there's a
paper by de haan called causal confusion and imitation learning

105
00:06:59,759 --> 00:07:06,171
now this is not specific to markovian or non-markovian policies
this can happen in all sorts of imitation learning scenarios

106
00:07:06,196 --> 00:07:09,783
but two questions that I might ask you to ponder

107
00:07:09,808 --> 00:07:16,021
question one does including history make it causal confusion
or make it worse

108
00:07:16,046 --> 00:07:21,380
I'd encourage you to think about this question
and consider writing your answer in the comments

109
00:07:21,440 --> 00:07:24,756
question two can dagger mitigate causal confusion

110
00:07:24,781 --> 00:07:29,560
so if you use dagger
will that fix this issue and if so why

111
00:07:29,599 --> 00:07:32,210
so I won't give you the answer this in the lecture

112
00:07:32,235 --> 00:07:36,880
but we could discuss this more in the
discussion section

113
00:07:37,759 --> 00:07:40,400
okay so that's non-markovian behavior

114
00:07:40,400 --> 00:07:42,539
now let's talk about the other potential
reason

115
00:07:42,560 --> 00:07:45,780
why we might fail to perfectly fit the
expert behavior

116
00:07:45,847 --> 00:07:48,466
which is multimodal behavior

117
00:07:48,491 --> 00:07:53,280
so let's say that you're flying that drone
in that video

118
00:07:53,280 --> 00:07:55,899
and you need to fly around a tree

119
00:07:55,924 --> 00:07:57,699
and maybe the human expert

120
00:07:57,759 --> 00:08:00,494
sometimes flies around the tree going left

121
00:08:00,519 --> 00:08:04,140
and sometimes flies around the tree
going right

122
00:08:04,720 --> 00:08:07,140
now if your actions are discrete

123
00:08:07,165 --> 00:08:10,519
if you just have a discrete left
action right action and straight action

124
00:08:10,560 --> 00:08:14,935
this is not a problem because a soft max
distribution

125
00:08:14,960 --> 00:08:16,991
of the sort that we typically use

126
00:08:17,016 --> 00:08:18,400
when we want neural networks

127
00:08:18,400 --> 00:08:20,375
to output categorical distributions

128
00:08:20,400 --> 00:08:22,294
could easily capture the notion

129
00:08:22,319 --> 00:08:26,614
that the left action has high probability
and the right action has high probability

130
00:08:26,639 --> 00:08:30,700
but the straight action is low
probability so that's not a problem

131
00:08:30,720 --> 00:08:32,910
but if we have continuous actions

132
00:08:32,935 --> 00:08:34,450
then we would typically

133
00:08:34,475 --> 00:08:39,580
parametrize the output distribution as a
multivariate normal distribution a gaussian

134
00:08:39,599 --> 00:08:42,297
determined by its mean and variance

135
00:08:42,880 --> 00:08:45,870
now if you just get to pick one mean and one variance

136
00:08:45,895 --> 00:08:47,679
how do you put that mean
and variance

137
00:08:47,680 --> 00:08:49,254
to model the fact that you can go left

138
00:08:49,279 --> 00:08:52,400
and you can go right we should never go
straight

139
00:08:52,480 --> 00:08:55,575
that causes a big problem right
because essentially

140
00:08:55,600 --> 00:08:58,620
you average together the possibilities

141
00:08:58,640 --> 00:09:00,295
and you get exactly the wrong thing

142
00:09:00,320 --> 00:09:02,480
you get exactly the behavior going straight

143
00:09:02,480 --> 00:09:05,375
because that's the average between left and right

144
00:09:05,400 --> 00:09:08,478
so this is why multimodal
behavior can cause problems

145
00:09:08,503 --> 00:09:10,700
but in general humans do tend to be multimodal

146
00:09:10,725 --> 00:09:14,535
humans tend to exhibit very complicated
distributions in their demonstrations

147
00:09:14,560 --> 00:09:19,962
unless they're performing very simple tasks and
without continuous actions that can be a big problem

148
00:09:19,987 --> 00:09:22,060
so there are a few possible solutions

149
00:09:22,080 --> 00:09:28,320
one solution is not to use
a single gaussian output distribution

150
00:09:28,399 --> 00:09:32,280
instead of outputting the parameters of
one gaussian a mean and a variance

151
00:09:32,320 --> 00:09:36,535
you can instead output a mixture of gaussians
multiple means and multiple variances

152
00:09:36,560 --> 00:09:39,420
which could capture multiple modes

153
00:09:39,519 --> 00:09:42,535
you could also use a latent variable model
which is a more sophisticated way to

154
00:09:42,560 --> 00:09:46,860
represent a complex probability
distribution in continuous spaces

155
00:09:46,880 --> 00:09:50,215
and you could also do what's called
autoregressive discretization

156
00:09:50,240 --> 00:09:53,400
so I'll talk about each of these three
next

157
00:09:53,440 --> 00:09:59,795
first let's talk about mixtures of
gaussians these are sometimes called mixture density networks

158
00:09:59,820 --> 00:10:05,230
so the idea is that instead of outputting mu
and sigma the mean and variance of one gaussian

159
00:10:05,255 --> 00:10:10,392
you output n mus and n sigmas
and also n ws

160
00:10:10,417 --> 00:10:15,090
and then the probability of a given o
is given by a gaussian mixture a sum over

161
00:10:15,115 --> 00:10:22,765
each of those n of wi times a gaussian
with mean ui and covariance sigma i

162
00:10:22,790 --> 00:10:24,758
so it's a very simple approach

163
00:10:24,783 --> 00:10:29,095
now some of the trade-offs with gaussian mixture

164
00:10:29,120 --> 00:10:33,895
mixture models for this are that you
need more output parameters

165
00:10:33,920 --> 00:10:41,095
and the ability to model multimodal
distributions in very high dimensions

166
00:10:41,120 --> 00:10:44,934
can be challenging so the higher the dimensionality
is the more mixture elements you need

167
00:10:44,959 --> 00:10:47,140
and in general for arbitrary distributions

168
00:10:47,165 --> 00:10:49,735
the number of mixture
elements needed to model them well

169
00:10:49,760 --> 00:10:52,659
in theory increases exponentially with dimensionality

170
00:10:52,684 --> 00:10:56,980
so if you're controlling the gas and brake and
steering of a car that's just two dimensions

171
00:10:57,040 --> 00:10:58,880
that's not a problem but if you're

172
00:10:58,880 --> 00:11:00,859
controlling all of the joints of a

173
00:11:00,895 --> 00:11:02,875
humanoid robot which might be hundreds

174
00:11:02,959 --> 00:11:05,360
or all the prices of all the products on

175
00:11:05,360 --> 00:11:07,280
amazon which might be millions

176
00:11:07,360 --> 00:11:12,295
then gaussian mixtures might not be such
a good choice

177
00:11:12,320 --> 00:11:16,154
a more sophisticated but more difficult
to implement option

178
00:11:16,179 --> 00:11:17,939
is a latent variable model

179
00:11:17,964 --> 00:11:23,140
so in a latent variable model the
output distribution is gaussian still

180
00:11:23,200 --> 00:11:25,415
but in addition to inputting the image

181
00:11:25,440 --> 00:11:29,924
we also input
a latent variable into our model

182
00:11:29,949 --> 00:11:32,455
which might be drawn from some prior
distribution

183
00:11:32,480 --> 00:11:36,459
so essentially our model takes in an
image and some noise

184
00:11:36,480 --> 00:11:41,040
and turns that image and noise into a
gaussian distribution of reactions

185
00:11:41,040 --> 00:11:44,615
for different noise inputs it might
produce different gaussian distributions

186
00:11:44,640 --> 00:11:50,170
and in theory you can actually show that
such a model can represent arbitrary distributions

187
00:11:50,195 --> 00:11:53,266
however training such a model
can be a little tricky

188
00:11:53,291 --> 00:11:57,472
so I'm not going to go into detail about the
math of how to train these latent variable models

189
00:11:57,497 --> 00:12:02,865
although we will have a lecture on something called
variational autoencoders towards the second half of the class

190
00:12:02,890 --> 00:12:05,575
but for now if you're interested in latent
variable models

191
00:12:05,600 --> 00:12:09,420
consider looking up conditional
variational autoencoders

192
00:12:09,440 --> 00:12:15,120
normalizing flows or stein variational
gradient descent

193
00:12:16,000 --> 00:12:17,944
the last option I'll cover

194
00:12:17,969 --> 00:12:24,285
which i think strikes a good balance between simplicity
and expressivity is autoregressive discretization

195
00:12:24,310 --> 00:12:26,460
so a mixture of gaussians is very simple

196
00:12:26,480 --> 00:12:29,500
but has difficulty with very complex
distributions

197
00:12:29,519 --> 00:12:33,494
a latent variable model is very
expressive but more complex to implement

198
00:12:33,519 --> 00:12:37,549
another regressive discretization is
perhaps a nice middle ground between the two

199
00:12:37,574 --> 00:12:41,850
it can represent arbitrary distributions but
in my opinion it's quite a bit easier to use

200
00:12:41,875 --> 00:12:43,962
than latent variable models

201
00:12:44,398 --> 00:12:47,833
so the idea in auto regressive discretization is
the following

202
00:12:47,858 --> 00:12:50,285
remember how if we have
discrete actions

203
00:12:50,310 --> 00:12:52,785
this multimodality problem is not an issue

204
00:12:52,810 --> 00:12:58,695
because of discrete actions a softmax categorical
distribution can easily represent any distribution

205
00:12:58,720 --> 00:13:01,040
however if you have continuous actions

206
00:13:01,040 --> 00:13:03,415
discretizing them can be challenging

207
00:13:03,440 --> 00:13:06,855
because in general the number of ㅊs
that you need for discretizing an

208
00:13:06,880 --> 00:13:10,319
n-dimensional action space is exponential in n

209
00:13:10,320 --> 00:13:14,215
so if you have only two action
dimensions like steering and gas and brake

210
00:13:14,240 --> 00:13:19,335
that's easy to do but if you have many more
dimensions this quickly becomes impractical

211
00:13:19,360 --> 00:13:21,254
autoregressive discretization

212
00:13:21,279 --> 00:13:23,900
discretizes one dimension at a time

213
00:13:23,920 --> 00:13:27,735
but can still represent arbitrary
distributions by using a club

214
00:13:27,760 --> 00:13:31,120
a clever neural network trick

215
00:13:31,600 --> 00:13:36,562
so first we will discretize the first
dimension of the action

216
00:13:36,587 --> 00:13:41,909
so we'll have a neural net that takes in
the image and outputs a discretization

217
00:13:41,934 --> 00:13:44,860
of the first action dimension

218
00:13:44,885 --> 00:13:49,720
then we will sample from the softmax

219
00:13:49,760 --> 00:13:53,900
and then we'll have a value for the
first action dimension

220
00:13:53,920 --> 00:13:58,855
and then we'll feed this value into another
neural net that's going to output a distribution

221
00:13:58,880 --> 00:14:01,316
over the second action dimension

222
00:14:01,341 --> 00:14:03,399
we'll sample that and repeat

223
00:14:03,440 --> 00:14:06,339
so we discretize one dimension at a time

224
00:14:06,399 --> 00:14:09,700
which means that we never need to incur
that exponential cost

225
00:14:09,760 --> 00:14:14,380
but because we're modeling the distribution over
the next dimension condition on the previous one

226
00:14:14,399 --> 00:14:18,700
then by the chain rule of probability we can
actually represent a full joint distribution

227
00:14:18,720 --> 00:14:21,600
over all of the action dimensions

228
00:14:21,600 --> 00:14:23,600
so this is an easy trick to implement

229
00:14:23,600 --> 00:14:28,460
and it actually ends up being quite
powerful in practice

230
00:14:28,959 --> 00:14:33,340
okay so to recap we talked about how

231
00:14:33,360 --> 00:14:36,215
just imitation learning by itself is

232
00:14:36,240 --> 00:14:38,934
just behavioral cloning by itself is
often but not always

233
00:14:38,959 --> 00:14:42,299
insufficient because of the distribution
mismatch problem

234
00:14:42,320 --> 00:14:44,407
but sometimes it works well

235
00:14:44,432 --> 00:14:48,399
it can work well for instance
as in that example with the nvidia paper

236
00:14:48,399 --> 00:14:51,655
where we use some trick 
to stabilize the system

237
00:14:51,680 --> 00:14:56,215
or more generally if our samples come
from a stable trajectory distribution

238
00:14:56,240 --> 00:15:00,934
or if we add some additional on policy
data for instance using dagger

239
00:15:00,959 --> 00:15:07,174
but also we can do a wide range of different
tricks we can add a wide range of different tricks

240
00:15:07,199 --> 00:15:10,840
to improve our model so that it fits the
data much more accurately

241
00:15:10,880 --> 00:15:14,480
and while this in theory doesn't
alleviate the distributional shift problem

242
00:15:14,480 --> 00:15:18,455
in practice it can actually make even
naive behavior cloning work

243
00:15:18,480 --> 00:15:22,054
if we're careful about tricks for
handling non-markovian

244
00:15:22,079 --> 00:15:27,839
policies and for handling multimodality

