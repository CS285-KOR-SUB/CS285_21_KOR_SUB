1
00:00:00,719 --> 00:00:05,200
hello and welcome to the second lecture

2
00:00:02,960 --> 00:00:06,240
of cs285

3
00:00:05,200 --> 00:00:09,200
today we're going to talk about

4
00:00:06,240 --> 00:00:10,800
supervised learning of behaviors

5
00:00:09,200 --> 00:00:13,120
so let's start with a little bit of

6
00:00:10,800 --> 00:00:15,759
terminology and notation

7
00:00:13,120 --> 00:00:16,960
if we have a regular supervised learning

8
00:00:15,759 --> 00:00:18,720
problem

9
00:00:16,960 --> 00:00:20,240
let's say a computer vision problem an

10
00:00:18,720 --> 00:00:22,240
object recognition problem

11
00:00:20,240 --> 00:00:24,640
we might want to recognize objects in an

12
00:00:22,240 --> 00:00:27,039
image so we might have some input image

13
00:00:24,640 --> 00:00:29,199
that goes through a deep neural network

14
00:00:27,039 --> 00:00:31,840
and the output is a label

15
00:00:29,199 --> 00:00:32,320
so the terminology we're going to use

16
00:00:31,840 --> 00:00:33,840
here

17
00:00:32,320 --> 00:00:36,079
is going to be kind of reinforcement

18
00:00:33,840 --> 00:00:37,200
learning terminology and i'll gradually

19
00:00:36,079 --> 00:00:39,600
work from

20
00:00:37,200 --> 00:00:41,120
using reinforcement terminology from us

21
00:00:39,600 --> 00:00:41,760
for a standard supervised learning

22
00:00:41,120 --> 00:00:44,480
example

23
00:00:41,760 --> 00:00:46,160
to then turn that into a reinforcement

24
00:00:44,480 --> 00:00:48,879
learning problem

25
00:00:46,160 --> 00:00:49,920
so we're going to call the input o for

26
00:00:48,879 --> 00:00:51,520
observation

27
00:00:49,920 --> 00:00:53,920
and we're going to call the output a for

28
00:00:51,520 --> 00:00:56,000
action but for now the input is an image

29
00:00:53,920 --> 00:00:57,920
and the output is a label

30
00:00:56,000 --> 00:00:59,280
the neural network in the middle or in

31
00:00:57,920 --> 00:01:00,079
general whatever kind of model you might

32
00:00:59,280 --> 00:01:02,800
want to have

33
00:01:00,079 --> 00:01:03,520
that maps the observations to actions

34
00:01:02,800 --> 00:01:06,479
we're going to call

35
00:01:03,520 --> 00:01:07,600
policy and we'll denote it with the the

36
00:01:06,479 --> 00:01:10,240
letter pi

37
00:01:07,600 --> 00:01:12,479
where the subscript theta represents the

38
00:01:10,240 --> 00:01:12,960
parameters of that policy so in a neural

39
00:01:12,479 --> 00:01:14,880
net

40
00:01:12,960 --> 00:01:16,960
theta represents the weights of that

41
00:01:14,880 --> 00:01:19,759
neural net so we have an input

42
00:01:16,960 --> 00:01:20,080
o an output a and a mapping between them

43
00:01:19,759 --> 00:01:21,759
pi

44
00:01:20,080 --> 00:01:23,200
subscript theta which gives a

45
00:01:21,759 --> 00:01:26,400
distribution over a

46
00:01:23,200 --> 00:01:28,320
given o

47
00:01:26,400 --> 00:01:30,479
now in reinforcement learning of course

48
00:01:28,320 --> 00:01:33,200
we're concerned with

49
00:01:30,479 --> 00:01:35,200
sequential decision making problems so

50
00:01:33,200 --> 00:01:37,200
all of these inputs and outputs

51
00:01:35,200 --> 00:01:39,119
occur at some point in time so we'll

52
00:01:37,200 --> 00:01:41,200
typically use a subscript

53
00:01:39,119 --> 00:01:43,600
t to denote the time step at which they

54
00:01:41,200 --> 00:01:45,759
happen usually in reinforcement learning

55
00:01:43,600 --> 00:01:47,119
we deal with discrete time problems so

56
00:01:45,759 --> 00:01:49,040
we assume that

57
00:01:47,119 --> 00:01:50,960
time is broken up into little discrete

58
00:01:49,040 --> 00:01:51,600
steps and t is an integer that

59
00:01:50,960 --> 00:01:54,159
represents

60
00:01:51,600 --> 00:01:55,119
at which step do you observe o and at

61
00:01:54,159 --> 00:01:58,320
which step do you

62
00:01:55,119 --> 00:02:00,240
emit a so now pi theta

63
00:01:58,320 --> 00:02:02,880
gives a distribution over a t

64
00:02:00,240 --> 00:02:04,880
conditional ot

65
00:02:02,880 --> 00:02:06,960
and of course unlike regular supervised

66
00:02:04,880 --> 00:02:09,440
learning in reinforcement learning

67
00:02:06,960 --> 00:02:10,080
the output at one time step influences

68
00:02:09,440 --> 00:02:13,599
the input

69
00:02:10,080 --> 00:02:17,200
at the next so a t has an effect on ot

70
00:02:13,599 --> 00:02:19,520
plus one so if you for example

71
00:02:17,200 --> 00:02:20,560
fail to recognize the tiger then at the

72
00:02:19,520 --> 00:02:23,120
next time step

73
00:02:20,560 --> 00:02:24,720
you might see something undesirable like

74
00:02:23,120 --> 00:02:26,400
maybe the tiger will be a lot closer to

75
00:02:24,720 --> 00:02:29,520
you

76
00:02:26,400 --> 00:02:32,640
so you could extend this basic idea

77
00:02:29,520 --> 00:02:34,800
to learn policies

78
00:02:32,640 --> 00:02:36,239
for control so obviously instead of

79
00:02:34,800 --> 00:02:37,120
outputting labels you would probably

80
00:02:36,239 --> 00:02:38,879
output something

81
00:02:37,120 --> 00:02:40,239
that looks a lot more like an action but

82
00:02:38,879 --> 00:02:40,959
it could still be a discrete action it

83
00:02:40,239 --> 00:02:43,200
could still

84
00:02:40,959 --> 00:02:44,560
use a soft max distribution so for

85
00:02:43,200 --> 00:02:45,920
instance you could choose from a

86
00:02:44,560 --> 00:02:48,400
discrete set of options

87
00:02:45,920 --> 00:02:50,239
upon seeing the tiger but you could also

88
00:02:48,400 --> 00:02:53,280
have a continuous action

89
00:02:50,239 --> 00:02:54,560
in which case pi theta outputs the

90
00:02:53,280 --> 00:02:55,440
parameters of some continuous

91
00:02:54,560 --> 00:02:57,440
distribution

92
00:02:55,440 --> 00:02:59,200
such as the mean and variance of a

93
00:02:57,440 --> 00:03:01,599
multivariate normal or gaussian

94
00:02:59,200 --> 00:03:04,480
distribution

95
00:03:01,599 --> 00:03:06,560
so to summarize the terminology ot

96
00:03:04,480 --> 00:03:09,920
represents the observation

97
00:03:06,560 --> 00:03:13,280
a t represents the action and then

98
00:03:09,920 --> 00:03:16,080
pi subscript theta a t given ot

99
00:03:13,280 --> 00:03:16,080
is the policy

100
00:03:16,319 --> 00:03:21,120
now another term that we'll see a lot in

101
00:03:19,680 --> 00:03:25,599
reinforcement learning

102
00:03:21,120 --> 00:03:27,519
is the state which we'll denote as st

103
00:03:25,599 --> 00:03:29,040
and sometimes we'll see the policy

104
00:03:27,519 --> 00:03:32,080
written as a t

105
00:03:29,040 --> 00:03:35,920
given st the difference between

106
00:03:32,080 --> 00:03:38,400
st and ot is that the state is typically

107
00:03:35,920 --> 00:03:39,680
assumed to be a markovian state which

108
00:03:38,400 --> 00:03:41,920
i'll explain shortly

109
00:03:39,680 --> 00:03:44,080
whereas ot is an observation that

110
00:03:41,920 --> 00:03:46,000
results from that state

111
00:03:44,080 --> 00:03:47,360
so most generally we would write a

112
00:03:46,000 --> 00:03:48,159
policy as being conditional and

113
00:03:47,360 --> 00:03:49,440
observation

114
00:03:48,159 --> 00:03:51,280
but sometimes we'll write it as being

115
00:03:49,440 --> 00:03:53,760
conditional and state and that is

116
00:03:51,280 --> 00:03:55,280
a more restrictive special case so let

117
00:03:53,760 --> 00:03:59,280
me explain the distinction

118
00:03:55,280 --> 00:03:59,280
between states and observations

119
00:03:59,360 --> 00:04:02,799
let's say that you observe this scene

120
00:04:01,519 --> 00:04:06,560
there is a cheetah

121
00:04:02,799 --> 00:04:08,400
chasing a gazelle now this observation

122
00:04:06,560 --> 00:04:09,920
consists of an image and the image is

123
00:04:08,400 --> 00:04:12,239
made of pixels

124
00:04:09,920 --> 00:04:13,599
those pixels might be sufficient to

125
00:04:12,239 --> 00:04:14,080
figure out where the cheetah and the

126
00:04:13,599 --> 00:04:17,120
gazelle

127
00:04:14,080 --> 00:04:20,000
are or they might not be but

128
00:04:17,120 --> 00:04:20,799
the image is produced by some underlying

129
00:04:20,000 --> 00:04:23,759
physics

130
00:04:20,799 --> 00:04:25,040
of some system and that system has a

131
00:04:23,759 --> 00:04:26,160
state it has a kind of a minimal

132
00:04:25,040 --> 00:04:28,960
representation

133
00:04:26,160 --> 00:04:29,840
so the image is the observation ot the

134
00:04:28,960 --> 00:04:32,639
state

135
00:04:29,840 --> 00:04:34,080
is the representation of the current

136
00:04:32,639 --> 00:04:36,160
configuration of the system

137
00:04:34,080 --> 00:04:37,600
which in this case might be for instance

138
00:04:36,160 --> 00:04:38,800
the position of the cheetah and the

139
00:04:37,600 --> 00:04:42,639
position of the gazelle

140
00:04:38,800 --> 00:04:45,440
and maybe their velocities

141
00:04:42,639 --> 00:04:46,160
now the observation might be altered in

142
00:04:45,440 --> 00:04:47,919
some way

143
00:04:46,160 --> 00:04:49,600
so the full state cannot be inferred

144
00:04:47,919 --> 00:04:51,520
exactly for instance if a car

145
00:04:49,600 --> 00:04:53,199
drives in front of the cheetah and you

146
00:04:51,520 --> 00:04:54,880
can't see it

147
00:04:53,199 --> 00:04:56,880
the observation might be insufficient to

148
00:04:54,880 --> 00:04:58,320
deduce the state

149
00:04:56,880 --> 00:05:00,160
but the state hasn't actually changed

150
00:04:58,320 --> 00:05:01,759
the cheat is still where it was before

151
00:05:00,160 --> 00:05:03,680
is just that now the image pixels and

152
00:05:01,759 --> 00:05:05,840
the observation are not enough

153
00:05:03,680 --> 00:05:07,199
to figure out where it is and that

154
00:05:05,840 --> 00:05:08,720
really

155
00:05:07,199 --> 00:05:10,560
gets at the difference between states

156
00:05:08,720 --> 00:05:12,639
and observations states

157
00:05:10,560 --> 00:05:14,000
are the true configuration of the system

158
00:05:12,639 --> 00:05:15,360
an observation

159
00:05:14,000 --> 00:05:17,840
is something that results from that

160
00:05:15,360 --> 00:05:21,680
state which may or may not be enough

161
00:05:17,840 --> 00:05:23,360
to deduce the state more formally

162
00:05:21,680 --> 00:05:24,960
we can explain the distinction between

163
00:05:23,360 --> 00:05:27,199
states and observations

164
00:05:24,960 --> 00:05:28,720
by using the terminology of graphical

165
00:05:27,199 --> 00:05:31,360
models

166
00:05:28,720 --> 00:05:33,039
so we can draw a graphical model that

167
00:05:31,360 --> 00:05:36,080
represents the relationship

168
00:05:33,039 --> 00:05:38,800
between states and actions and

169
00:05:36,080 --> 00:05:40,240
observations as i mentioned observations

170
00:05:38,800 --> 00:05:42,479
result from states

171
00:05:40,240 --> 00:05:44,000
so there's an arrow from s to o at every

172
00:05:42,479 --> 00:05:46,479
time step

173
00:05:44,000 --> 00:05:48,160
your policy uses the observations to

174
00:05:46,479 --> 00:05:49,680
choose the action so that's the arrow

175
00:05:48,160 --> 00:05:51,280
from o to a

176
00:05:49,680 --> 00:05:53,199
and the state in action at the current

177
00:05:51,280 --> 00:05:55,520
time step determines the state of the

178
00:05:53,199 --> 00:05:58,319
next time step so s1 and a1

179
00:05:55,520 --> 00:05:58,319
go to s2

180
00:05:58,639 --> 00:06:04,560
now from inspecting this graphical model

181
00:06:02,319 --> 00:06:05,759
we might conclude that there are certain

182
00:06:04,560 --> 00:06:09,280
independencies

183
00:06:05,759 --> 00:06:11,759
that are present in the system so

184
00:06:09,280 --> 00:06:13,840
this is the policy pi this is the

185
00:06:11,759 --> 00:06:14,639
transition probabilities p of s d plus

186
00:06:13,840 --> 00:06:18,479
one given s t

187
00:06:14,639 --> 00:06:21,840
a t and something we might note here

188
00:06:18,479 --> 00:06:21,840
is that

189
00:06:22,000 --> 00:06:28,960
p of s t plus 1 given s t 18

190
00:06:25,600 --> 00:06:30,080
is independent of s t minus 1. so for a

191
00:06:28,960 --> 00:06:32,639
state

192
00:06:30,080 --> 00:06:34,160
if you know the current state then you

193
00:06:32,639 --> 00:06:35,440
can figure out the distribution over the

194
00:06:34,160 --> 00:06:37,440
next state

195
00:06:35,440 --> 00:06:39,039
without any regard for the previous

196
00:06:37,440 --> 00:06:41,600
state

197
00:06:39,039 --> 00:06:43,520
that is to say the future is

198
00:06:41,600 --> 00:06:46,080
conditionally independent of the past

199
00:06:43,520 --> 00:06:47,840
given the present this is a very

200
00:06:46,080 --> 00:06:49,919
important independence property

201
00:06:47,840 --> 00:06:51,520
because it says that if you want to make

202
00:06:49,919 --> 00:06:54,560
a decision

203
00:06:51,520 --> 00:06:56,479
that will impact future states

204
00:06:54,560 --> 00:06:57,919
you do not have to consider how you

205
00:06:56,479 --> 00:06:59,280
reach the state you're currently in it's

206
00:06:57,919 --> 00:07:01,120
enough to just consider your current

207
00:06:59,280 --> 00:07:01,599
state and you can forget about previous

208
00:07:01,120 --> 00:07:04,960
states

209
00:07:01,599 --> 00:07:07,199
that led you to it this is called the

210
00:07:04,960 --> 00:07:08,880
markov property and the markov property

211
00:07:07,199 --> 00:07:10,479
is a very very important property in

212
00:07:08,880 --> 00:07:11,759
reinforcement learning and sequential

213
00:07:10,479 --> 00:07:13,759
decision making

214
00:07:11,759 --> 00:07:15,199
because without the markov property we

215
00:07:13,759 --> 00:07:16,880
would not be able to formulate

216
00:07:15,199 --> 00:07:19,520
optimal policies without considering

217
00:07:16,880 --> 00:07:22,240
entire histories

218
00:07:19,520 --> 00:07:24,160
however if our policy is conditioned on

219
00:07:22,240 --> 00:07:27,039
observations rather than states

220
00:07:24,160 --> 00:07:27,680
as it is in this picture we could ask

221
00:07:27,039 --> 00:07:30,160
well

222
00:07:27,680 --> 00:07:31,919
are the observations also conditionally

223
00:07:30,160 --> 00:07:34,160
independent in this way

224
00:07:31,919 --> 00:07:35,360
is the current observation entirely

225
00:07:34,160 --> 00:07:37,039
sufficient

226
00:07:35,360 --> 00:07:39,599
to figure out how to act so as to reach

227
00:07:37,039 --> 00:07:40,800
some state in the future

228
00:07:39,599 --> 00:07:42,319
take a moment to think about this

229
00:07:40,800 --> 00:07:45,840
question and consider writing your

230
00:07:42,319 --> 00:07:45,840
answer in the comments

231
00:07:47,039 --> 00:07:50,160
the trouble is that the observation is

232
00:07:48,639 --> 00:07:52,160
in general not

233
00:07:50,160 --> 00:07:54,000
going to satisfy the markov property

234
00:07:52,160 --> 00:07:55,520
meaning that the current observation

235
00:07:54,000 --> 00:07:57,440
might not be enough to fully determine

236
00:07:55,520 --> 00:07:58,319
the future without also observing the

237
00:07:57,440 --> 00:08:00,479
past

238
00:07:58,319 --> 00:08:02,319
and this is perhaps most obvious from

239
00:08:00,479 --> 00:08:04,479
the example with the cheetah

240
00:08:02,319 --> 00:08:05,759
when the car is in front of the cheetah

241
00:08:04,479 --> 00:08:07,120
and you cannot see where it is in the

242
00:08:05,759 --> 00:08:08,240
image

243
00:08:07,120 --> 00:08:09,759
you might not be able to figure out

244
00:08:08,240 --> 00:08:11,680
where it's going to go in the future

245
00:08:09,759 --> 00:08:14,319
because you can't see it right now

246
00:08:11,680 --> 00:08:16,000
but if in the previous point in time you

247
00:08:14,319 --> 00:08:17,280
could see if maybe the car was somewhere

248
00:08:16,000 --> 00:08:19,039
else before

249
00:08:17,280 --> 00:08:20,400
you could memorize where the cheetah was

250
00:08:19,039 --> 00:08:22,319
so that even when it's occluded by the

251
00:08:20,400 --> 00:08:23,520
car you still remember its state

252
00:08:22,319 --> 00:08:26,000
so in general if you're using

253
00:08:23,520 --> 00:08:27,919
observations past observations can

254
00:08:26,000 --> 00:08:28,960
actually give you additional information

255
00:08:27,919 --> 00:08:30,160
beyond what you would get from the

256
00:08:28,960 --> 00:08:32,399
current observation

257
00:08:30,160 --> 00:08:34,320
that would be useful for decision making

258
00:08:32,399 --> 00:08:36,080
whereas if you directly observe states

259
00:08:34,320 --> 00:08:37,519
then the current state is always going

260
00:08:36,080 --> 00:08:41,120
to give you everything you need

261
00:08:37,519 --> 00:08:42,479
because it satisfies the markov property

262
00:08:41,120 --> 00:08:43,839
now many reinforcement learning

263
00:08:42,479 --> 00:08:44,560
algorithms that we'll discuss in this

264
00:08:43,839 --> 00:08:47,680
course

265
00:08:44,560 --> 00:08:50,399
will actually require markovian

266
00:08:47,680 --> 00:08:51,760
states in which case i will write pi of

267
00:08:50,399 --> 00:08:53,920
a given s

268
00:08:51,760 --> 00:08:55,440
but in some cases i will also mention

269
00:08:53,920 --> 00:08:57,360
that a particular algorithm

270
00:08:55,440 --> 00:08:59,200
could be modified in some way to handle

271
00:08:57,360 --> 00:09:00,640
non-markovian observations

272
00:08:59,200 --> 00:09:02,800
and then i'll describe how that can be

273
00:09:00,640 --> 00:09:02,800
done

274
00:09:03,920 --> 00:09:08,720
now a little aside on notation in

275
00:09:06,800 --> 00:09:11,680
reinforcement learning we typically use

276
00:09:08,720 --> 00:09:13,120
s to denote state and a to denote action

277
00:09:11,680 --> 00:09:14,560
that's very reasonable because those are

278
00:09:13,120 --> 00:09:15,360
the first letters of those words in

279
00:09:14,560 --> 00:09:18,640
english

280
00:09:15,360 --> 00:09:20,240
this kind of terminology was

281
00:09:18,640 --> 00:09:22,320
widely popularized by the study of

282
00:09:20,240 --> 00:09:23,360
dynamic programming which in many ways

283
00:09:22,320 --> 00:09:25,279
was

284
00:09:23,360 --> 00:09:27,040
kind of pioneered by richard bellman in

285
00:09:25,279 --> 00:09:29,200
the 1950s

286
00:09:27,040 --> 00:09:31,440
if you have a background in robotics and

287
00:09:29,200 --> 00:09:32,800
optimal control and linear systems

288
00:09:31,440 --> 00:09:34,480
then you might be more familiar with a

289
00:09:32,800 --> 00:09:36,959
different notation where

290
00:09:34,480 --> 00:09:38,560
x is used to denote state and u is used

291
00:09:36,959 --> 00:09:41,680
to denote action

292
00:09:38,560 --> 00:09:43,200
this is exactly equivalent terminology x

293
00:09:41,680 --> 00:09:45,760
makes sense for state because

294
00:09:43,200 --> 00:09:47,760
that's usually the variable used for an

295
00:09:45,760 --> 00:09:50,000
unknown quantity in algebra

296
00:09:47,760 --> 00:09:52,240
and u is the first word for action in

297
00:09:50,000 --> 00:09:54,480
russian which is

298
00:09:52,240 --> 00:09:55,680
and uh this makes sense because this

299
00:09:54,480 --> 00:09:58,480
kind of terminology

300
00:09:55,680 --> 00:09:59,519
was actually popularized by folks like

301
00:09:58,480 --> 00:10:01,760
left panteragan

302
00:09:59,519 --> 00:10:05,279
who studied optimal control in the

303
00:10:01,760 --> 00:10:07,680
soviet union

304
00:10:05,279 --> 00:10:08,800
all right so that's a little bit of

305
00:10:07,680 --> 00:10:11,360
terminology

306
00:10:08,800 --> 00:10:12,240
but let's talk now about how we can

307
00:10:11,360 --> 00:10:14,560
actually learn

308
00:10:12,240 --> 00:10:16,079
policies and in today's lecture we'll

309
00:10:14,560 --> 00:10:17,440
actually start with a very simple way of

310
00:10:16,079 --> 00:10:19,120
learning policies

311
00:10:17,440 --> 00:10:20,640
that doesn't even require using very

312
00:10:19,120 --> 00:10:21,600
sophisticated reinforcement learning

313
00:10:20,640 --> 00:10:23,760
algorithms

314
00:10:21,600 --> 00:10:25,360
but instead learns policies in much the

315
00:10:23,760 --> 00:10:27,040
same way that we learn

316
00:10:25,360 --> 00:10:29,120
image classifiers and other kinds of

317
00:10:27,040 --> 00:10:32,240
models in supervised learning

318
00:10:29,120 --> 00:10:35,279
by utilizing data

319
00:10:32,240 --> 00:10:37,440
so let's go to a more realistic example

320
00:10:35,279 --> 00:10:39,440
running away from tigers is maybe

321
00:10:37,440 --> 00:10:41,040
not so important in our daily lives but

322
00:10:39,440 --> 00:10:41,760
how about another task the task of

323
00:10:41,040 --> 00:10:44,000
driving

324
00:10:41,760 --> 00:10:45,279
in driving your observations might

325
00:10:44,000 --> 00:10:48,000
consist of

326
00:10:45,279 --> 00:10:49,440
images from the cars camera and your

327
00:10:48,000 --> 00:10:51,120
action might consist of how you turn the

328
00:10:49,440 --> 00:10:53,600
steering wheel in order to keep the car

329
00:10:51,120 --> 00:10:53,600
on the road

330
00:10:54,079 --> 00:10:58,959
so let's uh kind of take an approach to

331
00:10:57,120 --> 00:11:00,720
driving similar to what we might do

332
00:10:58,959 --> 00:11:02,880
in uh you know for things like image

333
00:11:00,720 --> 00:11:04,880
classification computer vision

334
00:11:02,880 --> 00:11:06,800
and so on let's just take some label

335
00:11:04,880 --> 00:11:08,160
data and use that label data

336
00:11:06,800 --> 00:11:10,480
to learn a driving policy with

337
00:11:08,160 --> 00:11:13,279
supervised learning so we'll get an

338
00:11:10,480 --> 00:11:14,959
image from a person and their

339
00:11:13,279 --> 00:11:16,399
corresponding motor commands so this

340
00:11:14,959 --> 00:11:17,680
human driver will have

341
00:11:16,399 --> 00:11:19,120
turned the steering wheel in some way

342
00:11:17,680 --> 00:11:19,839
and will record what they saw from a

343
00:11:19,120 --> 00:11:22,240
camera

344
00:11:19,839 --> 00:11:24,160
and will record the steering command and

345
00:11:22,240 --> 00:11:27,040
we'll collect a large data set

346
00:11:24,160 --> 00:11:28,560
consisting of image and action tuples

347
00:11:27,040 --> 00:11:29,040
and then we'll simply use supervised

348
00:11:28,560 --> 00:11:31,120
learning

349
00:11:29,040 --> 00:11:32,640
to learn to map from observations to

350
00:11:31,120 --> 00:11:34,959
actions

351
00:11:32,640 --> 00:11:36,399
this is called imitation learning and

352
00:11:34,959 --> 00:11:37,760
this is a particular instance of

353
00:11:36,399 --> 00:11:38,480
invitational learning that is sometimes

354
00:11:37,760 --> 00:11:41,279
referred to

355
00:11:38,480 --> 00:11:42,880
as behavioral cloning it's called

356
00:11:41,279 --> 00:11:43,600
behavioral cloning because in a sense we

357
00:11:42,880 --> 00:11:46,720
are cloning

358
00:11:43,600 --> 00:11:48,320
the behavior of this human demonstrator

359
00:11:46,720 --> 00:11:50,320
the demonstrators also sometimes refer

360
00:11:48,320 --> 00:11:51,600
to as an expert because we assume that

361
00:11:50,320 --> 00:11:54,800
they are better at this task

362
00:11:51,600 --> 00:11:56,800
than the computer is all right

363
00:11:54,800 --> 00:11:58,079
so this is a very simple approach and we

364
00:11:56,800 --> 00:12:00,959
could ask the question well

365
00:11:58,079 --> 00:12:00,959
does it actually work

366
00:12:01,120 --> 00:12:06,880
um so this question has uh

367
00:12:05,120 --> 00:12:08,480
has been studied for a very long time in

368
00:12:06,880 --> 00:12:10,560
fact the original

369
00:12:08,480 --> 00:12:12,079
deep imitation imitation learning system

370
00:12:10,560 --> 00:12:13,360
or neural limitation learning system

371
00:12:12,079 --> 00:12:13,920
something that would be familiar to us

372
00:12:13,360 --> 00:12:17,200
today

373
00:12:13,920 --> 00:12:19,200
was proposed all the way back in 1989

374
00:12:17,200 --> 00:12:20,560
it was called alvin the autonomous land

375
00:12:19,200 --> 00:12:22,000
vehicle and neural network

376
00:12:20,560 --> 00:12:25,040
and alvin did some pretty interesting

377
00:12:22,000 --> 00:12:27,279
things the network by current standards

378
00:12:25,040 --> 00:12:29,200
was tiny had five hidden units

379
00:12:27,279 --> 00:12:31,600
but it could do you know interesting

380
00:12:29,200 --> 00:12:33,760
behaviors like staying on a road

381
00:12:31,600 --> 00:12:37,440
and there were some attempts to even try

382
00:12:33,760 --> 00:12:37,440
to drive it across america

383
00:12:37,519 --> 00:12:41,839
so we could ask does this uh basic

384
00:12:40,240 --> 00:12:43,839
principle work does this behavior

385
00:12:41,839 --> 00:12:46,399
cloning principle work

386
00:12:43,839 --> 00:12:48,000
in general the answer is no and to give

387
00:12:46,399 --> 00:12:49,600
you a little bit of intuition

388
00:12:48,000 --> 00:12:51,839
for why behavioral cloning might go

389
00:12:49,600 --> 00:12:53,279
wrong even while regular supervised

390
00:12:51,839 --> 00:12:55,519
learning would work