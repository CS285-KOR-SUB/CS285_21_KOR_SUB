1
00:00:01,120 --> 00:00:02,960
hey everyone my name is kevin i'm one of

2
00:00:02,960 --> 00:00:04,640
the gsis for this class and today i'll
hey everyone my name is kevin i'm one of

3
00:00:04,640 --> 00:00:06,000
be giving a lecture on pytorch and
the gsis for this class and today i'll

4
00:00:06,000 --> 00:00:08,400
neural networks
be giving a lecture on pytorch and

5
00:00:08,400 --> 00:00:09,840
so at a high level the goal of this
neural networks

6
00:00:09,840 --> 00:00:11,280
class is pretty much to train an agent
so at a high level the goal of this

7
00:00:11,280 --> 00:00:13,200
to perform useful tasks
class is pretty much to train an agent

8
00:00:13,200 --> 00:00:14,799
so this is what a typical training loop
to perform useful tasks

9
00:00:14,799 --> 00:00:16,800
might look like we start with some data
so this is what a typical training loop

10
00:00:16,800 --> 00:00:18,240
and we want to use that data to train a
might look like we start with some data

11
00:00:18,240 --> 00:00:19,680
model to determine how our agent is
and we want to use that data to train a

12
00:00:19,680 --> 00:00:20,800
going to act
model to determine how our agent is

13
00:00:20,800 --> 00:00:22,400
and using that model we'll collect some
going to act

14
00:00:22,400 --> 00:00:24,080
additional data and we'll keep repeating
and using that model we'll collect some

15
00:00:24,080 --> 00:00:25,599
that until we have something we're
additional data and we'll keep repeating

16
00:00:25,599 --> 00:00:27,439
satisfied with
that until we have something we're

17
00:00:27,439 --> 00:00:28,960
so that model might be something like a policy
satisfied with

18
00:00:28,960 --> 00:00:31,840
so that model might be something like a policy

19
00:00:32,559 --> 00:00:34,719
uh it could be a q function or

20
00:00:34,719 --> 00:00:36,239
it could even be a dynamics model if
uh it could be a q function or

21
00:00:36,239 --> 00:00:37,680
you're doing something like model based
it could even be a dynamics model if

22
00:00:37,680 --> 00:00:40,079
url
you're doing something like model based

23
00:00:40,719 --> 00:00:42,719
and so that's going to be the area that

24
00:00:42,719 --> 00:00:44,480
we focus on for today's lecture
and so that's going to be the area that

25
00:00:44,480 --> 00:00:46,399
the rest of the loop is not really
we focus on for today's lecture

26
00:00:46,399 --> 00:00:48,079
related to pytorch but you'll get some
the rest of the loop is not really

27
00:00:48,079 --> 00:00:51,520
experience with that in homework one two
related to pytorch but you'll get some

28
00:00:52,480 --> 00:00:54,559
so how do we train a model

29
00:00:54,559 --> 00:00:55,840
typically you'll get some kind of
so how do we train a model

30
00:00:55,840 --> 00:00:58,000
optimization problem like this so we
typically you'll get some kind of

31
00:00:58,000 --> 00:00:59,680
have some kind of loss function and we
optimization problem like this so we

32
00:00:59,680 --> 00:01:01,359
want to minimize the loss on our
have some kind of loss function and we

33
00:01:01,359 --> 00:01:03,359
training data set
want to minimize the loss on our

34
00:01:03,359 --> 00:01:04,559
so there are a lot of different pieces
training data set

35
00:01:04,559 --> 00:01:06,560
here we have a data set
so there are a lot of different pieces

36
00:01:06,560 --> 00:01:08,400
we have some kind of
here we have a data set

37
00:01:08,400 --> 00:01:10,159
model probably a neural network that is
we have some kind of

38
00:01:10,159 --> 00:01:11,760
going to be making predictions
model probably a neural network that is

39
00:01:11,760 --> 00:01:13,439
we have a loss function that defines how
going to be making predictions

40
00:01:13,439 --> 00:01:15,360
well we're performing so far
we have a loss function that defines how

41
00:01:15,360 --> 00:01:17,759
and then we have some kind of
well we're performing so far

42
00:01:17,759 --> 00:01:19,360
optimization algorithm that we use to
and then we have some kind of

43
00:01:19,360 --> 00:01:21,040
actually minimize the loss probably
optimization algorithm that we use to

44
00:01:21,040 --> 00:01:23,119
gradient descent
actually minimize the loss probably

45
00:01:23,119 --> 00:01:25,520
and pytorch helps us do all these
gradient descent

46
00:01:25,520 --> 00:01:27,840
things pretty much
and pytorch helps us do all these

47
00:01:27,840 --> 00:01:30,240
so at a high level what pytorch does is
things pretty much

48
00:01:30,240 --> 00:01:32,240
it's a python library that lets you
so at a high level what pytorch does is

49
00:01:32,240 --> 00:01:34,240
define neural networks and automatically
it's a python library that lets you

50
00:01:34,240 --> 00:01:37,360
compute gradients for gradient descent
define neural networks and automatically

51
00:01:37,360 --> 00:01:39,119
it definitely also lets you do a lot of
compute gradients for gradient descent

52
00:01:39,119 --> 00:01:41,119
other things like for example work with
it definitely also lets you do a lot of

53
00:01:41,119 --> 00:01:42,640
data sets switch out different
other things like for example work with

54
00:01:42,640 --> 00:01:44,560
optimizers train and do inference on
data sets switch out different

55
00:01:44,560 --> 00:01:46,479
gpus and stuff like that and we'll try
optimizers train and do inference on

56
00:01:46,479 --> 00:01:48,079
to cover some of these things in the
gpus and stuff like that and we'll try

57
00:01:48,079 --> 00:01:50,000
next few videos where we go through a
to cover some of these things in the

58
00:01:50,000 --> 00:01:54,280
collab with some examples
next few videos where we go through a

59
00:01:55,200 --> 00:01:57,600
so how does pytorch work um if you ever

60
00:01:57,600 --> 00:02:00,079
used um any other deep learning library
so how does pytorch work um if you ever

61
00:02:00,079 --> 00:02:02,079
for example tensorflow probably the big
used um any other deep learning library

62
00:02:02,079 --> 00:02:04,719
difference the biggest difference is um
for example tensorflow probably the big

63
00:02:04,719 --> 00:02:06,880
how pytorch computes gradients for
difference the biggest difference is um

64
00:02:06,880 --> 00:02:08,800
backprop(agaiton) everything else is pretty much
how pytorch computes gradients for

65
00:02:08,800 --> 00:02:10,720
just a matter of knowing the names of
backprop(agaiton) everything else is pretty much

66
00:02:10,720 --> 00:02:12,640
the different classes and functions if
just a matter of knowing the names of

67
00:02:12,640 --> 00:02:14,319
you're transitioning to pytorch from
the different classes and functions if

68
00:02:14,319 --> 00:02:16,239
something else
you're transitioning to pytorch from

69
00:02:16,239 --> 00:02:18,800
but for backprop
something else

70
00:02:18,800 --> 00:02:20,959
what pytorch does is basically
but for backprop

71
00:02:20,959 --> 00:02:22,879
let you define the forward pass the way
what pytorch does is basically

72
00:02:22,879 --> 00:02:24,800
you'd normally write code
let you define the forward pass the way

73
00:02:24,800 --> 00:02:26,160
and you pretty much don't have to worry
you'd normally write code

74
00:02:26,160 --> 00:02:28,160
about defining the backward pass at all
and you pretty much don't have to worry

75
00:02:28,160 --> 00:02:30,080
or any any kind of like computation graph
about defining the backward pass at all

76
00:02:30,080 --> 00:02:31,360
or any any kind of like computation graph

77
00:02:31,360 --> 00:02:33,440
so this example uh what we do is we
or any any kind of like computation graph

78
00:02:33,440 --> 00:02:36,480
define we're going to find three layers
so this example uh what we do is we

79
00:02:36,480 --> 00:02:38,239
each layer is going to be a linear
define we're going to find three layers

80
00:02:38,239 --> 00:02:41,120
function followed by some non-linearity
each layer is going to be a linear

81
00:02:41,120 --> 00:02:43,519
so we have h1 h2 and then we have our
function followed by some non-linearity

82
00:02:43,519 --> 00:02:45,920
output y
so we have h1 h2 and then we have our

83
00:02:45,920 --> 00:02:47,360
so at this point that's pretty much all
output y

84
00:02:47,360 --> 00:02:48,959
you need to do
so at this point that's pretty much all

85
00:02:48,959 --> 00:02:51,360
then if you want to do backprop you
you need to do

86
00:02:51,360 --> 00:02:52,959
basically choose some variable in this
then if you want to do backprop you

87
00:02:52,959 --> 00:02:54,959
case we'll choose the scalar y and we
basically choose some variable in this

88
00:02:54,959 --> 00:02:56,239
want to take all the gradients with
case we'll choose the scalar y and we

89
00:02:56,239 --> 00:02:58,239
respect to y so we just call y dot
want to take all the gradients with

90
00:02:58,239 --> 00:02:59,680
backward
respect to y so we just call y dot

91
00:02:59,680 --> 00:03:01,440
and what that's going to do is
backward

92
00:03:01,440 --> 00:03:02,879
pytorch is automatically going to
and what that's going to do is

93
00:03:02,879 --> 00:03:05,280
compute the gradients of y with respect
pytorch is automatically going to

94
00:03:05,280 --> 00:03:07,599
to each of your variables so y with
compute the gradients of y with respect

95
00:03:07,599 --> 00:03:11,920
respect to w3 w2 and w1
to each of your variables so y with

96
00:03:11,920 --> 00:03:13,680
and these gradients will be
respect to w3 w2 and w1

97
00:03:13,680 --> 00:03:15,440
automatically populated for your three
and these gradients will be

98
00:03:15,440 --> 00:03:17,840
variables
automatically populated for your three

99
00:03:17,840 --> 00:03:19,680
yeah this is a super nice feature of
variables

100
00:03:19,680 --> 00:03:22,879
pytorch basically you can write your code
yeah this is a super nice feature of

101
00:03:22,879 --> 00:03:24,799
naturally as if you're just defining
pytorch basically you can write your code

102
00:03:24,799 --> 00:03:26,560
functions that do some kind of
naturally as if you're just defining

103
00:03:26,560 --> 00:03:28,239
transformations on your input and you
functions that do some kind of

104
00:03:28,239 --> 00:03:29,360
don't have to worry about how the
transformations on your input and you

105
00:03:29,360 --> 00:03:31,280
backward pass is going to be computed
don't have to worry about how the

106
00:03:31,280 --> 00:03:34,879
that's all taken care of for you
backward pass is going to be computed

107
00:03:36,480 --> 00:03:37,920
so in the next few videos we'll be going

108
00:03:37,920 --> 00:03:39,760
over an interactive collab and we'll go
so in the next few videos we'll be going

109
00:03:39,760 --> 00:03:41,599
over some examples of how you can use
over an interactive collab and we'll go

110
00:03:41,599 --> 00:03:45,640
pytorch to train neural networks
over some examples of how you can use

