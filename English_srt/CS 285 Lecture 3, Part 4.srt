1
00:00:00,719 --> 00:00:01,920
all right now let's go over a full

2
00:00:01,920 --> 00:00:03,600
example to see the whole process of

3
00:00:03,600 --> 00:00:05,680
defining a neural network training it

4
00:00:05,680 --> 00:00:07,759
and then using it to make predictions

5
00:00:07,759 --> 00:00:10,000
so the task we'll have is to use a

6
00:00:10,000 --> 00:00:12,160
neural network to model a sine wave

7
00:00:12,160 --> 00:00:16,040
so this is our target

8
00:00:16,160 --> 00:00:19,119
this is the function we're trying to learn and

9
00:00:19,119 --> 00:00:21,680
we'll start by defining the architecture

10
00:00:21,680 --> 00:00:23,680
of our neural network the way we'll do

11
00:00:23,680 --> 00:00:25,680
that is using something called modules

12
00:00:25,680 --> 00:00:28,840
so the base class

13
00:00:28,840 --> 00:00:31,519
for pretty much all the layers of neural

14
00:00:31,519 --> 00:00:33,360
networks in PyTorch is something called

15
00:00:33,360 --> 00:00:36,160
an nn.Module and this module basically

16
00:00:36,160 --> 00:00:38,239
represents the building block of the

17
00:00:38,239 --> 00:00:40,000
full computation graph

18
00:00:40,000 --> 00:00:41,360
so typically

19
00:00:41,360 --> 00:00:43,280
your neural network might have things

20
00:00:43,280 --> 00:00:46,320
like convolution layers fully connected layers

21
00:00:46,320 --> 00:00:50,160
pooling things like that and each of those

22
00:00:50,160 --> 00:00:52,960
is a subclass of nn.Module and those

23
00:00:52,960 --> 00:00:57,039
three actually have already been defined for you in PyTorch

24
00:00:57,039 --> 00:00:58,960
so when you want to define your neural

25
00:00:58,960 --> 00:01:00,960
network you would just piece together a

26
00:01:00,960 --> 00:01:04,239
bunch of these modules into your own module

27
00:01:04,239 --> 00:01:07,280
for the overall network

28
00:01:07,680 --> 00:01:10,799
so as an example let's take this model

29
00:01:10,799 --> 00:01:11,760
which we

30
00:01:11,760 --> 00:01:14,159
sort of saw in the first video

31
00:01:14,159 --> 00:01:16,320
so we have three operations the first

32
00:01:16,320 --> 00:01:18,799
two are linear followed by non-linearity

33
00:01:18,799 --> 00:01:20,159
and the last one is just going to be a

34
00:01:20,159 --> 00:01:22,240
linear transformation to get a scalar

35
00:01:22,240 --> 00:01:23,600
output value

36
00:01:23,600 --> 00:01:26,400
so this is the architecture we're going to use

37
00:01:26,400 --> 00:01:29,280
for this problem

38
00:01:30,799 --> 00:01:33,840
so in order to define this architecture

39
00:01:33,840 --> 00:01:36,400
we're going to define a class in python

40
00:01:36,400 --> 00:01:37,680
you can call whatever you want we'll

41
00:01:37,680 --> 00:01:39,200
call it that in this case but the

42
00:01:39,200 --> 00:01:40,560
important thing is that we want it to be

43
00:01:40,560 --> 00:01:44,560
a subclass of an end module

44
00:01:44,799 --> 00:01:48,159
so what you're going to do is you

45
00:01:48,159 --> 00:01:51,040
typically need two things within your class

46
00:01:51,040 --> 00:01:53,439
in the constructor you're going to define

47
00:01:53,439 --> 00:01:56,079
all the layers of your model and in this

48
00:01:56,079 --> 00:01:57,360
forward function you're going to define

49
00:01:57,360 --> 00:01:59,680
what it does on an input

50
00:01:59,680 --> 00:02:04,079
and return some output as the prediction

51
00:02:04,079 --> 00:02:06,000
so in our constructor

52
00:02:06,000 --> 00:02:09,038
we have a three layers we need to define

53
00:02:09,038 --> 00:02:12,160
uh one two and three over here so we're

54
00:02:12,160 --> 00:02:15,040
gonna define those as fc1 through fc3

55
00:02:15,040 --> 00:02:17,040
and each of those is going to be using a

56
00:02:17,040 --> 00:02:20,640
built-in nn.linear class which is a

57
00:02:20,640 --> 00:02:22,640
subclass of nn.Module and we can

58
00:02:22,640 --> 00:02:24,560
specify what we want the inputs and

59
00:02:24,560 --> 00:02:27,440
outputs to look like

60
00:02:28,239 --> 00:02:31,760
then for the forward pass we're going to define

61
00:02:31,760 --> 00:02:34,239
what PyTorch what this model actually

62
00:02:34,239 --> 00:02:37,280
does to make a prediction so here we're

63
00:02:37,280 --> 00:02:39,120
going to send it through a linear layer

64
00:02:39,120 --> 00:02:41,519
then a related non-linearity

65
00:02:41,519 --> 00:02:43,280
do that again a second time for the

66
00:02:43,280 --> 00:02:44,959
second hidden layer and then for the

67
00:02:44,959 --> 00:02:46,959
last layer we're just going to do

68
00:02:46,959 --> 00:02:51,840
a linear transformation and then return the output

69
00:02:54,480 --> 00:02:57,280
so once this has been defined you can

70
00:02:57,280 --> 00:03:00,879
initialize instances of your network

71
00:03:00,879 --> 00:03:03,440
just like you would with any python class

72
00:03:03,440 --> 00:03:05,120
and when you print it out

73
00:03:05,120 --> 00:03:07,920
you can see this nice representation of

74
00:03:07,920 --> 00:03:09,280
what your neural network looks like and

75
00:03:09,280 --> 00:03:10,640
you can see all the layers you can see

76
00:03:10,640 --> 00:03:13,519
their names and you can see

77
00:03:13,519 --> 00:03:15,120
what their

78
00:03:15,120 --> 00:03:19,560
what their input and output shapes are

79
00:03:20,319 --> 00:03:22,400
so when you want to actually evaluate

80
00:03:22,400 --> 00:03:24,480
this neural net on some input

81
00:03:24,480 --> 00:03:26,720
the way you should do it is by

82
00:03:26,720 --> 00:03:28,319
calling it directly and passing the

83
00:03:28,319 --> 00:03:30,959
input so we initialized a variable net

84
00:03:30,959 --> 00:03:33,680
we're going to call it directly and pass an x

85
00:03:33,680 --> 00:03:36,560
you want to be really careful not to

86
00:03:36,560 --> 00:03:38,959
call net dot forward x

87
00:03:38,959 --> 00:03:41,519
like we're not just going to be calling

88
00:03:41,519 --> 00:03:44,080
the forward pass of uh this class that

89
00:03:44,080 --> 00:03:46,879
we did earlier

90
00:03:48,799 --> 00:03:51,200
and the reason is because what calling

91
00:03:51,200 --> 00:03:53,280
net directly does is uh there's some

92
00:03:53,280 --> 00:03:54,720
like initial setup code that's taken

93
00:03:54,720 --> 00:03:56,480
care of for you and then after that it

94
00:03:56,480 --> 00:04:00,000
it like actually calls net.forward

95
00:04:00,000 --> 00:04:02,879
so you don't want to ever call forward directly

96
00:04:02,879 --> 00:04:08,239
you want to call the module and pass into input

97
00:04:08,959 --> 00:04:11,599
and so once we do this we can see that the

98
00:04:11,599 --> 00:04:15,120
output is just a tensor that has

99
00:04:15,120 --> 00:04:17,918
for the first dimension the same the

100
00:04:17,918 --> 00:04:19,358
same length because this is the batch

101
00:04:19,358 --> 00:04:21,680
dimension we passed in 100

102
00:04:21,680 --> 00:04:24,240
inputs and we get 100 outputs

103
00:04:24,240 --> 00:04:26,000
and the second is just the output size

104
00:04:26,000 --> 00:04:29,840
that we defined earlier in our neural network

105
00:04:31,440 --> 00:04:33,280
so now we can visualize what this

106
00:04:33,280 --> 00:04:34,960
network looks like before training we

107
00:04:34,960 --> 00:04:36,880
have our inputs we have our outputs

108
00:04:36,880 --> 00:04:39,520
and so this is like what our randomly

109
00:04:39,520 --> 00:04:44,320
initialized model is predicting for these values

110
00:04:44,639 --> 00:04:49,360
so if we dive a bit deeper into like what's happening

111
00:04:49,360 --> 00:04:52,560
within this nn.Module

112
00:04:52,560 --> 00:04:54,720
each time you define an instance

113
00:04:54,720 --> 00:04:58,720
variable that's some kind of module

114
00:04:58,720 --> 00:05:02,160
and when this is done within the nn.Module class

115
00:05:02,160 --> 00:05:04,240
what it does is it

116
00:05:04,240 --> 00:05:07,360
it considers it a parameter of your module

117
00:05:07,360 --> 00:05:08,960
and the reason that's significant is

118
00:05:08,960 --> 00:05:10,720
because these parameters will

119
00:05:10,720 --> 00:05:12,160
automatically have their gradients

120
00:05:12,160 --> 00:05:16,080
tracked so requires grad equals true

121
00:05:17,840 --> 00:05:20,320
so if you print these out

122
00:05:20,320 --> 00:05:21,520
you can use

123
00:05:21,520 --> 00:05:23,520
this function called named parameters or

124
00:05:23,520 --> 00:05:24,880
parameters if you just care about the

125
00:05:24,880 --> 00:05:30,240
values and not names and you can see

126
00:05:30,240 --> 00:05:32,720
what layers or what what tensors make up

127
00:05:32,720 --> 00:05:35,039
this model so we had three fully

128
00:05:35,039 --> 00:05:36,880
connected layers each of those fully

129
00:05:36,880 --> 00:05:39,520
connected layers has two temperatures to

130
00:05:39,520 --> 00:05:41,360
represent the operation one for the

131
00:05:41,360 --> 00:05:44,639
weight and one for the bias

132
00:05:46,320 --> 00:05:47,919
and each of these parameters both the

133
00:05:47,919 --> 00:05:49,199
weights and the biases will

134
00:05:49,199 --> 00:05:51,680
automatically have their gradients stored

135
00:05:51,680 --> 00:05:54,080
like we said earlier because they were

136
00:05:54,080 --> 00:05:57,280
defined inside of the nn.Module subclass

137
00:05:57,280 --> 00:05:59,199
so if you print something like this out

138
00:05:59,199 --> 00:06:01,280
you'll see that it has a grad but in

139
00:06:01,280 --> 00:06:02,880
this case it's still none because we

140
00:06:02,880 --> 00:06:06,840
haven't done backprop yet

141
00:06:07,280 --> 00:06:10,960
one common mistake is if you're ever

142
00:06:10,960 --> 00:06:13,199
trying to define a list of modules for

143
00:06:13,199 --> 00:06:15,360
example if you want um to define a

144
00:06:15,360 --> 00:06:17,520
neural network with an arbitrary number

145
00:06:17,520 --> 00:06:19,520
of layers and you maybe use a for loop

146
00:06:19,520 --> 00:06:22,880
to define your individual layers

147
00:06:22,880 --> 00:06:25,039
you don't ever want to just like add

148
00:06:25,039 --> 00:06:27,440
them to a regular python list like this

149
00:06:27,440 --> 00:06:29,280
because if you do that PyTorch won't

150
00:06:29,280 --> 00:06:32,319
recognize that these are actually sub modules

151
00:06:32,319 --> 00:06:34,720
and they won't be considered um actual

152
00:06:34,720 --> 00:06:36,240
parameter module and their gradients

153
00:06:36,240 --> 00:06:37,520
won't be tracked

154
00:06:37,520 --> 00:06:40,240
um so instead the way to do that is by

155
00:06:40,240 --> 00:06:42,880
using this class called nn.ModuleList

156
00:06:42,880 --> 00:06:46,160
and passing in the list of

157
00:06:46,160 --> 00:06:48,800
layers that you actually want to use and

158
00:06:48,800 --> 00:06:50,240
if you do it this way then network one

159
00:06:50,240 --> 00:06:54,479
and network two will both have the gradients tracked properly

160
00:06:58,080 --> 00:07:01,520
so when you make that prediction so earlier

161
00:07:01,520 --> 00:07:05,840
we did

162
00:07:07,599 --> 00:07:11,919
y = net(x) to make a prediction so

163
00:07:11,919 --> 00:07:14,960
when you do that the output is just a tensor

164
00:07:14,960 --> 00:07:16,479
and you can perform operation on the

165
00:07:16,479 --> 00:07:18,880
tensor just like we did before

166
00:07:18,880 --> 00:07:21,039
in all the all the previous videos

167
00:07:21,039 --> 00:07:23,520
and it'll automatically

168
00:07:23,520 --> 00:07:24,880
track the computations and let you

169
00:07:24,880 --> 00:07:26,400
compute derivatives

170
00:07:26,400 --> 00:07:28,319
so here um

171
00:07:28,319 --> 00:07:29,919
for example we're using let's use a

172
00:07:29,919 --> 00:07:33,039
squared loss on our predictions and the target

173
00:07:33,039 --> 00:07:35,280
um and when we call lost.backward()

174
00:07:35,280 --> 00:07:37,039
that's going to automatically run back

175
00:07:37,039 --> 00:07:40,400
prop for us and populate the gradients of

176
00:07:40,400 --> 00:07:43,599
each of the weights of our model

177
00:07:43,759 --> 00:07:48,080
so we can now go into the p.grad

178
00:07:48,080 --> 00:07:49,599
property for each of the parameters of

179
00:07:49,599 --> 00:07:51,440
our model and we can

180
00:07:51,440 --> 00:07:53,199
manually implement gradient descent by

181
00:07:53,199 --> 00:07:55,280
basically like taking a step in the

182
00:07:55,280 --> 00:07:59,080
opposite direction of the gradient

183
00:07:59,520 --> 00:08:01,919
and remember we talked about this before

184
00:08:01,919 --> 00:08:03,759
if you call backward multiple times it's

185
00:08:03,759 --> 00:08:05,039
actually going to keep accumulating the

186
00:08:05,039 --> 00:08:06,319
gradient so if you're going to do

187
00:08:06,319 --> 00:08:07,680
gradient descent

188
00:08:07,680 --> 00:08:08,960
after you do this step you have to

189
00:08:08,960 --> 00:08:11,039
always make sure to zero the gradients

190
00:08:11,039 --> 00:08:14,960
so that next time you start with a fresh copy

191
00:08:16,240 --> 00:08:18,160
and so at this point we pretty much have

192
00:08:18,160 --> 00:08:19,759
all the pieces we need to train our

193
00:08:19,759 --> 00:08:22,240
network we can just use a for loop and

194
00:08:22,240 --> 00:08:25,199
do this repeatedly we compute the loss

195
00:08:25,199 --> 00:08:27,280
call backward to do a backprop and

196
00:08:27,280 --> 00:08:29,599
populate the gradients and then

197
00:08:29,599 --> 00:08:31,360
adjust each of the parameters using the

198
00:08:31,360 --> 00:08:33,440
gradients that were filled in

199
00:08:33,440 --> 00:08:36,479
and then we zero the gradients and start over again

200
00:08:36,479 --> 00:08:41,679
so with that we can get a network that

201
00:08:41,679 --> 00:08:43,039
models something pretty close to our

202
00:08:43,039 --> 00:08:47,040
sine wave so it definitely is learning

203
00:08:47,040 --> 00:08:52,320
but this implementation is kind of

204
00:08:52,320 --> 00:08:54,210
annoying to have to write out every single time

205
00:08:54,210 --> 00:08:57,519
so PyTorch has some

206
00:08:57,519 --> 00:08:59,279
helper functions basically that take

207
00:08:59,279 --> 00:09:01,440
care of certain things for you and so

208
00:09:01,440 --> 00:09:03,279
we'll slowly add those things in and see

209
00:09:03,279 --> 00:09:07,120
how that simplifies our training process

210
00:09:07,279 --> 00:09:09,519
the first thing is loss functions so

211
00:09:09,519 --> 00:09:11,440
PyTorch has a bunch of built-in loss

212
00:09:11,440 --> 00:09:13,760
functions and again these loss functions

213
00:09:13,760 --> 00:09:16,480
are modules just like any other layer

214
00:09:16,480 --> 00:09:18,320
that you had in your model

215
00:09:18,320 --> 00:09:20,000
and you can pass your data through them

216
00:09:20,000 --> 00:09:22,880
so let's say instead of manually

217
00:09:22,880 --> 00:09:24,880
implementing a loss function we

218
00:09:24,880 --> 00:09:29,200
define our loss function to be the mean squared loss

219
00:09:29,440 --> 00:09:31,839
and we can pass in

220
00:09:31,839 --> 00:09:33,839
like the outputs and our targets and we

221
00:09:33,839 --> 00:09:38,080
get some tensor as our output prediction

222
00:09:38,880 --> 00:09:40,959
so now we have the same thing except

223
00:09:40,959 --> 00:09:42,560
we've just replaced

224
00:09:42,560 --> 00:09:45,200
the lost with loss function and you can

225
00:09:45,200 --> 00:09:46,959
see that it will pretty much end up

226
00:09:46,959 --> 00:09:49,360
doing the same thing

227
00:09:49,360 --> 00:09:53,440
so it's not really a big deal here because

228
00:09:53,440 --> 00:09:55,200
our squared loss was pretty easy to

229
00:09:55,200 --> 00:09:57,279
implement but if you ever want to like

230
00:09:57,279 --> 00:09:58,880
experiment with more complicated loss

231
00:09:58,880 --> 00:10:00,800
functions or make it really easy to just

232
00:10:00,800 --> 00:10:02,800
like plug in arbitrary loss functions to

233
00:10:02,800 --> 00:10:04,560
see what works better

234
00:10:04,560 --> 00:10:08,720
this is definitely a good practice to follow

235
00:10:10,560 --> 00:10:14,079
the next thing is optimizers so we'll also use

236
00:10:14,079 --> 00:10:16,560
the optim package within PyTorch

237
00:10:16,560 --> 00:10:18,320
to use some of the built-in optimizers

238
00:10:18,320 --> 00:10:21,279
they have instead of having to manually

239
00:10:21,279 --> 00:10:24,880
implement gradient descent like we did before

240
00:10:24,880 --> 00:10:27,519
so what we'll do is we'll start over

241
00:10:27,519 --> 00:10:29,839
redefine our neural network

242
00:10:29,839 --> 00:10:33,120
and here we have an optimizer and here

243
00:10:33,120 --> 00:10:35,120
you can pretty much use any

244
00:10:35,120 --> 00:10:36,399
optimization function you want you can

245
00:10:36,399 --> 00:10:39,040
use SGD which would most closely match

246
00:10:39,040 --> 00:10:40,880
what we were doing before

247
00:10:40,880 --> 00:10:43,360
you can use RMSprop

248
00:10:43,360 --> 00:10:45,680
or in this case let's just stick with Adam

249
00:10:45,680 --> 00:10:47,040
so it's easy to like plug in different

250
00:10:47,040 --> 00:10:51,040
optimizers you might want and

251
00:10:51,040 --> 00:10:54,480
here uh okay so uh let's make some

252
00:10:54,480 --> 00:10:56,000
predictions and then see what the

253
00:10:56,000 --> 00:10:58,240
network looks like before training again

254
00:10:58,240 --> 00:11:00,720
so again we've reinitialized it so we

255
00:11:00,720 --> 00:11:04,240
should have some random values

256
00:11:04,240 --> 00:11:06,880
and now instead of

257
00:11:06,880 --> 00:11:08,560
manually implementing gradient descent

258
00:11:08,560 --> 00:11:11,600
we'll replace it with the optimizer so

259
00:11:11,600 --> 00:11:13,839
the way that it works is you compute

260
00:11:13,839 --> 00:11:16,560
your loss as usual

261
00:11:16,560 --> 00:11:19,839
but you'll do all optimizations through

262
00:11:19,839 --> 00:11:22,399
the optimizer so first you'll call

263
00:11:22,399 --> 00:11:25,360
optimizer.zero_grad()

264
00:11:25,360 --> 00:11:27,519
which zeroes out all the gradients

265
00:11:27,519 --> 00:11:29,680
then you do lost backward to populate

266
00:11:29,680 --> 00:11:32,079
the gradients of each of your parameters

267
00:11:32,079 --> 00:11:34,560
and then finally you do optimizer.step()

268
00:11:34,560 --> 00:11:38,399
so optimizer.step() is kind of equivalent to

269
00:11:38,399 --> 00:11:40,320
this code that we wrote which basically

270
00:11:40,320 --> 00:11:42,000
updates each of the parameters using a

271
00:11:42,000 --> 00:11:44,640
step of gradient descent

272
00:11:44,640 --> 00:11:51,680
and this optimizer.zero_grad() line is

273
00:11:51,680 --> 00:11:53,519
basically the same thing as

274
00:11:53,519 --> 00:11:55,200
this part of the code where we make sure

275
00:11:55,200 --> 00:11:56,800
to zero the gradients

276
00:11:56,800 --> 00:12:00,240
each time so it doesn't accumulate

277
00:12:01,440 --> 00:12:05,279
so just one thing to be careful of when you

278
00:12:05,279 --> 00:12:07,600
initialize the optimizer you want to

279
00:12:07,600 --> 00:12:10,720
make sure to pass in all the parameters of

280
00:12:10,720 --> 00:12:12,079
whatever neural network or neural

281
00:12:12,079 --> 00:12:14,240
networks you're trying to train

282
00:12:14,240 --> 00:12:15,680
because this is the only way that the

283
00:12:15,680 --> 00:12:18,639
optimizer actually becomes aware of

284
00:12:18,639 --> 00:12:20,639
what it's doing gradient descent on so

285
00:12:20,639 --> 00:12:22,959
when you call like optimizer.zero_grad()

286
00:12:22,959 --> 00:12:25,440
and optimizer.step it'll only actually

287
00:12:25,440 --> 00:12:27,519
update the gradients of the parameters

288
00:12:27,519 --> 00:12:29,040
that you pass in here

289
00:12:29,040 --> 00:12:30,399
um so if you have a single neural

290
00:12:30,399 --> 00:12:31,920
network like we do here it's really easy

291
00:12:31,920 --> 00:12:34,160
you just pass in net.parameters()

292
00:12:34,160 --> 00:12:35,600
but if you ever start working with like

293
00:12:35,600 --> 00:12:38,000
multiple models that are kind of

294
00:12:38,000 --> 00:12:39,760
training simultaneously

295
00:12:39,760 --> 00:12:41,519
you just want to be careful to make sure

296
00:12:41,519 --> 00:12:43,519
that you've actually given the optimizer

297
00:12:43,519 --> 00:12:46,240
all the parameters

298
00:12:48,560 --> 00:12:52,480
okay so now let's run this code

299
00:12:52,480 --> 00:12:54,399
and we can see that it did exactly the

300
00:12:54,399 --> 00:12:56,399
same thing that we had before except now

301
00:12:56,399 --> 00:12:59,680
our code is much more concise

302
00:12:59,920 --> 00:13:02,000
so here's a summary of what we did

303
00:13:02,000 --> 00:13:04,000
feel free to pause the video and

304
00:13:04,000 --> 00:13:05,519
or go back into the colab and just make

305
00:13:05,519 --> 00:13:07,839
sure that you understand

306
00:13:07,839 --> 00:13:09,279
what this whole

307
00:13:09,279 --> 00:13:12,000
training process looks like

308
00:13:12,000 --> 00:13:15,600
next we'll talk about gpu support in PyTorch so most of the time you'll

309
00:13:15,600 --> 00:13:18,000
probably want to do your training on a gpu

310
00:13:18,000 --> 00:13:20,480
because gpus are optimized for matrix

311
00:13:20,480 --> 00:13:22,079
operations and it'll make your training

312
00:13:22,079 --> 00:13:24,079
run much faster

313
00:13:24,079 --> 00:13:25,519
PyTorch makes it pretty easy to move

314
00:13:25,519 --> 00:13:28,000
your tensors between cpu and gpu to

315
00:13:28,000 --> 00:13:30,480
actually do that training process

316
00:13:30,480 --> 00:13:31,760
so we'll see an example of what that

317
00:13:31,760 --> 00:13:33,519
looks like here

318
00:13:33,519 --> 00:13:36,480
first you'll want to check to make sure

319
00:13:36,480 --> 00:13:39,360
you have a gpu available on your current machine

320
00:13:39,360 --> 00:13:43,199
so a lot of you might end up running your

321
00:13:43,199 --> 00:13:45,199
homeworks on colab colab is nice

322
00:13:45,199 --> 00:13:48,399
because it gives you free access to gpus

323
00:13:48,399 --> 00:13:49,680
if you're going to use colab just make

324
00:13:49,680 --> 00:13:52,160
sure that you go into runtime

325
00:13:52,160 --> 00:13:54,000
change runtime type and just make sure

326
00:13:54,000 --> 00:13:55,760
that you have gpu selected otherwise

327
00:13:55,760 --> 00:13:59,440
it's not actually going to be training on gpu

328
00:13:59,600 --> 00:14:03,360
so once that's set

329
00:14:03,360 --> 00:14:05,279
we can print and we can see that

330
00:14:05,279 --> 00:14:08,240
torch.cuda_is_available() is true

331
00:14:08,240 --> 00:14:10,240
and here's an example of

332
00:14:10,240 --> 00:14:12,800
how we can initialize a tensor on gpu

333
00:14:12,800 --> 00:14:14,240
instead of cpu

334
00:14:14,240 --> 00:14:16,399
so by default if you define

335
00:14:16,399 --> 00:14:18,720
some tensor like torch.ones() it's going

336
00:14:18,720 --> 00:14:22,880
to be on cpu but we can specify a device parameter

337
00:14:22,880 --> 00:14:24,800
and tell PyTorch that we actually want

338
00:14:24,800 --> 00:14:28,880
it to be initialized on gpu instead

339
00:14:28,959 --> 00:14:31,120
and you'll notice here it'll specify

340
00:14:31,120 --> 00:14:35,120
devices on something other than cpu

341
00:14:36,160 --> 00:14:37,920
one thing that you really want to be

342
00:14:37,920 --> 00:14:39,839
careful of is you can only do operations

343
00:14:39,839 --> 00:14:43,360
on tensors if they're actually on the same device

344
00:14:43,360 --> 00:14:45,279
so this code will error because we have

345
00:14:45,279 --> 00:14:48,560
x which is on cpu and y which is on gpu

346
00:14:48,560 --> 00:14:51,760
if we try to add them together

347
00:14:52,639 --> 00:14:56,680
we get this error message

348
00:14:58,399 --> 00:15:00,000
so if we wanted to combine them

349
00:15:00,000 --> 00:15:02,560
together we would have to maybe move

350
00:15:02,560 --> 00:15:05,199
x to the gpu and the way we do that if

351
00:15:05,199 --> 00:15:07,440
it was originally initialized on cpu is

352
00:15:07,440 --> 00:15:09,199
using the two function so we just use

353
00:15:09,199 --> 00:15:13,839
x.to() and specify that we want to go to gpu

354
00:15:15,279 --> 00:15:19,040
and this way we can add them together properly

355
00:15:20,800 --> 00:15:25,120
so kind of like you can't convert tensors that have

356
00:15:25,120 --> 00:15:27,199
gradients being tracked into numpy areas

357
00:15:27,199 --> 00:15:29,360
you also can't convert tensors that are

358
00:15:29,360 --> 00:15:31,920
on gpu directly into numpy arrays so if

359
00:15:31,920 --> 00:15:35,759
I do this i'm gonna get an error

360
00:15:35,839 --> 00:15:37,519
so instead if you want to convert to

361
00:15:37,519 --> 00:15:38,959
numpy what you're gonna have to do is

362
00:15:38,959 --> 00:15:41,040
first move it back to cpu and then

363
00:15:41,040 --> 00:15:44,000
convert to numpy

364
00:15:45,680 --> 00:15:47,519
yeah so that kind of adds

365
00:15:47,519 --> 00:15:50,160
uh like another layer of complexity

366
00:15:50,160 --> 00:15:54,560
I could think about when you want to convert tensors to

367
00:15:54,560 --> 00:15:56,560
numpy arrays you have to make sure that

368
00:15:56,560 --> 00:15:57,759
they're detached so they don't have a

369
00:15:57,759 --> 00:15:59,680
gradient you have to transfer them back

370
00:15:59,680 --> 00:16:01,440
to cpu and then you have to convert to

371
00:16:01,440 --> 00:16:04,560
numpy so this can sometimes be kind

372
00:16:04,560 --> 00:16:06,720
of like a headache to track so we have

373
00:16:06,720 --> 00:16:08,639
this nice utility function that will be

374
00:16:08,639 --> 00:16:10,959
available to you on all the homework assignments

375
00:16:10,959 --> 00:16:15,759
it's called ptu.from_numpy() and to_numpy()

376
00:16:15,759 --> 00:16:18,320
and basically what that will do is

377
00:16:18,320 --> 00:16:20,880
from_numpy() will convert a numpy array to

378
00:16:20,880 --> 00:16:24,320
a PyTorch tensor that's on gpu and

379
00:16:24,320 --> 00:16:26,000
to_numpy() will

380
00:16:26,000 --> 00:16:27,600
given a tensor that is on gpu it'll

381
00:16:27,600 --> 00:16:30,399
first move it to cpu and then convert to numpy

382
00:16:30,399 --> 00:16:32,000
so if you use these two functions you

383
00:16:32,000 --> 00:16:34,399
don't really have to think about

384
00:16:34,399 --> 00:16:36,800
whether you're using gpu or not anymore

385
00:16:36,800 --> 00:16:38,320
if it's supported then it's going to do

386
00:16:38,320 --> 00:16:42,320
all of your tensor operations on gpu

387
00:16:44,079 --> 00:16:46,560
okay so now that we've learned a bit about

388
00:16:46,560 --> 00:16:48,320
gpus let's try to incorporate this into

389
00:16:48,320 --> 00:16:50,560
a full training example on that same

390
00:16:50,560 --> 00:16:53,920
sine wave task from before

391
00:16:54,639 --> 00:16:58,880
so this is the full flow of training a model

392
00:16:58,880 --> 00:17:02,959
everything is pretty much the same as before except a few sections are new

393
00:17:02,959 --> 00:17:04,720
after defining the neural network this

394
00:17:04,720 --> 00:17:07,839
is something that you're gonna have to add in if you want gpu support so

395
00:17:07,839 --> 00:17:09,919
basically you define a device usually

396
00:17:09,919 --> 00:17:11,520
it's good practice to define it this way

397
00:17:11,520 --> 00:17:13,439
where we say if it's available then

398
00:17:13,439 --> 00:17:15,919
we'll use gpu if not we use cpu that way

399
00:17:15,919 --> 00:17:20,079
your code is still compatible either way

400
00:17:20,240 --> 00:17:22,240
and then we're going to want to move

401
00:17:22,240 --> 00:17:24,319
both our neural network

402
00:17:24,319 --> 00:17:28,079
and our targets to that new device

403
00:17:28,079 --> 00:17:30,640
usually in practice you're only going to

404
00:17:30,640 --> 00:17:31,840
move the neural network at first you're

405
00:17:31,840 --> 00:17:33,840
not going to move all the targets

406
00:17:33,840 --> 00:17:35,200
because you might be working with like a

407
00:17:35,200 --> 00:17:37,360
huge data set and it's not feasible to

408
00:17:37,360 --> 00:17:39,840
put it all on gpu at once

409
00:17:39,840 --> 00:17:42,799
instead what you do is

410
00:17:42,799 --> 00:17:44,960
work with them in batches just like

411
00:17:44,960 --> 00:17:47,039
when you're doing a

412
00:17:47,039 --> 00:17:51,679
stochastic gradient descent or mini batch gradient descent and

413
00:17:51,679 --> 00:17:54,720
within the for loop for your

414
00:17:54,720 --> 00:17:56,960
gradient descent code you can move each

415
00:17:56,960 --> 00:17:59,760
individual batch to the gpu separately

416
00:17:59,760 --> 00:18:01,039
but here since we have like a pretty

417
00:18:01,039 --> 00:18:02,240
small set of targets we'll just move

418
00:18:02,240 --> 00:18:06,160
them all to the gpu at once

419
00:18:06,480 --> 00:18:08,320
and the other thing we added is here in

420
00:18:08,320 --> 00:18:11,679
the training code um move our input

421
00:18:11,679 --> 00:18:14,720
from cpu to gpu so normally this is

422
00:18:14,720 --> 00:18:21,280
where you would do something like y_target = y.to(device)

423
00:18:21,280 --> 00:18:26,240
oops y_target.to(device)

424
00:18:27,280 --> 00:18:31,440
uh everything else is pretty much the same as before

425
00:18:32,640 --> 00:18:34,480
and we can run this code and you should

426
00:18:34,480 --> 00:18:37,600
notice that it trains a lot faster

427
00:18:37,600 --> 00:18:39,440
the difference will become much more

428
00:18:39,440 --> 00:18:40,720
significant if you start working with

429
00:18:40,720 --> 00:18:42,640
things like images

430
00:18:42,640 --> 00:18:45,280
or higher dimensional data right now

431
00:18:45,280 --> 00:18:47,039
it's kind of a small difference but

432
00:18:47,039 --> 00:18:48,559
you'll definitely want to use it on all

433
00:18:48,559 --> 00:18:51,600
of your homework assignments

434
00:18:55,200 --> 00:18:58,080
so now moving on with our example

435
00:18:58,080 --> 00:19:02,799
we're gonna evaluate the model this is

436
00:19:02,799 --> 00:19:04,960
usually what you would do to

437
00:19:04,960 --> 00:19:07,039
make predictions with a PyTorch model

438
00:19:07,039 --> 00:19:11,120
so notice there are two things we added here

439
00:19:11,120 --> 00:19:13,039
first we want to do net.eval() what that

440
00:19:13,039 --> 00:19:16,720
does is it switches the network to evaluation mode

441
00:19:16,720 --> 00:19:19,200
it doesn't matter for our example but if

442
00:19:19,200 --> 00:19:20,799
your neural network has things like

443
00:19:20,799 --> 00:19:23,520
dropout or batch norm where

444
00:19:23,520 --> 00:19:24,960
it should behave differently during

445
00:19:24,960 --> 00:19:27,200
training and test time you definitely

446
00:19:27,200 --> 00:19:28,960
want to make sure it's in evaluation

447
00:19:28,960 --> 00:19:31,600
mode so that it doesn't

448
00:19:31,600 --> 00:19:34,400
give you inconsistent results

449
00:19:34,400 --> 00:19:37,360
and the next thing is with torch.no_grad()

450
00:19:37,360 --> 00:19:40,400
this is a nice helper function that you should use when you're making

451
00:19:40,400 --> 00:19:42,799
predictions and basically it says that

452
00:19:42,799 --> 00:19:45,919
everything within this block

453
00:19:45,919 --> 00:19:47,600
any tensors that are that are in this

454
00:19:47,600 --> 00:19:48,559
block they're going to have their

455
00:19:48,559 --> 00:19:51,360
requires grad properties set to false

456
00:19:51,360 --> 00:19:54,000
so since when we're actually

457
00:19:54,000 --> 00:19:56,000
doing evaluation we don't we're not

458
00:19:56,000 --> 00:19:57,679
going to be doing backprop we don't

459
00:19:57,679 --> 00:19:59,919
care about tracking the gradients so

460
00:19:59,919 --> 00:20:01,840
this is kind of like a small performance

461
00:20:01,840 --> 00:20:03,679
improvement we tell PyTorch that we

462
00:20:03,679 --> 00:20:06,720
don't care about the gradients within here

463
00:20:06,720 --> 00:20:13,840
so this second thing is maybe less important this will make your

464
00:20:13,840 --> 00:20:15,280
code a little more efficient but even if

465
00:20:15,280 --> 00:20:17,039
you forget it that's fine as long as you

466
00:20:17,039 --> 00:20:20,640
don't accidentally like call backward on anything

467
00:20:20,640 --> 00:20:22,720
but this net.eval() is really crucial

468
00:20:22,720 --> 00:20:24,960
especially if you're doing things like dropout

469
00:20:24,960 --> 00:20:29,120
otherwise your results are just not going to be correct

470
00:20:29,280 --> 00:20:32,480
so now we have our predictions in the variable y

471
00:20:32,480 --> 00:20:34,640
and remember since these are on gpu we

472
00:20:34,640 --> 00:20:36,559
want to first convert them to cpu and

473
00:20:36,559 --> 00:20:38,400
then we can call numpy to convert them

474
00:20:38,400 --> 00:20:41,280
to numpy arrays

475
00:20:42,480 --> 00:20:45,840
and we can see that it did pretty well

476
00:20:46,640 --> 00:20:48,640
the last thing you'll do after training

477
00:20:48,640 --> 00:20:50,559
and evaluation is probably uh you'll

478
00:20:50,559 --> 00:20:53,520
want to save your model weights so that

479
00:20:53,520 --> 00:20:55,600
later on you can use this trained model

480
00:20:55,600 --> 00:20:57,360
to do something else

481
00:20:57,360 --> 00:21:00,080
so the way that you save models in

482
00:21:00,080 --> 00:21:03,280
PyTorch is that

483
00:21:03,280 --> 00:21:05,840
you you basically take your parameters

484
00:21:05,840 --> 00:21:07,600
and you store them in a dictionary that

485
00:21:07,600 --> 00:21:10,080
gets serialized into some kind of file

486
00:21:10,080 --> 00:21:14,000
so here we're specifying a path for a checkpoint

487
00:21:14,000 --> 00:21:16,400
the convention is usually to uh make the

488
00:21:16,400 --> 00:21:19,360
file extension.pt for PyTorch but

489
00:21:19,360 --> 00:21:20,559
it's not a big deal you can call this

490
00:21:20,559 --> 00:21:22,960
whatever you want

491
00:21:22,960 --> 00:21:26,000
then we have torch.save() so if we call

492
00:21:26,000 --> 00:21:29,360
net.state_dict() that gives us a dictionary

493
00:21:29,360 --> 00:21:32,080
of not only the parameters of the

494
00:21:32,080 --> 00:21:34,880
model but also anything else that's

495
00:21:34,880 --> 00:21:36,320
important for making predictions for

496
00:21:36,320 --> 00:21:38,799
example if you have a batch norm in your

497
00:21:38,799 --> 00:21:40,240
model it's going to keep track of like

498
00:21:40,240 --> 00:21:42,640
the running mean

499
00:21:42,640 --> 00:21:44,400
so that's where state dict is slightly

500
00:21:44,400 --> 00:21:46,559
different than parameters when you're

501
00:21:46,559 --> 00:21:48,080
saving your model you don't just want

502
00:21:48,080 --> 00:21:51,679
the parameters you want the entire state of the model

503
00:21:51,679 --> 00:21:56,159
so we're going to be saving this dictionary to this path

504
00:21:57,760 --> 00:21:59,280
and then let's say later on you come

505
00:21:59,280 --> 00:22:01,840
back and you want to uh you want to load

506
00:22:01,840 --> 00:22:04,000
this checkpoint um back into your model

507
00:22:04,000 --> 00:22:05,600
and make predictions again

508
00:22:05,600 --> 00:22:07,280
so what we'll do is we'll initialize a

509
00:22:07,280 --> 00:22:09,280
new model uh make sure it has like the

510
00:22:09,280 --> 00:22:12,480
same architecture as before and then

511
00:22:12,480 --> 00:22:14,880
we'll call load_state_dict() and we're

512
00:22:14,880 --> 00:22:16,320
going to pass in

513
00:22:16,320 --> 00:22:18,559
the result of loading that file that we

514
00:22:18,559 --> 00:22:20,000
saved to earlier

515
00:22:20,000 --> 00:22:21,440
and then don't forget to call eval if

516
00:22:21,440 --> 00:22:24,880
we're going to be doing this evaluation

517
00:22:25,360 --> 00:22:29,760
so if you run this we can see that

518
00:22:30,320 --> 00:22:32,880
let's see

519
00:22:36,960 --> 00:22:41,799
oh i see okay well

520
00:22:46,640 --> 00:22:48,480
all right let's just put them both on

521
00:22:48,480 --> 00:22:51,440
gpu and now we can compare their parameters

522
00:22:51,440 --> 00:22:52,840
and we'll see that

523
00:22:52,840 --> 00:22:56,320
the loaded checkpoint has the exact

524
00:22:56,320 --> 00:22:58,240
same names and parameters all the values

525
00:22:58,240 --> 00:23:01,840
as the original model

526
00:23:02,000 --> 00:23:04,240
to summarize what we did was we started

527
00:23:04,240 --> 00:23:05,919
by training the model with our loss

528
00:23:05,919 --> 00:23:08,159
function optimizer and the neural net that

529
00:23:08,159 --> 00:23:09,280
we defined

530
00:23:09,280 --> 00:23:11,440
making sure to move everything to gpu if

531
00:23:11,440 --> 00:23:13,280
that's what we want to train on

532
00:23:13,280 --> 00:23:15,600
we evaluated the model making sure to

533
00:23:15,600 --> 00:23:19,200
set it to evaluation mode and turn off the gradients

534
00:23:19,200 --> 00:23:20,960
and then finally we save the model ways

535
00:23:20,960 --> 00:23:22,559
to some checkpoint file so that later on

536
00:23:22,559 --> 00:23:24,720
we can load it and use it to make

537
00:23:24,720 --> 00:23:28,200
predictions again

