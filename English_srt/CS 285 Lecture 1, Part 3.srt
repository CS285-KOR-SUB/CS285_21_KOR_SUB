1
00:00:01,199 --> 00:00:03,199
What other problems do we need to solve

2
00:00:03,199 --> 00:00:05,802
to enable real-world sequential decision-making.

3
00:00:06,480 --> 00:00:09,480
Reinforcement learning deals with
the problem of maximizing rewards.

4
00:00:09,519 --> 00:00:13,019
So it assumes that it can interact with a system,

5
00:00:13,019 --> 00:00:16,827
receives ground truth reward supervision although not ground truth outputs,

6
00:00:16,852 --> 00:00:19,575
and then has to figure out the
right outputs the right actions to take

7
00:00:19,607 --> 00:00:22,028
to maximize those rewards.

8
00:00:22,250 --> 00:00:26,144
But this is not the only problem that
matters for sequential decision making.

9
00:00:26,400 --> 00:00:29,482
More advanced topics which we'll cover in this course

10
00:00:29,553 --> 00:00:33,050
will include things like learning reward
functions from example,

11
00:00:33,120 --> 00:00:35,761
which is called inverse reinforcement learning.

12
00:00:35,994 --> 00:00:38,583
Transfering knowledge between
different domains

13
00:00:38,666 --> 00:00:40,849
thus it is sometimes called transfer learning and meta learning

14
00:00:40,889 --> 00:00:45,975
and it deals with a question of how do you use past experience for different but related tasks

15
00:00:46,000 --> 00:00:49,312
to solve new tasks that you haven't been
able to solve before.

16
00:00:50,079 --> 00:00:52,835
Learning to predict and using prediction to act

17
00:00:52,914 --> 00:00:55,575
this is often referred to as
model based reinforcement learning

18
00:00:55,600 --> 00:00:59,279
where instead of learning behaviors
directly instead what you attempt to learn

19
00:00:59,280 --> 00:01:01,341
is some representation of how the world works

20
00:01:01,366 --> 00:01:06,776
which can then be used for planning
or for synthesizing into a policy.

21
00:01:07,065 --> 00:01:10,377
Here's one question that we'll deal with
in the second half of the course.

22
00:01:10,479 --> 00:01:12,221
Where do rewards come from?

23
00:01:12,525 --> 00:01:17,199
In some cases it seems very natural for that grasping robot that we talked about at the beginning

24
00:01:17,224 --> 00:01:21,123
its reward clearly should depend on
whether it grasps something successfully.

25
00:01:21,226 --> 00:01:23,226
If you're training a dog some trick

26
00:01:23,360 --> 00:01:26,478
maybe the dog's reward function depends on the treat that it gets.

27
00:01:26,880 --> 00:01:28,880
But more generally where they come from

28
00:01:30,079 --> 00:01:33,079
if you're learning an atari game maybe
the answer is obvious

29
00:01:33,348 --> 00:01:36,801
but what if you need a robot to learn to
pour a glass of water

30
00:01:36,880 --> 00:01:39,724
this is something that any child could
probably do but

31
00:01:39,777 --> 00:01:42,552
for a robot you would need an entire perception system

32
00:01:42,577 --> 00:01:46,577
just to understand whether the water
has been poured into the glass correctly.

33
00:01:48,560 --> 00:01:53,910
There was a paper that some of my colleagues at the berkeley ai research lab

34
00:01:53,935 --> 00:01:55,935
put out a few years back

35
00:01:55,960 --> 00:02:00,054
and in their paper they had this comment at the end as human agents

36
00:02:00,079 --> 00:02:02,299
we're accustomed to operating with rewards

37
00:02:02,324 --> 00:02:07,324
that are so sparse that we only
experience them once or twice in a lifetime if at all.

38
00:02:07,973 --> 00:02:11,673
And of course somebody on reddit responded to this by saying "i pity the author".

39
00:02:12,560 --> 00:02:16,517
But while somewhat tragic
this statement does reflect a lot of tasks

40
00:02:16,542 --> 00:02:18,584
that we really do have to solve in the real world

41
00:02:18,640 --> 00:02:21,960
those of you that are taking this class
now, you know

42
00:02:22,080 --> 00:02:27,655
one might say that you're taking this class because you
will receive some delayed reward later down the line

43
00:02:27,680 --> 00:02:30,767
when you graduate with a degree in
computer science

44
00:02:30,879 --> 00:02:34,315
but you can't possibly have learned to

45
00:02:34,480 --> 00:02:37,551
receive degrees in computer science from trial and error

46
00:02:37,576 --> 00:02:40,576
because that's probably only something
you're going to do once in your lifetime.

47
00:02:41,920 --> 00:02:46,559
We know in the brain that
there are structures that are responsible

48
00:02:46,560 --> 00:02:49,053
for reward mechanisms like the basal ganglia

49
00:02:49,077 --> 00:02:53,199
and these are complex structures so the rewards in the brain are not these simple switches

50
00:02:53,200 --> 00:02:56,057
that you get for very specific and very
simple events

51
00:02:56,082 --> 00:03:00,710
there is a lot of brain machinery that is
responsible for assigning appropriate rewards

52
00:03:00,735 --> 00:03:02,171
to appropriate behaviors

53
00:03:02,196 --> 00:03:04,528
and it's not difficult to see why this is necessary.

54
00:03:04,553 --> 00:03:08,829
besides the examples with college degrees

55
00:03:08,854 --> 00:03:13,241
You could imagine a more naturalistic example like a cheetah trying to chase down a gazelle

56
00:03:13,638 --> 00:03:19,919
well it seems pretty unlikely that the cheetah just randomly
flails around until it accidentally manages to maul a gazelle

57
00:03:19,920 --> 00:03:23,815
and from that figures out that because it received a reward should do that behavior more often

58
00:03:23,840 --> 00:03:26,251
this behavior is so difficult for the shooter to perform

59
00:03:26,276 --> 00:03:31,180
there must be some more sophisticated
more detailed guiding mechanism

60
00:03:31,205 --> 00:03:33,244
that caused it to acquire this behavior

61
00:03:34,959 --> 00:03:36,959
So are there other forms of supervision

62
00:03:36,984 --> 00:03:41,182
besides these kind of very sparse
very occasional rewards

63
00:03:41,599 --> 00:03:46,479
well you could imagine acquiring some
representation of behavior from demonstrations

64
00:03:46,480 --> 00:03:49,201
either by directly copying the behavior
that you observed

65
00:03:49,249 --> 00:03:53,375
or by inferring from that behavior
a more detailed reward function

66
00:03:53,400 --> 00:03:55,209
which is called inverse reinforcement learning

67
00:03:55,265 --> 00:04:00,238
So perhaps the cheetah saw an older more experienced cheetah chasing down a gazelle

68
00:04:00,239 --> 00:04:02,667
and figured out from from watching that

69
00:04:02,692 --> 00:04:05,739
that there are some rewarding behaviors that can happen down the line.

70
00:04:07,120 --> 00:04:09,786
You could also learn from observing what happens in the world

71
00:04:09,811 --> 00:04:12,945
so you could for instance learn to predict future events

72
00:04:12,993 --> 00:04:17,015
if you learn something about the
causal structure about the physics of the real world

73
00:04:17,040 --> 00:04:19,976
you can use that knowledge, that learned knowledge,

74
00:04:20,001 --> 00:04:22,809
to figure out which actions will lead to desired outcomes.

75
00:04:24,400 --> 00:04:27,835
Unsupervised learning methods take care of part of this problem

76
00:04:27,860 --> 00:04:29,947
You could also learn from other tasks

77
00:04:29,972 --> 00:04:34,135
so maybe you haven't solved this particular task at hand, maybe you've never obtained

78
00:04:34,160 --> 00:04:36,302
a degree in computer science before

79
00:04:36,327 --> 00:04:39,015
but you've performed other tasks that are structurally related

80
00:04:39,040 --> 00:04:43,365
you went to a high school perhaps
where you obtain a high school degree

81
00:04:43,390 --> 00:04:49,770
and from that you've understood something about the structure
of how you get education how you pass tests and so on.

82
00:04:50,320 --> 00:04:52,772
this is sometimes referred to as transfer learning

83
00:04:53,002 --> 00:04:56,719
A more sophisticated version is called meta learning which actually uses past tasks

84
00:04:56,720 --> 00:04:59,504
not just to become better at new tasks

85
00:04:59,529 --> 00:05:01,711
but actually become better at acquiring
a new task

86
00:05:01,736 --> 00:05:03,616
actually learning how to learn

87
00:05:03,641 --> 00:05:07,554
and this is hopefully something that you've all done through past classes that you've taken.

88
00:05:10,080 --> 00:05:14,478
Methods in these categories have been
showing you some pretty impressive things so

89
00:05:14,479 --> 00:05:18,479
This is a video released by nvidia
about four years ago

90
00:05:18,504 --> 00:05:23,439
showing an imitation learning system for autonomous driving so this is not a reinforcement learning system

91
00:05:23,440 --> 00:05:29,796
it doesn't learn from reward supervision it actually learns by attempting to mimic human drivers

92
00:05:31,440 --> 00:05:35,241
and we'll talk about imitation learning
in a lot of detail next week

93
00:05:37,120 --> 00:05:43,468
But observing other agents
performing useful behaviors can be used for more

94
00:05:43,493 --> 00:05:46,516
than just direct mimicry direct imitation.

95
00:05:46,541 --> 00:05:49,905
This is a video showing an
experiment in child psychology.

96
00:05:49,930 --> 00:05:52,977
so here this guy is trying to put some
books away

97
00:05:53,002 --> 00:05:55,652
and this child is going to watch what
he's doing now that

98
00:05:55,677 --> 00:05:58,175
the child is not going to mimic this person

99
00:05:58,200 --> 00:05:59,961
what the child is going to do instead

100
00:05:59,986 --> 00:06:02,747
is infer what this person is trying to do

101
00:06:02,772 --> 00:06:06,549
and then perform the tasks that they are
attempting to do but more optimally.

102
00:06:06,574 --> 00:06:09,240
So if you can observe
somebody doing something

103
00:06:09,265 --> 00:06:13,279
and figure out their goal perhaps you can actually accomplish their goal more effectively

104
00:06:13,280 --> 00:06:17,073
by performing a little bit of reinforcement learning with that inferred goal

105
00:06:17,955 --> 00:06:20,859
and of course we can turn this into real
world algorithms

106
00:06:20,884 --> 00:06:23,803
so here this is an experiment done by Chelsea Finn

107
00:06:23,828 --> 00:06:28,937
Chelsea is demonstrating to the robot a
behavior for pouring from one cup to another

108
00:06:28,962 --> 00:06:31,311
and the robot is not just
going to copy this behavior

109
00:06:31,336 --> 00:06:33,914
but it's going to do, it's going to infer its goal

110
00:06:33,939 --> 00:06:36,375
which is to position the orange
cup or the yellow cup

111
00:06:36,400 --> 00:06:39,136
and then pour into a cup at a different location

112
00:06:39,161 --> 00:06:42,153
the way that it does the physical
pouring might be a little bit different

113
00:06:42,178 --> 00:06:44,122
but the goal will be the same

114
00:06:48,800 --> 00:06:57,707
Prediction is a pretty fundamental
component of human animal control and decision making

115
00:06:57,732 --> 00:07:03,087
this quote says the idea that we
predict the consequences of our motor commands

116
00:07:03,112 --> 00:07:07,532
has emerged as an important theoretical
concept in all aspects of sensory motor control

117
00:07:08,365 --> 00:07:12,308
Prediction for real world control is
something that is studied

118
00:07:12,333 --> 00:07:14,610
in the domain of model-based
reinforcement learning.

119
00:07:14,635 --> 00:07:19,439
so here's an example from some work by Frederick Ebert who's a student here at UC Berkeley

120
00:07:19,440 --> 00:07:22,935
here we have a robot that is interacting
with objects in its environment

121
00:07:22,960 --> 00:07:29,133
and it's certain interacting very purposefully it's sort
of playing with objects just kind of moving around, randomly pushing things around,

122
00:07:29,158 --> 00:07:34,719
and that experience is going to be used not to
learn a policy for doing any particular thing

123
00:07:34,720 --> 00:07:38,055
but it's said to learn a model that
predicts what will happen next

124
00:07:38,080 --> 00:07:42,399
given what the robot is seeing now and
what actions is considering taking in the future.

125
00:07:42,400 --> 00:07:44,630
So each row in this illustration

126
00:07:44,655 --> 00:07:47,014
starts from the same starting image

127
00:07:47,039 --> 00:07:49,760
but predicts the outcome for different actions

128
00:07:49,785 --> 00:07:53,269
and this particular model is directly predicting the pixels that the robot will see.

129
00:07:53,340 --> 00:07:57,300
so it's predicting that if the, you know, in the top row

130
00:07:57,325 --> 00:08:02,483
if the robot arm moves up that's those are the pixels if it moves down the pixels are different and so on

131
00:08:02,508 --> 00:08:04,801
and of course as you can see in the bottom row

132
00:08:04,826 --> 00:08:08,597
it correctly predicts that if the arm moves to
the right then will also push some objects

133
00:08:08,622 --> 00:08:11,336
now the predictions are kind of blurry
they're quite uncertain

134
00:08:11,361 --> 00:08:15,392
but they count for some basic
properties about how objects will behave

135
00:08:15,646 --> 00:08:17,827
and then we can give the robot a goal we
can tell it

136
00:08:17,852 --> 00:08:20,017
take this object located at this red dot

137
00:08:20,042 --> 00:08:22,105
and move it to the position of this
green dot

138
00:08:22,130 --> 00:08:24,556
and by directly predicting the outcomes of its actions

139
00:08:24,581 --> 00:08:28,460
the robot can plan out a series of actions
that will move the object in the right way

140
00:08:28,485 --> 00:08:32,826
and here's the the result of
actually executing that behavior

141
00:08:34,640 --> 00:08:38,107
we could use predictive models for more
sophisticated behaviors this is worked by Annie

142
00:08:38,132 --> 00:08:41,266
she who was an undergraduate here at berkeley

143
00:08:41,291 --> 00:08:45,599
and what Annie did is extended
the system to use a predictive model

144
00:08:45,600 --> 00:08:47,234
to plan how to use tools

145
00:08:47,259 --> 00:08:51,415
so here a robot could plan to pick up a sponge
and use that sponge to move some objects

146
00:08:51,440 --> 00:08:56,287
it could pick up a little hook and use that
hook to move this blue object to the top

147
00:08:56,312 --> 00:09:02,375
and it could even improvise by figuring out that it can use
a water bottle to sweep some trash to the edge of the bin

148
00:09:05,120 --> 00:09:07,185
We can also use predictive models for
other tasks

149
00:09:07,210 --> 00:09:13,805
so this is some work by Kaiser at all which
focused on using predictive models to play video games

150
00:09:14,305 --> 00:09:19,199
so here in the left you see the predictions
made by the model for this particular video game

151
00:09:19,200 --> 00:09:23,319
and in the middle you can see the real images the image
on the right just shows the difference between the two

152
00:09:23,360 --> 00:09:25,907
the predictions are not actually
perfectly correct

153
00:09:25,932 --> 00:09:30,535
so here for instance the real
image had three opponents the predictor one had only one

154
00:09:30,560 --> 00:09:33,199
but they're kind of qualitatively
right so the model predicts that

155
00:09:33,200 --> 00:09:35,969
when you kick the opponents
they'll disappear

156
00:09:35,994 --> 00:09:40,398
and that you'll get variable numbers
of opponents coming at you

157
00:09:40,399 --> 00:09:43,510
but of course it's not perfect so
sometimes it has a few issues

158
00:09:43,535 --> 00:09:49,065
In this case, you'll have the example of
boxing game the video we'll repeat shortly

159
00:09:49,090 --> 00:09:51,691
in this boxing game
there are two boxers

160
00:09:51,716 --> 00:09:56,079
but they start getting blurry and
this kind of second boxer appears

161
00:09:56,104 --> 00:09:59,335
and that second boxer then grows a third arm

162
00:09:59,360 --> 00:10:01,717
so there's something very strange going on here

163
00:10:01,742 --> 00:10:05,677
something a little bit unusual
basically the model is generalizing in ways

164
00:10:05,702 --> 00:10:07,702
that are ,you know, a little bit off

